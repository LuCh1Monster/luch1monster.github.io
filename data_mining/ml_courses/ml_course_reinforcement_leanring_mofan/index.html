<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>【莫烦Python】强化学习 | 路痴大魔王</title>
  <meta name="keywords" content="">
  <meta name="description" content="【莫烦Python】强化学习 | 路痴大魔王">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="页面未找到！          document.querySelectorAll(&amp;apos;.github-emoji&amp;apos;)           .forEach(el =&amp;gt; {             if (!el.dataset.src) { return; }             const img = document.createElement(&amp;apos;img&amp;apos;);             i">
<meta property="og:type" content="website">
<meta property="og:title" content="404">
<meta property="og:url" content="http:&#x2F;&#x2F;www.monsteryu.top&#x2F;404.html">
<meta property="og:site_name" content="路痴大魔王">
<meta property="og:description" content="页面未找到！          document.querySelectorAll(&amp;apos;.github-emoji&amp;apos;)           .forEach(el =&amp;gt; {             if (!el.dataset.src) { return; }             const img = document.createElement(&amp;apos;img&amp;apos;);             i">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-12-01T14:55:52.000Z">
<meta name="twitter:card" content="summary">


<link rel="icon" href="/img/avatar4.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/rainbow.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1" ></script>

<script src="//cdn.bootcss.com/highlight.js/10.2.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.0.1" ></script>

    <!-- 自定义语言高亮 -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/dockerfile.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/erlang.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/powershell.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/stylus.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/django.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/scala.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/ini.min.js"></script>
    <link rel="stylesheet" href="http://cdn.bootcss.com/bootstrap/3.3.0/css/bootstrap.min.css">
    <script src="http://cdn.bootcss.com/bootstrap/3.3.0/js/bootstrap.min.js"></script>

<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="false">
  <input class="theme_blog_path" value="">
</div>

<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar.jpg" />
</a>
<div class="author">
    <span>郁明敏</span>
</div>

<div class="icon">
    
        
        <a title="github" href="https://github.com/luch1monster" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-github"></use>
                </svg>
            
        </a>
        
    
        
        <a title="csdn" href="https://blog.csdn.net/LuCh1Monster" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-csdn"></use>
                </svg>
            
        </a>
        
    
        
        <a title="email" href="mailto:yu_mingm623@163.com" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-email"></use>
                </svg>
            
        </a>
        
    
        
        <a title="qq" href="http://wpa.qq.com/msgrd?v=3&uin=442523981&site=qq&menu=yes" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-qq"></use>
                </svg>
            
        </a>
        
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(166)</small></div></li>
    
        
            
        
    
        
            
            <li><div data-rel="CDH"><i class="fold iconfont icon-right"></i>CDH<small>(1)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="CDH课程">CDH课程<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="Docker"><i class="fold iconfont icon-right"></i>Docker<small>(5)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Docker基础">Docker基础<small>(4)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Docker课程">Docker课程<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="Git"><i class="fold iconfont icon-right"></i>Git<small>(2)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Git基础">Git基础<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Git课程">Git课程<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="Go"><i class="fold iconfont icon-right"></i>Go<small>(1)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Go基础">Go基础<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
            <li><div data-rel="Hadoop"><i class="fold iconfont icon-right"></i>Hadoop<small>(2)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Hadoop基础">Hadoop基础<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Hadoop课程">Hadoop课程<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="Java"><i class="fold iconfont icon-right"></i>Java<small>(5)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="SpringBoot">SpringBoot<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Java基础">Java基础<small>(3)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Java课程">Java课程<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="LeetCode"><i class="fold iconfont icon-right"></i>LeetCode<small>(7)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="算法">算法<small>(4)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="数据库LC">数据库LC<small>(3)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
            <li><div data-rel="Linux"><i class="fold iconfont icon-right"></i>Linux<small>(3)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Linux基础">Linux基础<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Linux课程">Linux课程<small>(2)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="Portainer"><i class="fold iconfont icon-right"></i>Portainer<small>(7)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Portainer基础">Portainer基础<small>(7)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="Python"><i class="fold iconfont icon-right"></i>Python<small>(58)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Python基础">Python基础<small>(17)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Impyla">Impyla<small>(3)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="AirFlow">AirFlow<small>(3)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Python课程">Python课程<small>(11)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Jupyter">Jupyter<small>(7)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Python爬虫">Python爬虫<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Requests">Requests<small>(4)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="TensorFlow">TensorFlow<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Pandas">Pandas<small>(8)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="PyDub">PyDub<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="PyEcharts">PyEcharts<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="PyPrind">PyPrind<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="R"><i class="fold iconfont icon-right"></i>R<small>(2)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="R基础">R基础<small>(2)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="Scala"><i class="fold iconfont icon-right"></i>Scala<small>(3)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Scala基础">Scala基础<small>(2)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Scala课程">Scala课程<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="前端"><i class="fold iconfont icon-right"></i>前端<small>(13)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Hexo">Hexo<small>(8)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Nodejs">Nodejs<small>(3)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="WordPress">WordPress<small>(2)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="学习资源"><i class="fold iconfont icon-right"></i>学习资源<small>(4)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="基础技术">基础技术<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="大佬博客">大佬博客<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="学习路线">学习路线<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
            <li><div data-rel="工具"><i class="fold iconfont icon-right"></i>工具<small>(17)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Chrome">Chrome<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="GitHub">GitHub<small>(2)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="JetBrains">JetBrains<small>(5)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Markdown">Markdown<small>(2)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Notepad">Notepad<small>(2)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Sublime">Sublime<small>(4)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="XShell">XShell<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
            <li><div data-rel="数据库"><i class="fold iconfont icon-right"></i>数据库<small>(11)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Hive">Hive<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Impala">Impala<small>(7)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="MySQL">MySQL<small>(3)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="机器学习"><i class="fold iconfont icon-right"></i>机器学习<small>(11)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="基本知识">基本知识<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="数理统计">数理统计<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="ML库">ML库<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="ML应用">ML应用<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="模型QA">模型QA<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Model模板代码">Model模板代码<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="自然语言处理">自然语言处理<small>(2)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="超参调优">超参调优<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="ML课程">ML课程<small>(2)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="系统"><i class="fold iconfont icon-right"></i>系统<small>(14)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Mac">Mac<small>(5)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Windows">Windows<small>(9)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    </div>
    <div><a class="about  site_url"  href="/about">关于</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="166">
<input type="hidden" id="yelog_site_word_count" value="284.3k">
<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="#">叶落阁</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode==13){return false;}">
        <input class="search" type="text" placeholder="以 in: 开头进行全文搜索" autocomplete="off"id="local-search-input" >
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none" />
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a class="color5">常用网址</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">文档学习资源</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">统计基础</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">hive</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">impala</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">蓝鲸</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">Mac</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">chrome</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">xshell</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">AirFlow</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">算法</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">数据库</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">Java后端学习路线</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        <a onclick="refreshBtnCopy()" id="top" class="前端 Hexo "
           href="/web/hexo/build_personal_site_using_3-hexo_github/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="使用3-hexo和GitHub搭建个人博客">使用3-hexo和GitHub搭建个人博客</span>
            <span class="post-date" title="2019-11-27 16:53:35">2019/11/27</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Windows "
           href="/systems/windows/wins_install_windows_terminal/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Win10安装Windows Terminal">Win10安装Windows Terminal</span>
            <span class="post-date" title="2020-09-04 00:00:01">2020/09/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Windows "
           href="/systems/windows/wins_chocolatey/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Chocolatey包管理器">Chocolatey包管理器</span>
            <span class="post-date" title="2020-09-04 00:00:01">2020/09/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Windows "
           href="/systems/windows/wins_scoop/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Scoop包管理器">Scoop包管理器</span>
            <span class="post-date" title="2020-09-04 00:00:01">2020/09/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="LeetCode 算法 "
           href="/learning/leetcode/algorithms/%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/"
           data-tag="算法"
           data-author="郁明敏" >
            <span class="post-title" title="无重复字符的最长子串">无重复字符的最长子串</span>
            <span class="post-date" title="2020-03-27 00:00:01">2020/03/27</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="LeetCode 算法 "
           href="/learning/leetcode/algorithms/%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"
           data-tag="算法"
           data-author="郁明敏" >
            <span class="post-title" title="三数之和">三数之和</span>
            <span class="post-date" title="2020-03-25 00:00:01">2020/03/25</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="LeetCode 算法 "
           href="/learning/leetcode/algorithms/%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/"
           data-tag="算法"
           data-author="郁明敏" >
            <span class="post-title" title="两数之和">两数之和</span>
            <span class="post-date" title="2020-03-01 00:00:01">2020/03/01</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="LeetCode 算法 "
           href="/learning/leetcode/algorithms/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/"
           data-tag="算法"
           data-author="郁明敏" >
            <span class="post-title" title="两数相加">两数相加</span>
            <span class="post-date" title="2020-03-01 00:00:01">2020/03/01</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="LeetCode 数据库LC "
           href="/learning/leetcode/databases/%E7%AC%ACN%E9%AB%98%E7%9A%84%E8%96%AA%E6%B0%B4/"
           data-tag="数据库"
           data-author="郁明敏" >
            <span class="post-title" title="第N高的薪水">第N高的薪水</span>
            <span class="post-date" title="2020-03-01 00:00:01">2020/03/01</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="LeetCode 数据库LC "
           href="/learning/leetcode/databases/%E7%AC%AC%E4%BA%8C%E9%AB%98%E7%9A%84%E8%96%AA%E6%B0%B4/"
           data-tag="数据库"
           data-author="郁明敏" >
            <span class="post-title" title="第二高的薪水">第二高的薪水</span>
            <span class="post-date" title="2020-03-01 00:00:01">2020/03/01</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="LeetCode 数据库LC "
           href="/learning/leetcode/databases/%E7%BB%84%E5%90%88%E4%B8%A4%E4%B8%AA%E8%A1%A8/"
           data-tag="数据库"
           data-author="郁明敏" >
            <span class="post-title" title="组合两个表">组合两个表</span>
            <span class="post-date" title="2020-03-01 00:00:01">2020/03/01</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 Hexo "
           href="/web/hexo/hexo_install_live2d_model_plugins/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Hexo安装live2d动态模型">Hexo安装live2d动态模型</span>
            <span class="post-date" title="2020-01-01 14:48:43">2020/01/01</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 Hexo "
           href="/web/hexo/hexo_using_emoji/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Hexo使用emoji表情">Hexo使用emoji表情</span>
            <span class="post-date" title="2020-01-01 14:48:43">2020/01/01</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="CDH CDH课程 "
           href="/bigdata/cdh/cdh_courses/cdh_basic_feiguyun/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【飞谷云】CDH基础">【飞谷云】CDH基础</span>
            <span class="post-date" title="2019-12-22 13:10:28">2019/12/22</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Linux Linux课程 "
           href="/bigdata/linux/linux_courses/linux_couse_mooc/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Linux基础">Linux基础</span>
            <span class="post-date" title="2019-12-22 13:10:28">2019/12/22</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Scala Scala课程 "
           href="/bigdata/scala/scala_courses/scala_basic_feiguyun/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【飞谷云】Scala基础">【飞谷云】Scala基础</span>
            <span class="post-date" title="2019-12-22 13:10:28">2019/12/22</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Hadoop Hadoop课程 "
           href="/bigdata/hadoop/hadoop_courses/hadoop_course_feiguyun/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【飞谷云】Hadoop基础">【飞谷云】Hadoop基础</span>
            <span class="post-date" title="2019-12-22 13:10:28">2019/12/22</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Java Java课程 "
           href="/bigdata/java/java_courses/java_course_feiguyun/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【飞谷云】Java大数据开发">【飞谷云】Java大数据开发</span>
            <span class="post-date" title="2019-12-22 13:10:28">2019/12/22</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Linux Linux课程 "
           href="/bigdata/linux/linux_courses/linux_course_feiguyun/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【飞谷云】Linux基础">【飞谷云】Linux基础</span>
            <span class="post-date" title="2019-12-22 13:10:28">2019/12/22</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Pandas "
           href="/bigdata/python/pandas/add_dataframe_to_email/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="将DataFrame添加到邮件正文">将DataFrame添加到邮件正文</span>
            <span class="post-date" title="2019-12-12 19:30:49">2019/12/12</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Pandas "
           href="/bigdata/python/pandas/dataframe_turn_columns_rows/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="DataFrame行转列">DataFrame行转列</span>
            <span class="post-date" title="2019-12-12 19:30:49">2019/12/12</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Pandas "
           href="/bigdata/python/pandas/multi_dataFrame_write_to_one_excel_with_severval_sheet/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="将多个DataFrame写入一个Excel多个Sheet">将多个DataFrame写入一个Excel多个Sheet</span>
            <span class="post-date" title="2019-12-12 19:30:49">2019/12/12</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Pandas "
           href="/bigdata/python/pandas/numpy_pandas_close_scientific_notion_display/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="NumPy和Pandas关闭科学计数显示">NumPy和Pandas关闭科学计数显示</span>
            <span class="post-date" title="2019-12-12 19:30:49">2019/12/12</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Pandas "
           href="/bigdata/python/pandas/pandas_aggregation_ranking/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Pandas按照字段聚合排序">Pandas按照字段聚合排序</span>
            <span class="post-date" title="2019-12-12 19:30:49">2019/12/12</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Pandas "
           href="/bigdata/python/pandas/pandas_cut_relative_absolue_bin_by_score/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Pandas根据模型分切绝对bin和相对bin">Pandas根据模型分切绝对bin和相对bin</span>
            <span class="post-date" title="2019-12-12 19:30:49">2019/12/12</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Pandas "
           href="/bigdata/python/pandas/pandas_read_files/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Pandas读取文件">Pandas读取文件</span>
            <span class="post-date" title="2019-12-12 19:30:49">2019/12/12</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Pandas "
           href="/bigdata/python/pandas/pandas_set_max_rows_columns/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Pandas设置set_option">Pandas设置set_option</span>
            <span class="post-date" title="2019-12-12 19:30:49">2019/12/12</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Windows "
           href="/systems/windows/wins_softwares_download_sources/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Windows软件下载">Windows软件下载</span>
            <span class="post-date" title="2019-12-12 13:53:23">2019/12/12</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Windows "
           href="/tools/cmder/cmder_config_default_open_fold/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Cmder设置默认打开目录">Cmder设置默认打开目录</span>
            <span class="post-date" title="2019-12-12 13:53:23">2019/12/12</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 Notepad "
           href="/tools/notepad/notepad_nppftp_plugin/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="NppFTP插件连接远程服务器">NppFTP插件连接远程服务器</span>
            <span class="post-date" title="2019-12-08 20:51:47">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 Notepad "
           href="/tools/notepad/notepad_plugins/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Notepad++插件列表">Notepad++插件列表</span>
            <span class="post-date" title="2019-12-08 20:51:47">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 JetBrains "
           href="/tools/jetbrains/pycharm_questions/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="PyCharm常见问题">PyCharm常见问题</span>
            <span class="post-date" title="2019-12-08 20:46:04">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Java SpringBoot "
           href="/bigdata/springboot/build_springboot_dev_env_in_wins/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Windows搭建SpringBoot开发环境">Windows搭建SpringBoot开发环境</span>
            <span class="post-date" title="2019-12-08 20:42:31">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 Sublime "
           href="/tools/sublime/sublime_activation/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Sublime激活">Sublime激活</span>
            <span class="post-date" title="2019-12-08 20:42:31">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 Sublime "
           href="/tools/sublime/sublime_cancel_update/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Sublime取消升级">Sublime取消升级</span>
            <span class="post-date" title="2019-12-08 20:42:31">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 Sublime "
           href="/tools/sublime/sublime_plugins/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Sublime插件">Sublime插件</span>
            <span class="post-date" title="2019-12-08 20:42:31">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 Sublime "
           href="/tools/sublime/sublime_questions/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Sublime常见问题">Sublime常见问题</span>
            <span class="post-date" title="2019-12-08 20:42:31">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Windows "
           href="/systems/windows/wins_install_wsl/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Windows安装WSL">Windows安装WSL</span>
            <span class="post-date" title="2019-12-08 20:31:41">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Windows "
           href="/systems/windows/wins_network_questions/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Windows网络常见问题">Windows网络常见问题</span>
            <span class="post-date" title="2019-12-08 20:31:41">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 Chrome "
           href="/tools/chrome/chrome_common_plugins/"
           data-tag="chrome"
           data-author="郁明敏" >
            <span class="post-title" title="Chrome常用插件">Chrome常用插件</span>
            <span class="post-date" title="2019-12-08 20:29:18">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Windows "
           href="/systems/windows/wins_bat/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="bat脚本">bat脚本</span>
            <span class="post-date" title="2019-12-08 20:22:58">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Windows "
           href="/systems/windows/wins_softwares_list/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Windows软件列表">Windows软件列表</span>
            <span class="post-date" title="2019-12-08 20:22:58">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_common_packages/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python常用库">Python常用库</span>
            <span class="post-date" title="2019-12-08 20:04:22">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Mac "
           href="/systems/mac/build_a_perfect_prgraming_mac/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="打造强大的Mac系统">打造强大的Mac系统</span>
            <span class="post-date" title="2019-12-08 19:49:04">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Mac "
           href="/systems/mac/mac_change_typora_vue_theme/"
           data-tag="Mac"
           data-author="郁明敏" >
            <span class="post-title" title="Mac下Typora更换Vue主题">Mac下Typora更换Vue主题</span>
            <span class="post-date" title="2019-12-08 19:49:04">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Mac "
           href="/systems/mac/mac_common_keyshort/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="MAC常用快捷键">MAC常用快捷键</span>
            <span class="post-date" title="2019-12-08 19:49:04">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Mac "
           href="/systems/mac/mac_install_zsh/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Mac安装Zsh">Mac安装Zsh</span>
            <span class="post-date" title="2019-12-08 19:49:04">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="系统 Mac "
           href="/systems/mac/mac_use_frp/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Mac使用FRP">Mac使用FRP</span>
            <span class="post-date" title="2019-12-08 19:49:04">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 MySQL "
           href="/databases/mysql/centos_migrate_mysql_databases/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Centos下迁移MySQL数据库">Centos下迁移MySQL数据库</span>
            <span class="post-date" title="2019-12-08 19:37:05">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 MySQL "
           href="/databases/mysql/wins_install_mysql/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Windows下迁移MySQL数据库">Windows下迁移MySQL数据库</span>
            <span class="post-date" title="2019-12-08 19:37:05">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_install_complex_packages/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python安装第三方库">Python安装第三方库</span>
            <span class="post-date" title="2019-12-08 19:29:16">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 MySQL "
           href="/databases/mysql/centos_install_mysql/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Centos安装MySQL服务">Centos安装MySQL服务</span>
            <span class="post-date" title="2019-12-08 19:01:46">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Linux Linux基础 "
           href="/bigdata/linux/linux_basic/linux_common_usages/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Linux总结">Linux总结</span>
            <span class="post-date" title="2019-12-08 18:02:08">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Jupyter "
           href="/bigdata/python/jupyter/jupyter_install_languages_kernel/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Jupyter安装其他语言内核">Jupyter安装其他语言内核</span>
            <span class="post-date" title="2019-12-08 17:39:50">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Jupyter "
           href="/bigdata/python/jupyter/jupyter_nohup/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Jupyter后台运行">Jupyter后台运行</span>
            <span class="post-date" title="2019-12-08 17:39:50">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Jupyter "
           href="/bigdata/python/jupyter/jupyter_plugins_nbdime/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="nbdime插件">nbdime插件</span>
            <span class="post-date" title="2019-12-08 17:39:50">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Jupyter "
           href="/bigdata/python/jupyter/jupyter_plugins_nbextensions/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="nbextensions插件">nbextensions插件</span>
            <span class="post-date" title="2019-12-08 17:39:50">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Jupyter "
           href="/bigdata/python/jupyter/jupyter_reload_user_defined_packages/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Jupyter重新导入自定义包">Jupyter重新导入自定义包</span>
            <span class="post-date" title="2019-12-08 17:39:50">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Jupyter "
           href="/bigdata/python/jupyter/jupyter_set_password/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Jupyter设置访问密码">Jupyter设置访问密码</span>
            <span class="post-date" title="2019-12-08 17:39:50">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Jupyter "
           href="/bigdata/python/jupyter/jupyter_uninstall_laguages_kernel/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Jupyter卸载其他语言内核">Jupyter卸载其他语言内核</span>
            <span class="post-date" title="2019-12-08 17:39:50">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 Nodejs "
           href="/web/nodejs/mac_install_npm_cnpm/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Mac下安装NPM和CNPM">Mac下安装NPM和CNPM</span>
            <span class="post-date" title="2019-12-08 16:03:30">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 Nodejs "
           href="/web/nodejs/node_install_packages/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Node安装开发环境">Node安装开发环境</span>
            <span class="post-date" title="2019-12-08 16:03:30">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 WordPress "
           href="/web/wordpress/wp_plugins/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="WordPress常用插件">WordPress常用插件</span>
            <span class="post-date" title="2019-12-08 16:03:30">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 Nodejs "
           href="/web/nodejs/ubuntu_install_nodejs/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Ubuntu下安装Nodejs">Ubuntu下安装Nodejs</span>
            <span class="post-date" title="2019-12-08 16:03:30">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 WordPress "
           href="/web/wordpress/wp_themes/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="WordPress主题">WordPress主题</span>
            <span class="post-date" title="2019-12-08 16:03:30">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python爬虫 "
           href="/bigdata/python/python_crawler/python_crawl_workday_info/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Requests获取工作日信息">Requests获取工作日信息</span>
            <span class="post-date" title="2019-12-08 16:03:30">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python AirFlow "
           href="/bigdata/python/airflow/airflow%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3/"
           data-tag="AirFlow"
           data-author="郁明敏" >
            <span class="post-title" title="AirFlow参考文档">AirFlow参考文档</span>
            <span class="post-date" title="2019-12-08 15:59:10">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 Impala "
           href="/databases/impala/impala_bluewhale/"
           data-tag="蓝鲸"
           data-author="郁明敏" >
            <span class="post-title" title="蓝鲸使用">蓝鲸使用</span>
            <span class="post-date" title="2019-12-08 15:40:18">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Go Go基础 "
           href="/bigdata/go/go_install/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Go语言安装">Go语言安装</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Docker Docker基础 "
           href="/bigdata/docker/docker_basic/docker_mirrors/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Docker镜像源">Docker镜像源</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Docker Docker基础 "
           href="/bigdata/docker/docker_basic/anaconda_dockerfile/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="基于Centos7下Anaconda的dockerfile">基于Centos7下Anaconda的dockerfile</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Docker Docker基础 "
           href="/bigdata/docker/docker_basic/docker_install/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Docker安装">Docker安装</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Docker Docker基础 "
           href="/bigdata/docker/docker_basic/docker_questions/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Docker问题总结">Docker问题总结</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Docker Docker课程 "
           href="/bigdata/docker/docker_courses/docker_course_qianfeng/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【千锋学院】Docker基础">【千锋学院】Docker基础</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Java Java基础 "
           href="/bigdata/java/java_basic/jdk_install/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="JDK安装">JDK安装</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Impyla "
           href="/bigdata/python/Impyla/python_connect_impala_timeout/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python连接Impala超时">Python连接Impala超时</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python PyDub "
           href="/bigdata/python/pydub/python_convert_mp3_into_wav/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python将MP3文件转换为WAV文件">Python将MP3文件转换为WAV文件</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python PyEcharts "
           href="/bigdata/python/pyecharts/pyecharts_plot_examples/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="PyEcharts简单图示例">PyEcharts简单图示例</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_command_line_parameters/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python命令行参数">Python命令行参数</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_config_tunatsinghua_mirror/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python使用清华镜像源">Python使用清华镜像源</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_display_percent_num/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python将数字百分比显示">Python将数字百分比显示</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_send_emial/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python使用Outlook发送邮件">Python使用Outlook发送邮件</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_user_defined_decrator/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python自定义装饰器">Python自定义装饰器</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Requests "
           href="/bigdata/python/requests/requests_send_messages_to_workchat/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Requests企业微信推送监控消息">Requests企业微信推送监控消息</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Scala Scala基础 "
           href="/bigdata/scala/scala_basic/scala_command_line_parameters/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="scala命令行参数">scala命令行参数</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Scala Scala基础 "
           href="/bigdata/scala/scala_basic/scala_install/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Scala安装">Scala安装</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Impyla "
           href="/bigdata/python/Impyla/python_create_table_in_impala/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="使用Python在Impala中建表">使用Python在Impala中建表</span>
            <span class="post-date" title="2019-12-08 15:13:02">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_zip_fold/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="使用Python压缩文件夹">使用Python压缩文件夹</span>
            <span class="post-date" title="2019-12-08 15:13:02">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Requests "
           href="/bigdata/python/requests/requests_headers/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="requests headers">requests headers</span>
            <span class="post-date" title="2019-12-08 15:13:02">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Requests "
           href="/bigdata/python/requests/requests_questions/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="requests问题总结">requests问题总结</span>
            <span class="post-date" title="2019-12-08 15:13:02">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python TensorFlow "
           href="/bigdata/python/tensorflow/windows_install_tensorflow_gpu/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Windows安装TensorFlow-GPU版本">Windows安装TensorFlow-GPU版本</span>
            <span class="post-date" title="2019-12-08 15:13:02">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 JetBrains "
           href="/tools/jetbrains/datagrip_user_define_connect_sources/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="DataGrip自定义连接Hive和Impala">DataGrip自定义连接Hive和Impala</span>
            <span class="post-date" title="2019-12-08 15:08:00">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 JetBrains "
           href="/tools/jetbrains/idea_plugins_leetcode/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="IDEA插件-LeetCode刷题">IDEA插件-LeetCode刷题</span>
            <span class="post-date" title="2019-12-08 15:08:00">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 JetBrains "
           href="/tools/jetbrains/jetbrains_ide_using_mono_font/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="IDE使用JetBrains新字体">IDE使用JetBrains新字体</span>
            <span class="post-date" title="2019-12-08 15:08:00">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 Impala "
           href="/databases/impala/impala_aggregation/"
           data-tag="impala"
           data-author="郁明敏" >
            <span class="post-title" title="Impala聚合操作">Impala聚合操作</span>
            <span class="post-date" title="2019-12-08 14:56:27">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 Impala "
           href="/databases/impala/impala_common_usages/"
           data-tag="impala"
           data-author="郁明敏" >
            <span class="post-title" title="Impala常规使用">Impala常规使用</span>
            <span class="post-date" title="2019-12-08 14:56:27">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 Impala "
           href="/databases/impala/impala_date/"
           data-tag="impala"
           data-author="郁明敏" >
            <span class="post-title" title="Impala操作日期函数">Impala操作日期函数</span>
            <span class="post-date" title="2019-12-08 14:56:27">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 Impala "
           href="/databases/impala/impala_json/"
           data-tag="impala"
           data-author="郁明敏" >
            <span class="post-title" title="Impala解析JSON字符串">Impala解析JSON字符串</span>
            <span class="post-date" title="2019-12-08 14:56:27">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 Impala "
           href="/databases/impala/impala_regexp/"
           data-tag="impala"
           data-author="郁明敏" >
            <span class="post-title" title="Impala正则匹配">Impala正则匹配</span>
            <span class="post-date" title="2019-12-08 14:56:27">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 Impala "
           href="/databases/impala/impala_url/"
           data-tag="impala"
           data-author="郁明敏" >
            <span class="post-title" title="Impala解析URL">Impala解析URL</span>
            <span class="post-date" title="2019-12-08 14:56:27">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="数据库 Hive "
           href="/databases/hive/hive_common_questions/"
           data-tag="hive"
           data-author="郁明敏" >
            <span class="post-title" title="Hive问题总结">Hive问题总结</span>
            <span class="post-date" title="2019-12-08 14:48:43">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 Hexo "
           href="/web/hexo/hexo_plugins/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Hexo好用的插件">Hexo好用的插件</span>
            <span class="post-date" title="2019-12-08 14:48:43">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 Hexo "
           href="/web/hexo/hexo_questions/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Hexo问题总结">Hexo问题总结</span>
            <span class="post-date" title="2019-12-08 14:48:43">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 Hexo "
           href="/web/hexo/hexo_user_defined_languages_highlight/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Hexo添加自定义语言高亮">Hexo添加自定义语言高亮</span>
            <span class="post-date" title="2019-12-08 14:48:43">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Portainer Portainer基础 "
           href="/bigdata/portainer/portainer_basic_configuration/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Portainer基础配置">Portainer基础配置</span>
            <span class="post-date" title="2019-12-08 14:27:24">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Portainer Portainer基础 "
           href="/bigdata/portainer/portainer_create_mirrors/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Portainer创建镜像">Portainer创建镜像</span>
            <span class="post-date" title="2019-12-08 14:27:24">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Portainer Portainer基础 "
           href="/bigdata/portainer/portainer_create_centos_container/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Portainer创建CentOS容器">Portainer创建CentOS容器</span>
            <span class="post-date" title="2019-12-08 14:27:24">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Portainer Portainer基础 "
           href="/bigdata/portainer/portainer_create_wp_container/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Portainer创建WordPress容器">Portainer创建WordPress容器</span>
            <span class="post-date" title="2019-12-08 14:27:24">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Portainer Portainer基础 "
           href="/bigdata/portainer/portainer_create_mysql_container/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Portainer创建MySQL容器">Portainer创建MySQL容器</span>
            <span class="post-date" title="2019-12-08 14:27:24">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Portainer Portainer基础 "
           href="/bigdata/portainer/portainer_install/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Portainer安装">Portainer安装</span>
            <span class="post-date" title="2019-12-08 14:27:24">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Portainer Portainer基础 "
           href="/bigdata/portainer/portainer_user_access/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Portainer权限管理">Portainer权限管理</span>
            <span class="post-date" title="2019-12-08 14:27:24">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Hadoop Hadoop基础 "
           href="/bigdata/hadoop/hadoop_basic/hadoop%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Hadoop问题总结">Hadoop问题总结</span>
            <span class="post-date" title="2019-12-08 14:27:24">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python AirFlow "
           href="/bigdata/python/airflow/airflow%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/"
           data-tag="AirFlow"
           data-author="郁明敏" >
            <span class="post-title" title="AirFlow基础使用">AirFlow基础使用</span>
            <span class="post-date" title="2019-12-08 14:24:18">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 Markdown "
           href="/tools/markdown/latex_usages/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="LaTex使用总结">LaTex使用总结</span>
            <span class="post-date" title="2019-12-08 13:40:22">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 Markdown "
           href="/tools/markdown/markdown_usages/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Markdown用法总结">Markdown用法总结</span>
            <span class="post-date" title="2019-12-08 13:40:22">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 ML库 "
           href="/data_mining/ml_packages/shap_introduction/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python可解释机器学习库SHAP">Python可解释机器学习库SHAP</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 ML应用 "
           href="/data_mining/model_application/cerdit_card_model_score_calibration/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="信用评分卡模型分校准">信用评分卡模型分校准</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 ML课程 "
           href="/data_mining/ml_courses/ml_course_engineer_hanxiaoyang/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【寒小阳】机器学习工程师">【寒小阳】机器学习工程师</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Git Git基础 "
           href="/bigdata/git/git_basic/git_questions/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Git问题总结">Git问题总结</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Git Git课程 "
           href="/bigdata/git/git_courses/course_git_github/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Git与GitHub基础课程">Git与GitHub基础课程</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_basic_regexp_expression/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python基本正则表达式">Python基本正则表达式</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_gui_introduction/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【极客学院】Python图形程序入门">【极客学院】Python图形程序入门</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_matplotlib_visualization/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【极客学院】Matplitlib可视化">【极客学院】Matplitlib可视化</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_multi_processing_mofan/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【莫烦Python】Python多进程">【莫烦Python】Python多进程</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_multi_threading_mofan/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【莫烦Python】Python多线程">【莫烦Python】Python多线程</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_pandas_package/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="bigdata/python/python_courses/python_course_pandas_package">bigdata/python/python_courses/python_course_pandas_package</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_scrapy_first_exploration/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【极客学院】Scrapy初探">【极客学院】Scrapy初探</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_scientific_computation/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【极客学院】Python科学计算">【极客学院】Python科学计算</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_single_thread_crawler/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【极客学院】Python单线程爬虫">【极客学院】Python单线程爬虫</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_tkinter_mofan/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【莫烦Python】Tkinter">【莫烦Python】Tkinter</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python课程 "
           href="/bigdata/python/python_courses/python_course_matplotlib_visualization_data_analysis/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Matplitlib数据可视化分析">Matplitlib数据可视化分析</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 ML课程 "
           href="/data_mining/ml_courses/ml_course_reinforcement_leanring_mofan/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="【莫烦Python】强化学习">【莫烦Python】强化学习</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 GitHub "
           href="/tools/github/github_advanced_tips_in_searching_open_projects/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="GitHub搜索开源项目技巧">GitHub搜索开源项目技巧</span>
            <span class="post-date" title="2019-12-08 11:10:15">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 GitHub "
           href="/tools/github/the_way_improving_github_speed/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="GitHub提速的方法">GitHub提速的方法</span>
            <span class="post-date" title="2019-12-08 11:10:15">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 JetBrains "
           href="/tools/jetbrains/pycharm_latest_activation_method/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="PyCharm2019.2最新激活方式">PyCharm2019.2最新激活方式</span>
            <span class="post-date" title="2019-12-08 10:27:59">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Java Java基础 "
           href="/bigdata/java/java_basic/java_command_line_parameters/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="java虚拟机参数">java虚拟机参数</span>
            <span class="post-date" title="2019-12-08 10:27:59">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Java Java基础 "
           href="/bigdata/java/java_basic/javac_command_line_parameters/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="javac虚拟机参数">javac虚拟机参数</span>
            <span class="post-date" title="2019-12-08 10:27:59">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="R R基础 "
           href="/bigdata/r/r_basic/r_install/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="R语言安装">R语言安装</span>
            <span class="post-date" title="2019-12-08 10:27:59">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="R R基础 "
           href="/bigdata/r/r_basic/rstudio_install/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="RStudio安装">RStudio安装</span>
            <span class="post-date" title="2019-12-08 10:27:59">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 数理统计 "
           href="/data_mining/mathematical_statistics/statistics_hypothesis_test/"
           data-tag="统计基础"
           data-author="郁明敏" >
            <span class="post-title" title="统计假设检验">统计假设检验</span>
            <span class="post-date" title="2019-12-08 10:26:23">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 模型QA "
           href="/data_mining/model_qas/model_qa/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="模型问题整理">模型问题整理</span>
            <span class="post-date" title="2019-12-08 10:26:23">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 超参调优 "
           href="/data_mining/optimization_parameters/lgbm_optimizaton_parameters/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="LGBM超参调优">LGBM超参调优</span>
            <span class="post-date" title="2019-12-08 10:26:23">2019/12/08</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="工具 XShell "
           href="/tools/xshell/xshell_connect_docker_server/"
           data-tag="xshell"
           data-author="郁明敏" >
            <span class="post-title" title="XShell连接Docker服务器下Centos终端">XShell连接Docker服务器下Centos终端</span>
            <span class="post-date" title="2019-12-07 23:40:55">2019/12/07</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python AirFlow "
           href="/bigdata/python/airflow/airflow_install_linux/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Linux下AirFlow安装">Linux下AirFlow安装</span>
            <span class="post-date" title="2019-12-07 13:52:21">2019/12/07</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_inbuilt_keywords/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="冷僻的Python内置关键字">冷僻的Python内置关键字</span>
            <span class="post-date" title="2019-12-07 02:15:12">2019/12/07</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="学习资源 基础技术 "
           href="/learning/%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99/"
           data-tag="常用网址"
           data-author="郁明敏" >
            <span class="post-title" title="基础技术学习网站">基础技术学习网站</span>
            <span class="post-date" title="2019-12-06 22:52:18">2019/12/06</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="学习资源 大佬博客 "
           href="/learning/%E5%AF%92%E5%B0%8F%E9%98%B3%E7%9A%84%E5%8D%9A%E5%AE%A2/"
           data-tag="文档学习资源"
           data-author="郁明敏" >
            <span class="post-title" title="寒小阳的个人博客">寒小阳的个人博客</span>
            <span class="post-date" title="2019-12-06 22:52:18">2019/12/06</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="学习资源 "
           href="/learning/%E6%96%87%E6%A1%A3%E8%B5%84%E6%BA%90/"
           data-tag="文档学习资源"
           data-author="郁明敏" >
            <span class="post-title" title="文档资源">文档资源</span>
            <span class="post-date" title="2019-12-06 22:52:18">2019/12/06</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="学习资源 学习路线 "
           href="/learning/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF-Java%E5%90%8E%E7%AB%AF/"
           data-tag="Java后端学习路线"
           data-author="郁明敏" >
            <span class="post-title" title="学习路线【Java后端】">学习路线【Java后端】</span>
            <span class="post-date" title="2019-12-06 22:52:18">2019/12/06</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Impyla "
           href="/bigdata/python/Impyla/python_add_multi_hive_partitions/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="使用Python批量添加Hive分区">使用Python批量添加Hive分区</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python PyPrind "
           href="/bigdata/python/pyprind/python_add_progress_bar_with_pyprind_in_tasks/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="使用Python给程序加一个进度条">使用Python给程序加一个进度条</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_add_user_defined_packages_in_env/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="自定义Python包添加到环境中">自定义Python包添加到环境中</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_generate_random_password/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python生成随机密码">Python生成随机密码</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_get_random_data_from_collections/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python随机从集合中取数">Python随机从集合中取数</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_md5_encryption/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python使用md5加密">Python使用md5加密</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_obtain_current_file_path/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python获取当前文件的路径">Python获取当前文件的路径</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_questions/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python问题总结">Python问题总结</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_sqlalchmey_connect_mysql/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Python使用SQLAlchmey连接MySQL">Python使用SQLAlchmey连接MySQL</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Python基础 "
           href="/bigdata/python/python_basic/python_upload_files_windows_share_fold/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="使用Python将文件上传到Windows共享文件夹">使用Python将文件上传到Windows共享文件夹</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="Python Requests "
           href="/bigdata/python/requests/requests_xml_test_servers_status/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="使用Requests和XML测试服务器状态">使用Requests和XML测试服务器状态</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 基本知识 "
           href="/data_mining/machine_learning/common_steps_and_tricks/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="机器学习步骤与套路">机器学习步骤与套路</span>
            <span class="post-date" title="2019-12-03 14:59:37">2019/12/03</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 Model模板代码 "
           href="/data_mining/model_template_code/lr_code_by_statsmodels/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="LR代码模板">LR代码模板</span>
            <span class="post-date" title="2019-12-03 14:59:37">2019/12/03</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 自然语言处理 "
           href="/data_mining/nlp/nlp_cookbook/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="NLP Cookbook">NLP Cookbook</span>
            <span class="post-date" title="2019-12-03 14:59:37">2019/12/03</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="机器学习 自然语言处理 "
           href="/data_mining/nlp/word2vec_train_process/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="Word2Vec训练过程">Word2Vec训练过程</span>
            <span class="post-date" title="2019-12-03 14:59:37">2019/12/03</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 Hexo "
           href="/web/hexo/hexo_optimization_formula_display/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="hexo优化公式显示">hexo优化公式显示</span>
            <span class="post-date" title="2019-11-27 16:53:35">2019/11/27</span>
        </a>
        
        <a onclick="refreshBtnCopy()"  class="前端 Hexo "
           href="/web/hexo/hexo_optimization_markdown/"
           data-tag=""
           data-author="郁明敏" >
            <span class="post-title" title="hexo优化markdown使用">hexo优化markdown使用</span>
            <span class="post-date" title="2019-11-27 16:53:35">2019/11/27</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first">
                <i class="fa fa-arrow-circle-left" aria-hidden="true"></i>
            </div>
            <div class="brackets">
                <i class="fa fa-arrow-circle-right" aria-hidden="true"></i>
            </div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-data_mining/ml_courses/ml_course_reinforcement_leanring_mofan" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">【莫烦Python】强化学习</h1>
    
    <div class="article-meta">
        
        
        <span class="author"><a>郁明敏</a></span>
        
        
        <span class="book">
            
                <a  data-rel="机器学习">机器学习</a>/
            
                <a  data-rel="ML课程">ML课程</a>
            
        </span>
        
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title='更新时间: 2020-09-13 16:21:22'>2019-12-08 12:49</time>
        
    </div>
    <div class="article-meta">
        
        <span>字数:34.6k</span>
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-什么是强化学习-reinforcement-learning"><span class="toc-text">1. 什么是强化学习(Reinforcement Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-从无到有"><span class="toc-text">1.1 从无到有</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-虚拟老师"><span class="toc-text">1.2 虚拟老师</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-对比监督学习"><span class="toc-text">1.3 对比监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-rl算法"><span class="toc-text">1.4 RL算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-强化学习汇总"><span class="toc-text">2. 强化学习汇总</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-model-free和model-based"><span class="toc-text">2.1 Model-free和Model-based</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-基于概率和基于价值"><span class="toc-text">2.2 基于概率和基于价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-回合更新和单步更新"><span class="toc-text">2.3 回合更新和单步更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-在线学习和离线学习"><span class="toc-text">2.4 在线学习和离线学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-为什么用强化学习"><span class="toc-text">3. 为什么用强化学习?</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-强化学习介绍"><span class="toc-text">3.1 强化学习介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-模拟程序提前看"><span class="toc-text">3.2 模拟程序提前看</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-课程要求"><span class="toc-text">4. 课程要求</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-教程必备模块"><span class="toc-text">4.1 教程必备模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-快速了解强化学习"><span class="toc-text">4.2 快速了解强化学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-什么是q-learning"><span class="toc-text">5. 什么是Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-行为准则"><span class="toc-text">5.1 行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-q-learning决策"><span class="toc-text">5.2 Q-Learning决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-q-learning更新"><span class="toc-text">5.3 Q-Learning更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-q-learning整体算法"><span class="toc-text">5.4 Q-Learning整体算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-q-learning中的gamma"><span class="toc-text">5.5 Q-Learning中的Gamma</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-强化学习小例子"><span class="toc-text">6. 强化学习小例子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-要点"><span class="toc-text">6.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-预设值"><span class="toc-text">6.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-q表"><span class="toc-text">6.3 Q表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-定义动作"><span class="toc-text">6.4 定义动作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-环境反馈-s-r"><span class="toc-text">6.5 环境反馈 S_，R</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-环境更新"><span class="toc-text">6.6 环境更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-7-强化学习主循环"><span class="toc-text">6.7 强化学习主循环</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-8-q-table的演变"><span class="toc-text">6.8 Q-Table的演变</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-q-learning算法更新"><span class="toc-text">7. Q-Learning算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-要点"><span class="toc-text">7.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-算法"><span class="toc-text">7.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-算法的代码形式"><span class="toc-text">7.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-q-learning思维决策"><span class="toc-text">8. Q-Learning思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-代码主结构"><span class="toc-text">8.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-预设值"><span class="toc-text">8.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-决定行为"><span class="toc-text">8.3 决定行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-学习"><span class="toc-text">8.4 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-检测-state-是否存在"><span class="toc-text">8.5 检测 state 是否存在</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-什么是sarsa"><span class="toc-text">9. 什么是Sarsa</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-sarsa决策"><span class="toc-text">9.1 Sarsa决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-sarsa更新行为准则"><span class="toc-text">9.2 Sarsa更新行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-对比sarsa和q-learning算法"><span class="toc-text">9.3 对比Sarsa和Q-Learning算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-sarsa算法更新"><span class="toc-text">10. Sarsa算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-要点"><span class="toc-text">10.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-算法"><span class="toc-text">10.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-算法的代码形式"><span class="toc-text">10.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-sarsa思维决策"><span class="toc-text">11. Sarsa思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-1-代码主结构"><span class="toc-text">11.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-2-学习"><span class="toc-text">11.2 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-什么是sarsa-lambda"><span class="toc-text">12. 什么是Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-sarsa-n"><span class="toc-text">12.1 Sarsa(n)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-2-单步更新和回合更新"><span class="toc-text">12.2 单步更新和回合更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-有时迷茫"><span class="toc-text">12.3 有时迷茫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-4-lambda含义"><span class="toc-text">12.4 Lambda含义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-5-lambda取值"><span class="toc-text">12.5 Lambda取值</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-sarsa-lambda"><span class="toc-text">13. Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#13-1-要点"><span class="toc-text">13.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-2-代码主结构"><span class="toc-text">13.2 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-3-预设值"><span class="toc-text">13.3 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-4-检测state是否存在"><span class="toc-text">13.4 检测state是否存在</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-5-学习"><span class="toc-text">13.5 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14-什么是dqn"><span class="toc-text">14. 什么是DQN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#14-1-强化学习与神经网络"><span class="toc-text">14.1 强化学习与神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-2-神经网络的作用"><span class="toc-text">14.2 神经网络的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-3-更新神经网络"><span class="toc-text">14.3 更新神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-4-dqn两大利器"><span class="toc-text">14.4 DQN两大利器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-dqn算法更新-tensorflow"><span class="toc-text">15. DQN算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#15-1-要点"><span class="toc-text">15.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-2-算法"><span class="toc-text">15.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-3-算法的代码形式"><span class="toc-text">15.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#16-dqn神经网络-tensorflow"><span class="toc-text">16. DQN神经网络—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#16-1-要点"><span class="toc-text">16.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-2-两个神经网络"><span class="toc-text">16.2 两个神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-3-神经网络结构"><span class="toc-text">16.3 神经网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-4-常见两个网络"><span class="toc-text">16.4 常见两个网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#17-dqn思维决策-tensorflow"><span class="toc-text">17. DQN思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#17-1-代码主结构"><span class="toc-text">17.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-2-初始值"><span class="toc-text">17.2 初始值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-3-存储记忆"><span class="toc-text">17.3 存储记忆</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-4-选行为"><span class="toc-text">17.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-5-学习"><span class="toc-text">17.5 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-6-学习效果"><span class="toc-text">17.6 学习效果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-7-修改版的-dqn"><span class="toc-text">17.7 修改版的 DQN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#18-openai-gym环境库"><span class="toc-text">18. OpenAI gym环境库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#18-1-要点"><span class="toc-text">18.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-2-安装gym"><span class="toc-text">18.2 安装gym</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-3-cartpole例子"><span class="toc-text">18.3 CartPole例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-4-mountaincar例子"><span class="toc-text">18.4 MountainCar例子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#19-double-dqn-tensorflow"><span class="toc-text">19. Double DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#19-1-要点"><span class="toc-text">19.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-2-double-dqn算法"><span class="toc-text">19.2 Double DQN算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-3-更新方法"><span class="toc-text">19.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-4-记录-q-值"><span class="toc-text">19.4 记录 Q 值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-5-对比结果"><span class="toc-text">19.5 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#20-prioritized-experience-replay-dqn-tensorflow"><span class="toc-text">20. Prioritized Experience Replay(DQN)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#20-1-要点"><span class="toc-text">20.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-2-prioritized-replay算法"><span class="toc-text">20.2 Prioritized Replay算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-3-sumtree有效抽样"><span class="toc-text">20.3 SumTree有效抽样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-4-memory类"><span class="toc-text">20.4 Memory类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-5-更新方法"><span class="toc-text">20.5 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-6-对比结果"><span class="toc-text">20.6 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#21-dueling-dqn-tensorflow"><span class="toc-text">21. Dueling DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#21-1-要点"><span class="toc-text">21.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-2-dueling算法"><span class="toc-text">21.2 Dueling算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-3-更新方法"><span class="toc-text">21.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-4-对比结果"><span class="toc-text">21.4 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#22-什么是policy-gradients"><span class="toc-text">22. 什么是Policy Gradients</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#22-1-和以往的强化学习方法不同"><span class="toc-text">22.1 和以往的强化学习方法不同</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-2-更新不同之处"><span class="toc-text">22.2 更新不同之处</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-3-具体更新步骤"><span class="toc-text">22.3 具体更新步骤</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#23-policy-gradients算法更新-tensorflow"><span class="toc-text">23. Policy Gradients算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#23-1-要点"><span class="toc-text">23.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-2-算法"><span class="toc-text">23.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-3-算法代码形式"><span class="toc-text">23.3 算法代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#24-policy-gradients思维决策-tensorflow"><span class="toc-text">24. Policy Gradients思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#24-1-主要代码结构"><span class="toc-text">24.1 主要代码结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-2-初始化"><span class="toc-text">24.2 初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-3-建立policy神经网络"><span class="toc-text">24.3 建立Policy神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-4-选行为"><span class="toc-text">24.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-5-存储回合"><span class="toc-text">24.5 存储回合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-6-学习"><span class="toc-text">24.6 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#25-什么是actor-critic"><span class="toc-text">25. 什么是Actor Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#25-1-为什么要有actor和critic"><span class="toc-text">25.1 为什么要有Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-2-actor和critic"><span class="toc-text">25.2 Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-3-增加单步更新属性"><span class="toc-text">25.3 增加单步更新属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-4-改进版deep-deterministic-policy-gradient-ddpg"><span class="toc-text">25.4 改进版Deep Deterministic Policy Gradient(DDPG)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#26-actot-critic-tensorflow"><span class="toc-text">26. Actot Critic—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#26-1-要点"><span class="toc-text">26.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-2-算法"><span class="toc-text">26.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-3-代码主结构"><span class="toc-text">26.3 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-4-两者学习方式"><span class="toc-text">26.4 两者学习方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-5-每回合算法"><span class="toc-text">26.5 每回合算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#27-什么是deep-deterministic-policy-gradient-ddpg"><span class="toc-text">27. 什么是Deep Deterministic Policy Gradient(DDPG)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#27-1-拆分细讲"><span class="toc-text">27.1 拆分细讲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-2-deep和dqn"><span class="toc-text">27.2 Deep和DQN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-3-deterministic-policy-gradient"><span class="toc-text">27.3 Deterministic Policy Gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-4-ddpg神经网络"><span class="toc-text">27.4 DDPG神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#28-deep-deterministic-policy-gradient-ddpg-tensorflow"><span class="toc-text">28. Deep Deterministic Policy Gradient(DDPG)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#28-1-要点"><span class="toc-text">28.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-2-算法"><span class="toc-text">28.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-3-主结构"><span class="toc-text">28.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-5-actor-critic"><span class="toc-text">28.5 Actor Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-6-记忆库mmeory"><span class="toc-text">28.6 记忆库Mmeory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-7-每回合算法"><span class="toc-text">28.7 每回合算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-8-简化版代码"><span class="toc-text">28.8 简化版代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#29-什么是asynchronous-advantage-actor-critic-a3c"><span class="toc-text">29. 什么是Asynchronous Advantage Actor-Critic (A3C)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#29-1-平行宇宙"><span class="toc-text">29.1 平行宇宙</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-2-平行训练"><span class="toc-text">29.2 平行训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-3-多核训练"><span class="toc-text">29.3 多核训练</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#30-asynchronous-advantage-actor-critic-a3c-tensorflow"><span class="toc-text">30. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#30-1-要点"><span class="toc-text">30.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-2-算法"><span class="toc-text">30.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-3-主结构"><span class="toc-text">30.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-4-actor-critic网络"><span class="toc-text">30.4 Actor Critic网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-5-worker"><span class="toc-text">30.5 Worker</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-6-worker并行工作"><span class="toc-text">30.6 Worker并行工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-7-机械手臂"><span class="toc-text">30.7 机械手臂</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-8-multiprocessing-a3c"><span class="toc-text">30.8 multiprocessing+A3C</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#31-distributed-proximal-policy-optimization-dppo-tensorflow"><span class="toc-text">31. Distributed Proximal Policy Optimization (DPPO) —Tensorflow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#31-1-要点"><span class="toc-text">31.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-2-openai-和-deepmind-的-demo"><span class="toc-text">31.2 OpenAI 和 DeepMind 的 Demo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-3-算法"><span class="toc-text">31.3 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-4-简单的-ppo-主结构"><span class="toc-text">31.4 简单的 PPO 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-5-distributed-ppo"><span class="toc-text">31.5 Distributed PPO</span></a></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-4 i,
    .toc-level-4 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><div class='inner-toc'><h2>目录</h2><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-什么是强化学习-reinforcement-learning"><span class="toc-text">1. 什么是强化学习(Reinforcement Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-从无到有"><span class="toc-text">1.1 从无到有</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-虚拟老师"><span class="toc-text">1.2 虚拟老师</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-对比监督学习"><span class="toc-text">1.3 对比监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-rl算法"><span class="toc-text">1.4 RL算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-强化学习汇总"><span class="toc-text">2. 强化学习汇总</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-model-free和model-based"><span class="toc-text">2.1 Model-free和Model-based</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-基于概率和基于价值"><span class="toc-text">2.2 基于概率和基于价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-回合更新和单步更新"><span class="toc-text">2.3 回合更新和单步更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-在线学习和离线学习"><span class="toc-text">2.4 在线学习和离线学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-为什么用强化学习"><span class="toc-text">3. 为什么用强化学习?</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-强化学习介绍"><span class="toc-text">3.1 强化学习介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-模拟程序提前看"><span class="toc-text">3.2 模拟程序提前看</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-课程要求"><span class="toc-text">4. 课程要求</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-教程必备模块"><span class="toc-text">4.1 教程必备模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-快速了解强化学习"><span class="toc-text">4.2 快速了解强化学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-什么是q-learning"><span class="toc-text">5. 什么是Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-行为准则"><span class="toc-text">5.1 行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-q-learning决策"><span class="toc-text">5.2 Q-Learning决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-q-learning更新"><span class="toc-text">5.3 Q-Learning更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-q-learning整体算法"><span class="toc-text">5.4 Q-Learning整体算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-q-learning中的gamma"><span class="toc-text">5.5 Q-Learning中的Gamma</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-强化学习小例子"><span class="toc-text">6. 强化学习小例子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-要点"><span class="toc-text">6.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-预设值"><span class="toc-text">6.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-q表"><span class="toc-text">6.3 Q表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-定义动作"><span class="toc-text">6.4 定义动作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-环境反馈-s-r"><span class="toc-text">6.5 环境反馈 S_，R</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-环境更新"><span class="toc-text">6.6 环境更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-7-强化学习主循环"><span class="toc-text">6.7 强化学习主循环</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-8-q-table的演变"><span class="toc-text">6.8 Q-Table的演变</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-q-learning算法更新"><span class="toc-text">7. Q-Learning算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-要点"><span class="toc-text">7.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-算法"><span class="toc-text">7.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-算法的代码形式"><span class="toc-text">7.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-q-learning思维决策"><span class="toc-text">8. Q-Learning思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-代码主结构"><span class="toc-text">8.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-预设值"><span class="toc-text">8.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-决定行为"><span class="toc-text">8.3 决定行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-学习"><span class="toc-text">8.4 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-检测-state-是否存在"><span class="toc-text">8.5 检测 state 是否存在</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-什么是sarsa"><span class="toc-text">9. 什么是Sarsa</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-sarsa决策"><span class="toc-text">9.1 Sarsa决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-sarsa更新行为准则"><span class="toc-text">9.2 Sarsa更新行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-对比sarsa和q-learning算法"><span class="toc-text">9.3 对比Sarsa和Q-Learning算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-sarsa算法更新"><span class="toc-text">10. Sarsa算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-要点"><span class="toc-text">10.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-算法"><span class="toc-text">10.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-算法的代码形式"><span class="toc-text">10.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-sarsa思维决策"><span class="toc-text">11. Sarsa思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-1-代码主结构"><span class="toc-text">11.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-2-学习"><span class="toc-text">11.2 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-什么是sarsa-lambda"><span class="toc-text">12. 什么是Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-sarsa-n"><span class="toc-text">12.1 Sarsa(n)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-2-单步更新和回合更新"><span class="toc-text">12.2 单步更新和回合更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-有时迷茫"><span class="toc-text">12.3 有时迷茫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-4-lambda含义"><span class="toc-text">12.4 Lambda含义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-5-lambda取值"><span class="toc-text">12.5 Lambda取值</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-sarsa-lambda"><span class="toc-text">13. Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#13-1-要点"><span class="toc-text">13.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-2-代码主结构"><span class="toc-text">13.2 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-3-预设值"><span class="toc-text">13.3 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-4-检测state是否存在"><span class="toc-text">13.4 检测state是否存在</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-5-学习"><span class="toc-text">13.5 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14-什么是dqn"><span class="toc-text">14. 什么是DQN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#14-1-强化学习与神经网络"><span class="toc-text">14.1 强化学习与神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-2-神经网络的作用"><span class="toc-text">14.2 神经网络的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-3-更新神经网络"><span class="toc-text">14.3 更新神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-4-dqn两大利器"><span class="toc-text">14.4 DQN两大利器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-dqn算法更新-tensorflow"><span class="toc-text">15. DQN算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#15-1-要点"><span class="toc-text">15.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-2-算法"><span class="toc-text">15.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-3-算法的代码形式"><span class="toc-text">15.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#16-dqn神经网络-tensorflow"><span class="toc-text">16. DQN神经网络—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#16-1-要点"><span class="toc-text">16.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-2-两个神经网络"><span class="toc-text">16.2 两个神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-3-神经网络结构"><span class="toc-text">16.3 神经网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-4-常见两个网络"><span class="toc-text">16.4 常见两个网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#17-dqn思维决策-tensorflow"><span class="toc-text">17. DQN思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#17-1-代码主结构"><span class="toc-text">17.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-2-初始值"><span class="toc-text">17.2 初始值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-3-存储记忆"><span class="toc-text">17.3 存储记忆</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-4-选行为"><span class="toc-text">17.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-5-学习"><span class="toc-text">17.5 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-6-学习效果"><span class="toc-text">17.6 学习效果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-7-修改版的-dqn"><span class="toc-text">17.7 修改版的 DQN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#18-openai-gym环境库"><span class="toc-text">18. OpenAI gym环境库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#18-1-要点"><span class="toc-text">18.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-2-安装gym"><span class="toc-text">18.2 安装gym</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-3-cartpole例子"><span class="toc-text">18.3 CartPole例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-4-mountaincar例子"><span class="toc-text">18.4 MountainCar例子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#19-double-dqn-tensorflow"><span class="toc-text">19. Double DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#19-1-要点"><span class="toc-text">19.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-2-double-dqn算法"><span class="toc-text">19.2 Double DQN算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-3-更新方法"><span class="toc-text">19.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-4-记录-q-值"><span class="toc-text">19.4 记录 Q 值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-5-对比结果"><span class="toc-text">19.5 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#20-prioritized-experience-replay-dqn-tensorflow"><span class="toc-text">20. Prioritized Experience Replay(DQN)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#20-1-要点"><span class="toc-text">20.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-2-prioritized-replay算法"><span class="toc-text">20.2 Prioritized Replay算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-3-sumtree有效抽样"><span class="toc-text">20.3 SumTree有效抽样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-4-memory类"><span class="toc-text">20.4 Memory类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-5-更新方法"><span class="toc-text">20.5 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-6-对比结果"><span class="toc-text">20.6 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#21-dueling-dqn-tensorflow"><span class="toc-text">21. Dueling DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#21-1-要点"><span class="toc-text">21.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-2-dueling算法"><span class="toc-text">21.2 Dueling算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-3-更新方法"><span class="toc-text">21.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-4-对比结果"><span class="toc-text">21.4 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#22-什么是policy-gradients"><span class="toc-text">22. 什么是Policy Gradients</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#22-1-和以往的强化学习方法不同"><span class="toc-text">22.1 和以往的强化学习方法不同</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-2-更新不同之处"><span class="toc-text">22.2 更新不同之处</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-3-具体更新步骤"><span class="toc-text">22.3 具体更新步骤</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#23-policy-gradients算法更新-tensorflow"><span class="toc-text">23. Policy Gradients算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#23-1-要点"><span class="toc-text">23.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-2-算法"><span class="toc-text">23.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-3-算法代码形式"><span class="toc-text">23.3 算法代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#24-policy-gradients思维决策-tensorflow"><span class="toc-text">24. Policy Gradients思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#24-1-主要代码结构"><span class="toc-text">24.1 主要代码结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-2-初始化"><span class="toc-text">24.2 初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-3-建立policy神经网络"><span class="toc-text">24.3 建立Policy神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-4-选行为"><span class="toc-text">24.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-5-存储回合"><span class="toc-text">24.5 存储回合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-6-学习"><span class="toc-text">24.6 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#25-什么是actor-critic"><span class="toc-text">25. 什么是Actor Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#25-1-为什么要有actor和critic"><span class="toc-text">25.1 为什么要有Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-2-actor和critic"><span class="toc-text">25.2 Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-3-增加单步更新属性"><span class="toc-text">25.3 增加单步更新属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-4-改进版deep-deterministic-policy-gradient-ddpg"><span class="toc-text">25.4 改进版Deep Deterministic Policy Gradient(DDPG)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#26-actot-critic-tensorflow"><span class="toc-text">26. Actot Critic—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#26-1-要点"><span class="toc-text">26.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-2-算法"><span class="toc-text">26.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-3-代码主结构"><span class="toc-text">26.3 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-4-两者学习方式"><span class="toc-text">26.4 两者学习方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-5-每回合算法"><span class="toc-text">26.5 每回合算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#27-什么是deep-deterministic-policy-gradient-ddpg"><span class="toc-text">27. 什么是Deep Deterministic Policy Gradient(DDPG)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#27-1-拆分细讲"><span class="toc-text">27.1 拆分细讲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-2-deep和dqn"><span class="toc-text">27.2 Deep和DQN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-3-deterministic-policy-gradient"><span class="toc-text">27.3 Deterministic Policy Gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-4-ddpg神经网络"><span class="toc-text">27.4 DDPG神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#28-deep-deterministic-policy-gradient-ddpg-tensorflow"><span class="toc-text">28. Deep Deterministic Policy Gradient(DDPG)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#28-1-要点"><span class="toc-text">28.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-2-算法"><span class="toc-text">28.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-3-主结构"><span class="toc-text">28.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-5-actor-critic"><span class="toc-text">28.5 Actor Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-6-记忆库mmeory"><span class="toc-text">28.6 记忆库Mmeory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-7-每回合算法"><span class="toc-text">28.7 每回合算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-8-简化版代码"><span class="toc-text">28.8 简化版代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#29-什么是asynchronous-advantage-actor-critic-a3c"><span class="toc-text">29. 什么是Asynchronous Advantage Actor-Critic (A3C)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#29-1-平行宇宙"><span class="toc-text">29.1 平行宇宙</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-2-平行训练"><span class="toc-text">29.2 平行训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-3-多核训练"><span class="toc-text">29.3 多核训练</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#30-asynchronous-advantage-actor-critic-a3c-tensorflow"><span class="toc-text">30. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#30-1-要点"><span class="toc-text">30.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-2-算法"><span class="toc-text">30.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-3-主结构"><span class="toc-text">30.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-4-actor-critic网络"><span class="toc-text">30.4 Actor Critic网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-5-worker"><span class="toc-text">30.5 Worker</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-6-worker并行工作"><span class="toc-text">30.6 Worker并行工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-7-机械手臂"><span class="toc-text">30.7 机械手臂</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-8-multiprocessing-a3c"><span class="toc-text">30.8 multiprocessing+A3C</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#31-distributed-proximal-policy-optimization-dppo-tensorflow"><span class="toc-text">31. Distributed Proximal Policy Optimization (DPPO) —Tensorflow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#31-1-要点"><span class="toc-text">31.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-2-openai-和-deepmind-的-demo"><span class="toc-text">31.2 OpenAI 和 DeepMind 的 Demo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-3-算法"><span class="toc-text">31.3 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-4-简单的-ppo-主结构"><span class="toc-text">31.4 简单的 PPO 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-5-distributed-ppo"><span class="toc-text">31.5 Distributed PPO</span></a></li></ol></li></ol></div></p>
<p>本博客是学习了莫烦强化学习课程的总结，部分内容转载自莫烦的个人博客，在他的个人主页上 https://morvanzhou.github.io 上有很多关于机器学习的相关课程，且配有视频和文字。莫烦是一位我非常敬佩的博主，能够使用最浅显易懂语言让你了解各种模型的原理，有兴趣的作者可以自己看学习一下其他的课程。</p>
<p>所有代码在莫烦的 GitHub 中可以找到，这里放一下地址: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents</p>
<h1 id="1-什么是强化学习-reinforcement-learning">1. 什么是强化学习(Reinforcement Learning)</h1>
<p>强化学习是机器学习大家族中的一大类，使用强化学习能够让机器学着如何在环境中拿到高分，表现出优秀的成绩。而这些成绩背后却是他所付出的辛苦劳动，不断的试错，不断地尝试，累积经验，学习经验。</p>
<h2 id="1-1-从无到有">1.1 从无到有</h2>
<p><img src="https://img-blog.csdnimg.cn/20191210204551928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>强化学习是一类算法，是让计算机实现从一开始什么都不懂，脑袋里没有一点想法，通过不断地尝试，从错误中学习，最后找到规律，学会了达到目的的方法。这就是一个完整的强化学习过程，实际中的强化学习例子有很多。比如近期最有名的 AlphaGo，机器头一次在围棋场上战胜人类高手，让计算机自己学着玩经典游戏 Atari，这些都是让计算机在不断的尝试中更新自己的行为准则，从而一步步学会如何下好围棋，如何操控游戏得到高分。既然要让计算机自己学，那计算机通过什么来学习呢？</p>
<h2 id="1-2-虚拟老师">1.2 虚拟老师</h2>
<p><img src="https://img-blog.csdnimg.cn/20191210204737184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>原来计算机也需要一位虚拟的老师，这个老师比较吝啬，他不会告诉你如何移动，如何做决定，他为你做的事只有给你的行为打分，那我们应该以什么形式学习这些现有的资源，或者说怎么样只从分数中学习到我应该怎样做决定呢？很简单，我只需要记住那些高分，低分对应的行为，下次用同样的行为拿高分，并避免低分的行为。</p>
<p>比如老师会根据我的开心程度来打分，我开心时，可以得到高分，我不开心时得到低分。有了这些被打分的经验，我就能判断为了拿到高分，我应该选择一张开心的脸，避免选到伤心的脸，这也是强化学习的核心思想。可以看出在强化学习中，一种行为的分数是十分重要的。所以强化学习具有分数导向性。我们换一个角度来思考，这种分数导向性好比我们在监督学习中的正确标签。</p>
<h2 id="1-3-对比监督学习">1.3 对比监督学习</h2>
<p><img src="https://img-blog.csdnimg.cn/20191210204924115.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们知道监督学习，是已经有了数据和数据对应的正确标签，比如这样。监督学习就能学习出那些脸对应哪种标签。不过强化学习还要更进一步，一开始它并没有数据和标签。</p>
<p>他要通过一次次在环境中的尝试，获取这些数据和标签，然后再学习通过哪些数据能够对应哪些标签，通过学习到的这些规律，竟可能地选择带来高分的行为 (比如这里的开心脸)。这也就证明了在强化学习中，分数标签就是他的老师，他和监督学习中的老师也差不多。</p>
<h2 id="1-4-rl算法">1.4 RL算法</h2>
<p><img src="https://img-blog.csdnimg.cn/20191210205048606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>强化学习是一个大家族，它包含了很多种算法，我们也会——提到之中一些比较有名的算法，比如有通过行为的价值来选取特定行为的方法，包括使用表格学习的 Q-Learnin、Sarsa，使用神经网络学习的 Deep Q Network，还有直接输出行为的 Policy Gradients，又或者了解所处的环境，想象出一个虚拟的环境并从虚拟的环境中学习等等。</p>
<h1 id="2-强化学习汇总">2. 强化学习汇总</h1>
<p>了解强化学习中常用到的几种方法，以及他们的区别，对我们根据特定问题选择方法时很有帮助。强化学习是一个大家族，发展历史也不短，具有很多种不同方法。比如说比较知名的控制方法 Q-Learning、Policy Gradients，还有基于对环境的理解的 Model-based RL 等等。接下来我们通过分类的方式来了解他们的区别。</p>
<h2 id="2-1-model-free和model-based">2.1 Model-free和Model-based</h2>
<p><img src="https://img-blog.csdnimg.cn/20191210205404948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们可以将所有强化学习的方法分为理不理解所处环境，如果我们不尝试去理解环境，环境给了我们什么就是什么。我们就把这种方法叫做 Model-free，这里的 Model 就是用模型来表示环境，那理解了环境也就是学会了用一个模型来代表环境，所以这种就是 Model-based 方法。我们想象，现在环境就是我们的世界，我们的机器人正在这个世界里玩耍，他不理解这个世界是怎样构成的，也不理解世界对于他的行为会怎么样反馈。举个例子，他决定丢颗原子弹去真实的世界，结果把自己给炸死了，所有结果都是那么现实。不过如果采取的是 Model-based RL，机器人会通过过往的经验，先理解真实世界是怎样的，并建立一个模型来模拟现实世界的反馈，最后他不仅可以在现实世界中玩耍，也能在模拟的世界中玩耍，这样就没必要去炸真实世界，连自己也炸死了，他可以像玩游戏一样炸炸游戏里的世界，也保住了自己的小命。那我们就来说说这两种方式的强化学习各用那些方法吧。</p>
<p>Model-free 的方法有很多, 像 Q-Learning、Sarsa、Policy Gradients 都是从环境中得到反馈然后从中学习。而 Model-based RL 只是多了一道程序，为真实世界建模，也可以说他们都是 Model-free 的强化学习，只是 Model-based 多出了一个虚拟环境，我们不仅可以像 Model-free 那样在现实中玩耍，还能在游戏中玩耍，而玩耍的方式也都是 Model-free 中那些玩耍方式，最终 Model-based 还有一个杀手锏是 Model-free 超级羡慕的，那就是想象力。</p>
<p>Model-free 中，机器人只能按部就班，一步一步等待真实世界的反馈，再根据反馈采取下一步行动。而 Model-based 能通过想象来预判断接下来将要发生的所有情，然后选择这些想象情况中最好的那种，并依据这种情况来采取下一步的策略，这也就是围棋场上 AlphaGo 能够超越人类的原因。接下来，我们再来用另外一种分类方法将强化学习分为基于概率和基于价值。</p>
<h2 id="2-2-基于概率和基于价值">2.2 基于概率和基于价值</h2>
<p><img src="https://img-blog.csdnimg.cn/2019121020593729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>基于概率是强化学习中最直接的一种，他能通过感官分析所处的环境，直接输出下一步要采取的各种动作的概率，然后根据概率采取行动，所以每种动作都有可能被选中，只是可能性不同。而基于价值的方法输出则是所有动作的价值，我们会根据最高价值来选着动作，相比基于概率的方法，基于价值的决策部分更为铁定，毫不留情，就选价值最高的；而基于概率的，即使某个动作的概率最高，但是还是不一定会选到他。</p>
<p>我们现在说的动作都是一个一个不连续的动作，而对于选取连续的动作，基于价值的方法是无能为力的。我们却能用一个<strong>概率分布</strong>在连续动作中选取特定动作，这也是基于概率的方法的优点之一。那么这两类使用的方法又有哪些呢？</p>
<p>比如在基于概率这边，有 Policy Gradients，在基于价值这边有 Q-Learning、Sarsa 等。而且我们还能结合这两类方法的优势之处，创造更牛逼的一种方法，叫做 Actor-Critic。<code>actor</code> 会基于概率做出动作，而 <code>critic</code> 会对做出的动作给出动作的价值，这样就在原有的 Policy Gradients 上加速了学习过程。</p>
<h2 id="2-3-回合更新和单步更新">2.3 回合更新和单步更新</h2>
<p><img src="https://img-blog.csdnimg.cn/20191210210243643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>强化学习还能用另外一种方式分类，<strong>回合更新</strong>和<strong>单步更新</strong>，想象强化学习就是在玩游戏，游戏回合有开始和结束。回合更新指的是游戏开始后，我们要等待游戏结束，然后再总结这一回合中的所有转折点，再更新我们的行为准则。而单步更新则是在游戏进行中每一步都在更新，不用等待游戏的结束， 这样我们就能边玩边学习了。</p>
<p>再来说说方法，Monte-carlo Learning 和基础版的 Policy Gradients 等都是回合更新制，Q-Learning、 Sarsa、升级版的 Policy Gradients 等都是单步更新制。因为单步更新更有效率，所以现在大多方法都是基于单步更新。比如有的强化学习问题并不属于回合问题。</p>
<h2 id="2-4-在线学习和离线学习">2.4 在线学习和离线学习</h2>
<p><img src="https://img-blog.csdnimg.cn/2019121021045949.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>最后一种分类方式是<strong>在线学习</strong>和<strong>离线学习</strong>，所谓在线学习，就是指我必须本人在场，并且一定是本人边玩边学习，而离线学习是你可以选择自己玩，也可以选择看着别人玩，通过看别人玩来学习别人的行为准则，离线学习 同样是从过往的经验中学习，但是这些过往的经历没必要是自己的经历，任何人的经历都能被学习。或者我也不必要边玩边学习，我可以白天先存储下来玩耍时的记忆，然后晚上通过离线学习来学习白天的记忆。那么每种学习的方法又有哪些呢？</p>
<p>最典型的在线学习就是 Sarsa 了，还有一种优化 Sarsa 的算法，叫做 Sarsa-Lambda，最典型的离线学习就是 Q-Learning，后来人也根据离线学习的属性，开发了更强大的算法，比如让计算机学会玩电动的 Deep-Q-Network。</p>
<p>这就是我们从各种不同的角度来对比了强化学习中的多种算法。</p>
<h1 id="3-为什么用强化学习">3. 为什么用强化学习?</h1>
<h2 id="3-1-强化学习介绍">3.1 强化学习介绍</h2>
<p><strong>强化学习</strong>(Reinforcement Learning) 是一个机器学习大家族中的分支，由于近些年来的技术突破，和<strong>深度学习</strong> (Deep Learning) 的整合使得强化学习有了进一步的运用。比如让计算机学着玩游戏，AlphaGo 挑战世界围棋高手，都是强化学习在行的事。强化学习也是让你的程序从对当前环境完全陌生，成长为一个在环境中游刃有余的高手。</p>
<p>这些教程的教学，不依赖于任何强化学习的 Python 模块。因为强化学习的复杂性、多样，到现在还没有比较好的统一化模块。不过我们还是能用最基础的方法编出优秀的强化学习程序!</p>
<h2 id="3-2-模拟程序提前看">3.2 模拟程序提前看</h2>
<p>下面是其中莫烦强化学习教程中一些模拟视频:</p>
<ul>
<li>
<p>Maze：https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20sarsa_lambda.mp4</p>
</li>
<li>
<p>Cartpole：https://morvanzhou.github.io/static/results/reinforcement-learning/cartpole%20dqn.mp4</p>
</li>
<li>
<p>Mountain Car：https://morvanzhou.github.io/static/results/reinforcement-learning/mountaincar%20dqn.mp4</p>
</li>
</ul>
<h1 id="4-课程要求">4. 课程要求</h1>
<h2 id="4-1-教程必备模块">4.1 教程必备模块</h2>
<p>强化学习有一些现成的模块可以使，但是那些模块并不全面，而且强化学习很依赖与你给予的学习环境。对于不同学习环境的强化学，可能 RL 的代码就不同。所以我们要抱着以不变应万变的心态，用基础的模块，从基础学起。懂了原理，再复杂的环境也不在话下。</p>
<p>所以用到的模块和对应的教程:</p>
<ul>
<li>Numpy, Pandas (必学), 用于学习的数据处理</li>
<li>Matplotlib (可学), 偶尔会用来呈现误差曲线什么的</li>
<li>Tkinter (可学), 你可以自己用它来编写模拟环境</li>
<li>Tensorflow (可学), 后面实现神经网络与强化学习结合的时候用到</li>
<li>OpenAI gym (可学), 提供了很多现成的模拟环境</li>
</ul>
<h2 id="4-2-快速了解强化学习">4.2 快速了解强化学习</h2>
<p>莫烦制作了每种强化学习对应的简介视频(有趣的机器学习)，大家可以只花很少的时间来观看了解这些学习方法的不同之处. 有了一定概念和基础，我们在这套教材里实现起来就容易多了。而且不懂的时候也能只花很少的时间回顾就。</p>
<h1 id="5-什么是q-learning">5. 什么是Q-Learning</h1>
<p>今天我们会来说说强化学习中一个很有名的算法— Q-learning。</p>
<h2 id="5-1-行为准则">5.1 行为准则</h2>
<p><img src="https://img-blog.csdnimg.cn/20191209231339444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="写作业OR看电视"></p>
<p>我们做事情都会有一个自己的行为准则，比如小时候爸妈常说“不写完作业就不准看电视”。所以我们在 写作业的这种状态下，好的行为就是继续写作业，直到写完它，我们还可以得到奖励。不好的行为就是没写完就跑去看电视了，被爸妈发现，后果很严重。小时候这种事情做多了，也就变成我们不可磨灭的记忆。这和我们要提到的 Q-Learning 有什么关系呢？原来 Q-Learning 也是一个决策过程，和小时候的这种情况差不多。我们举例说明。</p>
<p>假设现在我们处于写作业的状态而且我们以前并没有尝试过写作业时看电视，所以现在我们有两种选择: 1) 继续写作业；2) 跑去看电视。因为以前没有被罚过，所以我选看电视，然后现在的状态变成了看电视, 我又选了继续看电视，接着我还是看电视，最后爸妈回家，发现我没写完作业就去看电视了，狠狠地惩罚了我一次。我也深刻地记下了这一次经历，并在我的脑海中将 “没写完作业就看电视” 这种行为更改为负面行为，我们在看看 Q-Learning 根据很多这样的经历是如何来决策的吧。</p>
<h2 id="5-2-q-learning决策">5.2 Q-Learning决策</h2>
<p><img src="https://img-blog.csdnimg.cn/20191209235013646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>假设我们的行为准则已经学习好了，现在我们处于状态 <code>s1</code> ，我在写作业，我有两个行为 <code>a1</code> 和 <code>a2</code>，分别是看电视和写作业。根据我的经验，在这种 <code>s1</code> 状态下，<code>a2</code> 写作业带来的潜在奖励要比 <code>a1</code> 看电视高，这里的潜在奖励我们可以用一个有关于 <code>s</code> 和 <code>a</code> 的 <code>Q 表格</code>代替。在我的记忆 <code>Q表格</code> 中，<code>Q(s1, a1)=-2</code> 要小于 <code>Q(s1, a2)=1</code>，所以我们判断要选择 <code>a2</code> 作为下一个行为。现在我们的状态更新成 <code>s2</code> ，我们还是有两个同样的选择，重复上面的过程，在行为准则 <code>Q 表</code>中寻找 <code>Q(s2, a1)</code>和 <code>Q(s2, a2</code>) 的值，并比较他们的大小，选取较大的一个。接着根据 <code>a2</code> 我们到达 <code>s3</code> 并在此重复上面的决策过程。Q-Learning 的方法也就是这样决策的。看完决策，我看在来研究一下这张行为准则 Q 表是通过什么样的方式更改和提升的。</p>
<h2 id="5-3-q-learning更新">5.3 Q-Learning更新</h2>
<p><img src="https://img-blog.csdnimg.cn/20191209233020278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>所以我们回到之前的流程，根据 Q 表的估计，因为在 <code>s1</code> 中，<code>a2</code> 的值比较大，通过之前的决策方法，我们在 <code>s1</code> 采取了 <code>a2</code>，并到达 <code>s2</code>，这时我们开始更新用于决策的 Q 表。接着我们并没有在实际中采取任何行为，而是再想象自己在 <code>s2</code> 上采取了每种行为，分别看看两种行为哪一个的 Q 值大，比如说 <code>Q(s2, a2)</code> 的值比 <code>Q(s2, a1)</code> 的大，所以我们把大的 <code>Q(s2, a2)</code> 乘上一个衰减值 <code>gamma</code> (比如是 0.9) 并加上到达 <code>s2</code> 时所获取的奖励 <code>R</code> (这里还没有获取到我们的棒棒糖，所以奖励为 0)，因为会获取实实在在的奖励 <code>R</code>， 我们将这个作为我现实中 <code>Q(s1, a2)</code> 的值，但是我们之前是根据 Q 表估计 <code>Q(s1, a2)</code> 的值。所以有了<strong>现实</strong>和<strong>估计值</strong>, 我们就能更新 <code>Q(s1, a2)</code> ，根据<strong>估计与现实的差距</strong>，将这个差距乘以一个学习效率 <code>alpha</code> 累加上老的 <code>Q(s1, a2)</code> 的值变成新的值。但时刻记住, 我们虽然用 <code>maxQ(s2)</code> 估算了一下 <code>s2</code> 状态，但还没有在 <code>s2</code> 做出任何的行为，<code>s2</code> 的行为决策要等到更新完了以后再重新另外做。这就是 <code>off-policy</code> 的 Q-Learning 是如何决策和学习优化决策的过程。</p>
<h2 id="5-4-q-learning整体算法">5.4 Q-Learning整体算法</h2>
<p><img src="https://img-blog.csdnimg.cn/20191209233154685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这一张图概括了我们之前所有的内容。这也是 Q-Learning 的算法，每次更新我们都用到了 <code>Q 现实</code>和 <code>Q 估计</code>，而且 Q-Learning 的迷人之处就是 在 <code>Q(s1, a2) 现实</code>中, 也包含了一个 <code>Q(s2)</code> 的最大估计值，将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实，很奇妙吧。最后我们来说说这套算法中一些参数的意义。<code>Epsilon greedy</code> 是用在决策上的一种策略，比如 <code>epsilon=0.9</code> 时, 就说明有 <strong>90%</strong> 的情况我会按照 Q 表的最优值选择行为，<strong>10%</strong> 的时间使用随机选行为。<code>alpha</code> 是学习率，来决定这次的误差有多少是要被学习的，<code>alpha</code> 是一个小于1 的数。<code>gamma</code> 是对未来 reward 的衰减值。我们可以这样想象。</p>
<h2 id="5-5-q-learning中的gamma">5.5 Q-Learning中的Gamma</h2>
<p><img src="https://img-blog.csdnimg.cn/20191209233227569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们重写一下 <code>Q(s1)</code> 的公式，将 <code>Q(s2)</code> 拆开，因为 <code>Q(s2)</code> 可以像 <code>Q(s1)</code> 一样，是关于 <code>Q(s3)</code> 的，所以可以写成这样，然后以此类推，不停地这样写下去，最后就能写成这样。可以看出 <code>Q(s1)</code> 是有关于之后所有的奖励，但这些奖励正在衰减，离 <code>s1</code> 越远的状态衰减越严重。不好理解？行，我们想象 Q-Learning 的机器人天生近视眼，<code>gamma=1</code> 时，机器人有了一副合适的眼镜，在 <code>s1</code> 看到的 Q 是未来没有任何衰变的奖励，也就是机器人能清清楚楚地看到之后所有步的全部价值，但是当 <code>gamma=0</code>，近视机器人没了眼镜，只能摸到眼前的 reward, 同样也就只在乎最近的大奖励，如果 <code>gamma</code> 从 0 变到 1，眼镜的度数由浅变深，对远处的价值看得越清楚，所以机器人渐渐变得有远见，不仅仅只看眼前的利益，也为自己的未来着想。</p>
<h1 id="6-强化学习小例子">6. 强化学习小例子</h1>
<h2 id="6-1-要点">6.1 要点</h2>
<p>这一次我们会用 tabular Q-Learning 的方法实现一个小例子，例子的环境是一个一维世界，在世界的右边有宝藏，探索者只要得到宝藏尝到了甜头，然后以后就记住了得到宝藏的方法，这就是他用强化学习所学习到的行为。</p>
<h2 id="6-2-预设值">6.2 预设值</h2>
<p>这一次需要的模块和参数设置:</p>
<pre><code class="language-python">import numpy as np
import pandas as pd
import time

N_STATES = 6  # 1维世界的宽度
ACTIONS = ['left', 'right']  # 探索者的可用动作
EPSILON = 0.9  # 贪婪度 greedy
ALPHA = 0.1    # 学习率
GAMMA = 0.9    # 奖励递减值
MAX_EPISODES = 13  # 最大回合数
FRESH_TIME = 0.3   # 移动间隔时间

</code></pre>
<h2 id="6-3-q表">6.3 Q表</h2>
<p>对于 tabular Q-Learning，我们必须将所有的 <code>Q-values</code> (行为值) 放在 <code>q_table</code> 中，更新 <code>q_table</code> 也是在更新他的行为准则。 <code>q_table</code> 的 <code>index</code> 是所有对应的 <code>state</code> (探索者位置)，<code>columns</code> 是对应的 <code>action</code> (探索者行为)。</p>
<pre><code class="language-python">def build_q_table(n_states, actions):
    table = pd.DataFrame(
        np.zeros((n_states, len(actions))),     # q_table 全 0 初始
        columns=actions,    # columns 对应的是行为名称
    )
    return table

# q_table:
"""
   left  right
0   0.0    0.0
1   0.0    0.0
2   0.0    0.0
3   0.0    0.0
4   0.0    0.0
5   0.0    0.0
"""

</code></pre>
<h2 id="6-4-定义动作">6.4 定义动作</h2>
<p>接着定义探索者是如何挑选行为的，这时我们引入 <code>epsilon greedy</code> 的概念。因为在初始阶段，随机的探索环境往往比固定的行为模式要好，所以这也是累积经验的阶段，我们希望探索者不会那么贪婪(greedy)。所以 <code>EPSILON</code> 就是用来控制贪婪程度的值。<code>EPSILON</code> 可以随着探索时间不断提升(越来越贪婪)，不过在这个例子中，我们就固定成 <code>EPSILON = 0.9</code>，90% 的时间是选择最优策略，10% 的时间来探索。</p>
<pre><code class="language-python"># 在某个 state 地点, 选择行为
def choose_action(state, q_table):
    state_actions = q_table.iloc[state, :]  # 选出这个 state 的所有 action 值
    if (np.random.uniform() &gt; EPSILON) or (state_actions.all() == 0):  # 非贪婪 or 或者这个 state 还没有探索过
        action_name = np.random.choice(ACTIONS)
    else:
        action_name = state_actions.argmax()    # 贪婪模式
    return action_name

</code></pre>
<h2 id="6-5-环境反馈-s-r">6.5 环境反馈 S_，R</h2>
<p>做出行为后，环境也要给我们的行为一个反馈，反馈出下个 <code>state (S_)</code> 和 在上个 <code>state (S)</code> 做出 <code>action (A)</code> 所得到的 <code>reward (R)</code>。 这里定义的规则就是，只有当 <code>o</code> 移动到了 <code>T</code>，探索者才会得到唯一的一个奖励，奖励值 <code>R=1</code>，其他情况都没有奖励。</p>
<pre><code class="language-python">def get_env_feedback(S, A):
    # This is how agent will interact with the environment
    if A == 'right':    # move right
        if S == N_STATES - 2:   # terminate
            S_ = 'terminal'
            R = 1
        else:
            S_ = S + 1
            R = 0
    else:   # move left
        R = 0
        if S == 0:
            S_ = S  # reach the wall
        else:
            S_ = S - 1
    return S_, R

</code></pre>
<h2 id="6-6-环境更新">6.6 环境更新</h2>
<p>接下来就是环境的更新，不用细看。</p>
<pre><code class="language-python">def update_env(S, episode, step_counter):
    # This is how environment be updated
    env_list = ['-']*(N_STATES-1) + ['T']   # '-----T' our environment
    if S == 'terminal':
        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)
        print('\r{}'.format(interaction), end='')
        time.sleep(2)
        print('\r                                ', end='')
    else:
        env_list[S] = 'o'
        interaction = ''.join(env_list)
        print('\r{}'.format(interaction), end='')
        time.sleep(FRESH_TIME)

</code></pre>
<h2 id="6-7-强化学习主循环">6.7 强化学习主循环</h2>
<p>最重要的地方就在这里，你定义的 RL 方法都在这里体现。在之后的教程中，我们会更加详细得讲解 RL 中的各种方法，下面的内容大家大概看看就行，这节内容不用仔细研究。</p>
<p><img src="https://img-blog.csdnimg.cn/2019121011194063.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="RL主要算法"></p>
<pre><code class="language-python">def rl():
    q_table = build_q_table(N_STATES, ACTIONS)  # 初始 q table
    for episode in range(MAX_EPISODES):     # 回合
        step_counter = 0
        S = 0   # 回合初始位置
        is_terminated = False   # 是否回合结束
        update_env(S, episode, step_counter)    # 环境更新
        while not is_terminated:

            A = choose_action(S, q_table)   # 选行为
            S_, R = get_env_feedback(S, A)  # 实施行为并得到环境的反馈
            q_predict = q_table.loc[S, A]    # 估算的(状态-行为)值
            if S_ != 'terminal':
                # 实际的(状态-行为)值 (回合没结束)
                q_target = R + GAMMA * q_table.iloc[S_, :].max()   
            else:
                q_target = R  #  实际的(状态-行为)值 (回合结束)
                is_terminated = True  # terminate this episode

            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # q_table 更新
            S = S_  # 探索者移动到下一个 state

            update_env(S, episode, step_counter+1)  # 环境更新

            step_counter += 1
    return q_table

</code></pre>
<p>写好所有的评估和更新准则后，我们就能开始训练了，把探索者丢到环境中，让它自己去玩吧。</p>
<pre><code class="language-python">if __name__ == "__main__":
    q_table = rl()
    print('\r\nQ-table:\n')
    print(q_table)

</code></pre>
<h2 id="6-8-q-table的演变">6.8 Q-Table的演变</h2>
<p>Q-Learning 学习过程其实就是不断去更新 Q 表的过程，实际训练时你会发现没带迭代到达终点的步数越来越少，其实就是下一次的迭代会基于上一次的 Q 表来做判断，这样模型就更能够判断准确。</p>
<p>这里训练 10 次，看一下到达终点的步数已经 Q 表的更新过程:</p>
<pre><code>Episode 1: total_steps = 38
   left  right
0   0.0    0.0
1   0.0    0.0
2   0.0    0.0
3   0.0    0.0
4   0.0    0.1
5   0.0    0.0
==================================================
Episode 2: total_steps = 22
   left  right
0   0.0  0.000
1   0.0  0.000
2   0.0  0.000
3   0.0  0.009
4   0.0  0.190
5   0.0  0.000
==================================================
Episode 3: total_steps = 9
   left  right
0   0.0  0.00000
1   0.0  0.00000
2   0.0  0.00081
3   0.0  0.02520
4   0.0  0.27100
5   0.0  0.00000

==================================================
Episode 4: total_steps = 5
   left  right
0   0.0  0.000000
1   0.0  0.000073
2   0.0  0.002997
3   0.0  0.047070
4   0.0  0.343900
5   0.0  0.000000

==================================================
Episode 5: total_steps = 7
      left  right
0  0.00000  0.000007
1  0.00000  0.000572
2  0.00003  0.006934
3  0.00000  0.073314
4  0.00000  0.409510
5  0.00000  0.000000

==================================================
Episode 6: total_steps = 5
      left  right
0  0.00000  0.000057
1  0.00000  0.001138
2  0.00003  0.012839
3  0.00000  0.102839
4  0.00000  0.468559
5  0.00000  0.000000

==================================================
Episode 7: total_steps = 5
      left  right
0  0.00000  0.000154
1  0.00000  0.002180
2  0.00003  0.020810
3  0.00000  0.134725
4  0.00000  0.521703
5  0.00000  0.000000

==================================================
Episode 8: total_steps = 5
      left  right
0  0.00000  0.000335
1  0.00000  0.003835
2  0.00003  0.030854
3  0.00000  0.168206
4  0.00000  0.569533
5  0.00000  0.000000

==================================================
Episode 9: total_steps = 5
      left  right
0  0.00000  0.000647
1  0.00000  0.006228
2  0.00003  0.042907
3  0.00000  0.202643
4  0.00000  0.612580
5  0.00000  0.000000

==================================================
Episode 10: total_steps = 5
      left  right
0  0.00000  0.001142
1  0.00000  0.009467
2  0.00003  0.056855
3  0.00000  0.237511
4  0.00000  0.651322
5  0.00000  0.000000

==================================================
Q-table:
      left     right
0  0.00000  0.001142
1  0.00000  0.009467
2  0.00003  0.056855
3  0.00000  0.237511
4  0.00000  0.651322
5  0.00000  0.000000

</code></pre>
<p>上述结果模型基本训练了5次左右，就已经学习到了相关的经验了，后面基本上每次只需要5步左右就可以到达终点了。</p>
<p>再来看看 Q 表的更新过程:</p>
<ul>
<li>第1步: 只更新了 <code>s4</code> 的 <code>right</code> 的 Q 值，其中未更新的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mtext>right</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(s4, \text{right})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">right</span></span><span class="mclose">)</span></span></span></span> 就是 <code>q_predict</code> 为 0，这里 <code>(1 + 0.9*max(s5,:))</code> 表示 <code>q_target</code>。</li>
</ul>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mtext>right</mtext><mo stretchy="false">)</mo><mo>←</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mtext>right</mtext><mo stretchy="false">)</mo><mo>+</mo><mn>0.1</mn><mo>×</mo><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo>×</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>s</mi><mn>5</mn><mo separator="true">,</mo><mo>:</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>−</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mtext>right</mtext><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mspace linebreak="newline"></mspace><mo>=</mo><mn>0</mn><mo>+</mo><mn>0.1</mn><mo>×</mo><mo stretchy="false">[</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo>×</mo><mn>0</mn><mo stretchy="false">]</mo><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">Q(s4, \text{right}) \leftarrow Q(s4, \text{right}) + 0.1 \times [(1 + 0.9 \times max(s5,:)) - Q(s4, \text{right})] 
\\ =0 + 0.1 \times [1+0.9 \times 0] = 0.1
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">right</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">right</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">right</span></span><span class="mclose">)</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span></span></span></span></span></p>
<ul>
<li>第2步: 在第1步基础上更新了 Q表，这一步会用到上述已经更新好的 <code>Q(s4)</code> 的值，同时会更新 <code>Q(s4)</code> 和 <code>Q(s3)</code>
<ul>
<li>首先更新的是 <code>Q(s3)</code>，这一步是当前状态是 <code>s3</code>，且一步动作是 <code>right</code>。</li>
<li>然后更新的是 <code>Q(s4)</code>，这一步是当前状态是 <code>s4</code>，且一步动作是 <code>right</code>。</li>
</ul>
</li>
</ul>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mo>=</mo><mo>=</mo><mo>=</mo><mo>=</mo><mtext>更新</mtext><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mn>3</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mo>=</mo><mo>=</mo><mo>=</mo><mo>=</mo><mspace linebreak="newline"></mspace><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>3</mn><mo separator="true">,</mo><mtext>right</mtext><mo stretchy="false">)</mo><mo>←</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>3</mn><mo separator="true">,</mo><mtext>right</mtext><mo stretchy="false">)</mo><mo>+</mo><mn>0.1</mn><mo>×</mo><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mn>0</mn><mo>+</mo><mn>0.</mn><mo>×</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mo>:</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>−</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>3</mn><mo separator="true">,</mo><mtext>right</mtext><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mspace linebreak="newline"></mspace><mo>=</mo><mn>0</mn><mo>+</mo><mn>0.1</mn><mo>×</mo><mo stretchy="false">[</mo><mn>0</mn><mo>+</mo><mn>0.9</mn><mo>×</mo><mn>0.1</mn><mo>−</mo><mn>0</mn><mo stretchy="false">]</mo><mo>=</mo><mn>0.009</mn><mspace linebreak="newline"></mspace><mo>=</mo><mo>=</mo><mo>=</mo><mo>=</mo><mo>=</mo><mtext>更新</mtext><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mn>4</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mo>=</mo><mo>=</mo><mo>=</mo><mo>=</mo><mspace linebreak="newline"></mspace><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mtext>right</mtext><mo stretchy="false">)</mo><mo>←</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mtext>right</mtext><mo stretchy="false">)</mo><mo>+</mo><mn>0.1</mn><mo>×</mo><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo>×</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>s</mi><mn>5</mn><mo separator="true">,</mo><mo>:</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>−</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mtext>right</mtext><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mspace linebreak="newline"></mspace><mo>=</mo><mn>0.1</mn><mo>+</mo><mn>0.1</mn><mo>×</mo><mo stretchy="false">[</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><mo>×</mo><mn>0</mn><mo>−</mo><mn>0.1</mn><mo stretchy="false">]</mo><mo>=</mo><mn>0.190</mn></mrow><annotation encoding="application/x-tex">=====更新Q(s_3)=====
\\ Q(s3, \text{right}) \leftarrow Q(s3, \text{right}) + 0.1 \times [(0 + 0. \times max(s4,:)) - Q(s3, \text{right})] 
\\ =0 + 0.1 \times [0+0.9 \times 0.1 - 0] = 0.009 
\\ =====更新Q(s_4)=====
\\ Q(s4, \text{right}) \leftarrow Q(s4, \text{right}) + 0.1 \times [(1 + 0.9 \times max(s5,:)) - Q(s4, \text{right})] 
\\ =0.1 + 0.1 \times [1+0.9 \times 0 - 0.1] = 0.190
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord cjk_fallback">更</span><span class="mord cjk_fallback">新</span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">right</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">right</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mopen">(</span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">right</span></span><span class="mclose">)</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">0</span><span class="mord">9</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord cjk_fallback">更</span><span class="mord cjk_fallback">新</span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">right</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">right</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">right</span></span><span class="mclose">)</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mord">9</span><span class="mord">0</span></span></span></span></span></p>
<p>这里可能有人会问，这个 <code>Q(s4)=0.190</code> 的值是基于 <code>s4</code> 状态下向=右走更新的值，为什么 <code>s4</code> 的状态不会选择采取 <code>left</code> 的动作跳回 <code>s3</code>么？如果选择往左走，<code>Q(s4)</code> 的值又会怎么变化？</p>
<p>显然在 <code>s4</code> 的状态下，是有可能选择往左走的，但是概率是 10%，90% 的情况会选择往右走，因为此时 <code>Q(s4,right)</code> 已经有相应的价值分了。下面演算一下如果 <code>s4</code> 状态下 10% 概率下选择往走时 <code>Q(s4, left)</code> 的值。</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mtext>left</mtext><mo stretchy="false">)</mo><mo>←</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mtext>left</mtext><mo stretchy="false">)</mo><mo>+</mo><mn>0.1</mn><mo>×</mo><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mn>0</mn><mo>+</mo><mn>0.9</mn><mo>×</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>s</mi><mn>3</mn><mo separator="true">,</mo><mo>:</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>−</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mn>4</mn><mo separator="true">,</mo><mtext>left</mtext><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mspace linebreak="newline"></mspace><mo>=</mo><mn>0</mn><mo>+</mo><mn>0.1</mn><mo>×</mo><mo stretchy="false">[</mo><mn>0</mn><mo>+</mo><mn>0.9</mn><mo>×</mo><mn>0.1</mn><mo>−</mo><mn>0</mn><mo stretchy="false">]</mo><mo>=</mo><mn>0.009</mn></mrow><annotation encoding="application/x-tex">Q(s4, \text{left}) \leftarrow Q(s4, \text{left}) + 0.1 \times [(0 + 0.9 \times max(s3,:)) - Q(s4, \text{left})] 
\\ = 0 + 0.1\times[0+0.9 \times 0.1 - 0] = 0.009
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">left</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">left</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mopen">(</span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">left</span></span><span class="mclose">)</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">0</span><span class="mord">9</span></span></span></span></span></p>
<p>其他的步骤也都是基于这种演算过程，大家可以自己算一下。</p>
<h1 id="7-q-learning算法更新">7. Q-Learning算法更新</h1>
<h2 id="7-1-要点">7.1 要点</h2>
<p>上次我们知道了 RL 之中的 Q-Learning 方法是在做什么事，今天我们就来说说一个更具体的例子。让探索者学会走迷宫，黄色的是天堂 (<code>reward=1</code>)，黑色的地狱(<code>reward=-1</code>)。大多数 RL 是由 reward 导向的，所以定义 reward 是 RL 中比较重要的一点。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20q.mp4</p>
<h2 id="7-2-算法">7.2 算法</h2>
<p><img src="https://img-blog.csdnimg.cn/20191210133923830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="主要公式"></p>
<p>整个算法就是一直不断更新 Q table 里的值，然后再根据新的值来判断要在某个 state 采取怎样的 action。Q-Learning 是一个 <code>off-policy</code> 的算法，因为里面的 <code>max</code> action 让 Q-table 的更新可以不基于正在经历的经验(可以是现在学习着很久以前的经验，甚至是学习他人的经验)。不过这一次的例子, 我们没有运用到 <code>off-policy</code>，而是把 Q-Learning 用在了 <code>on-policy</code> 上，也就是现学现卖，将现在经历的直接当场学习并运用。<code>On-policy</code> 和 <code>off-policy</code> 的差别我们会在之后的 <a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-1-DQN1/" target="_blank" rel="noopener">Deep Q network (off-policy)</a> 学习中见识到。而之后的教程也会讲到一个 <code>on-policy</code> (Sarsa) 的形式，我们之后再对比。</p>
<h2 id="7-3-算法的代码形式">7.3 算法的代码形式</h2>
<p>首先我们先 <code>import</code> 两个模块，<code>maze_env</code> 是我们的环境模块，已经编写好了。大家可以直接莫烦的GitHub中<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/2_Q_Learning_maze/maze_env.py" target="_blank" rel="noopener">下载</a>，<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境。 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<ul>
<li>导入自定义模块</li>
</ul>
<pre><code class="language-python">from maze_env import Maze
from RL_brain import QLearningTable

</code></pre>
<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Q-Learning 最重要的迭代更新部分啦。</p>
<pre><code class="language-python">def update():
    # 学习 100 回合
    for episode in range(100):
        # 初始化 state 的观测值
        observation = env.reset()

        while True:
            # 更新可视化环境
            env.render()

            # RL 大脑根据 state 的观测值挑选 action
            action = RL.choose_action(str(observation))

            # 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值,
            # reward 和 done (是否是掉下地狱或者升上天堂)
            observation_, reward, done = env.step(action)

            # RL 从这个序列 (state, action, reward, state_) 中学习
            RL.learn(str(observation), action, reward, str(observation_))

            # 将下一个 state 的值传到下一次循环
            observation = observation_

            # 如果掉下地狱或者升上天堂, 这回合就结束了
            if done:
                break

    # 结束游戏并关闭窗口
    print('game over')
    env.destroy()

if __name__ == "__main__":
    # 定义环境 env 和 RL 方式
    env = Maze()
    RL = QLearningTable(actions=list(range(env.n_actions)))

    # 开始可视化环境 env
    env.after(100, update)
    env.mainloop()

</code></pre>
<h1 id="8-q-learning思维决策">8. Q-Learning思维决策</h1>
<h2 id="8-1-代码主结构">8.1 代码主结构</h2>
<p>与上回不一样的地方是，我们将要以一个 <code>class</code> 形式定义 Q-Learning，并把这种 tabular Q-Learning 方法叫做 <code>QLearningTable</code>。</p>
<pre><code class="language-python">class QLearningTable:
    # 初始化
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):

    # 选行为
    def choose_action(self, observation):

    # 学习更新参数
    def learn(self, s, a, r, s_):

    # 检测 state 是否存在
    def check_state_exist(self, state):

</code></pre>
<h2 id="8-2-预设值">8.2 预设值</h2>
<p>初始的参数意义不会在这里提及了，请参考这个快速了解通道 <a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-3-tabular-q2/#" target="_blank" rel="noopener">机器学习系列-Q learning</a>。</p>
<pre><code class="language-python">import numpy as np
import pandas as pd


class QLearningTable:
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        self.actions = actions  # actions list
        self.lr = learning_rate # 学习率
        self.gamma = reward_decay   # 奖励衰减
        self.epsilon = e_greedy     # 贪婪度
        # 初始 q_table
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)   
        
</code></pre>
<h2 id="8-3-决定行为">8.3 决定行为</h2>
<p>这里是定义如何根据所在的 <code>state</code>，或者是在这个 <code>state</code> 上的 观测值 (observation) 来决策。</p>
<pre><code class="language-python">def choose_action(self, observation):
    # 检测本 state 是否在 q_table 中存在(见后面标题内容)
	self.check_state_exist(observation) 

    # 选择 action
    if np.random.uniform() &lt; self.epsilon:  # 选择 Q value 最高的 action
        state_action = self.q_table.loc[observation, :]
        # 同一个 state, 可能会有多个相同的 Q action value, 所以我们乱序一下
        action = np.random.choice(state_action[state_action == np.max(state_action)].index)
	else:  # 随机选择 action
		action = np.random.choice(self.actions)

	return action

</code></pre>
<h2 id="8-4-学习">8.4 学习</h2>
<p>同上一个简单的 Q-Learning 例子一样，我们根据是否是 <code>terminal</code> state (回合终止符) 来判断应该如何更新 <code>q_table</code>。更新的方式是不是很熟悉呢:</p>
<pre><code class="language-python">update = self.lr * (q_target - q_predict)
</code></pre>
<p>这可以理解成神经网络中的更新方式，<code>学习率 * (真实值 - 预测值)</code>。 将判断误差传递回去，有着和神经网络更新的异曲同工之处。</p>
<pre><code class="language-python">def learn(self, s, a, r, s_):
    self.check_state_exist(s_)  # 检测 q_table 中是否存在 s_ (见后面标题内容)
    q_predict = self.q_table.loc[s, a]
    if s_ != 'terminal':
        # 下个 state 不是 终止符
        q_target = r + self.gamma * self.q_table.loc[s_, :].max()  
    else:
        q_target = r  # 下个 state 是终止符
    # 更新对应的 state-action 值
    self.q_table.loc[s, a] += self.lr * (q_target - q_predict)

</code></pre>
<h2 id="8-5-检测-state-是否存在">8.5 检测 state 是否存在</h2>
<p>这个功能就是检测 <code>q_table</code> 中有没有当前 <code>state</code> 的步骤了，如果还没有当前 <code>state</code>，那我我们就插入一组全 0 数据，当做这个 <code>state</code> 的所有 <code>action</code> 初始 values。</p>
<pre><code class="language-python">def check_state_exist(self, state):
    if state not in self.q_table.index:
        # append new state to q table
        self.q_table = self.q_table.append(
            pd.Series(
                [0]*len(self.actions),
                index=self.q_table.columns,
                name=state,
            )
        )

</code></pre>
<h1 id="9-什么是sarsa">9. 什么是Sarsa</h1>
<p>今天我们会来说说强化学习中一个和 Q-Learning 类似的算法，叫做 Sarsa。</p>
<p><img src="https://img-blog.csdnimg.cn/20191210212458422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>在强化学习中 Sarsa 和 Q-Learning 及其类似，这节内容会基于之前我们所讲的 Q-Learning。所以还不熟悉 Q-Learning 的朋友们，请前往我制作的 Q-Learning 简介(知乎专栏)。我们会对比 Q-Learning，来看看 Sarsa 是特殊在哪些方面。和上次一样，我们还是使用写作业和看电视这个例子。没写完作业去看电视被打，写完了作业有糖吃。</p>
<h2 id="9-1-sarsa决策">9.1 Sarsa决策</h2>
<p><img src="https://img-blog.csdnimg.cn/20191210212638770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Sarsa 的决策部分和 Q-Learning 一模一样，因为我们使用的是 Q 表的形式决策，所以我们会在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩。但是不同的地方在于 Sarsa 的更新方式是不一样的。</p>
<h2 id="9-2-sarsa更新行为准则">9.2 Sarsa更新行为准则</h2>
<p><img src="https://img-blog.csdnimg.cn/20191210212726800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>同样，我们会经历正在写作业的状态 <code>s1</code>，然后再挑选一个带来最大潜在奖励的动作 <code>a2</code>，这样我们就到达了继续写作业状态 <code>s2</code>，而在这一步，如果你用的是 Q-Learning，你会观看一下在 <code>s2</code> 上选取哪一个动作会带来最大的奖励，但是在真正要做决定，却不一定会选取到那个带来最大奖励的动作，Q-Learning 在这一步只是估计了一下接下来的动作值。而 Sarsa 是实践派，他说到做到，在 <code>s2</code> 这一步估算的动作也是接下来要做的动作。所以 <code>Q(s1, a2)</code> 现实的计算值，我们也会稍稍改，去掉<code>maxQ</code> ，取而代之的是在 <code>s2</code> 上我们实实在在选取的 a2 的 Q 值。最后像 Q-Learning 一样，求出现实和估计的差距，并更新 Q 表里的 <code>Q(s1, a2)</code>。</p>
<h2 id="9-3-对比sarsa和q-learning算法">9.3 对比Sarsa和Q-Learning算法</h2>
<p><img src="https://img-blog.csdnimg.cn/2019121021312130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>从算法来看，这就是他们两最大的不同之处。因为 Sarsa 是说到做到型，所以我们也叫他 on-policy，在线学，学着自己在做的事情。而 Q-Learning 是说到但并不一定做到，所以它也叫作 Off-policy，离线学习。而因为有了 <code>maxQ</code>, Q-Learning 也是一个特别勇敢的算法。</p>
<h1 id="10-sarsa算法更新">10. Sarsa算法更新</h1>
<h2 id="10-1-要点">10.1 要点</h2>
<p>这次我们用同样的迷宫例子来实现 RL 中另一种和 Q-Learning 类似的算法，叫做 <code>Sarsa</code>(state-action-reward-state-action)。我们从这一个简称可以了解到，Sarsa 的整个循环都将是在一个路径上，也就是 on-policy，下一个 <code>state_</code>，和下一个 <code>action_</code> 将会变成他真正采取的 <code>action</code> 和 <code>state</code>。和 Q-Learning 的不同之处就在。Q-Learning 的下个一个 <code>state_</code> 和 <code>action_</code> 在算法更新的时候都还是不确定的(off-policy)。而 Sarsa 的 <code>state_</code> 和 <code>action_</code> 在这次算法更新的时候已经确定好了(on-policy)。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20sarsa.mp4</p>
<h2 id="10-2-算法">10.2 算法</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113142800375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>整个算法还是一直不断更新 <code>Q table</code> 里的值，然后再根据新的值来判断要在某个 <code>state</code> 采取怎样的 <code>action</code>。不过与 Q-Learning 不同之处:</p>
<ul>
<li>他在当前 <code>state</code> 已经想好了 <code>state</code> 对应的 <code>action</code>，而且想好了 下一个 <code>state_</code> 和下一个 <code>action_</code> (Q-Learning 还没有想好下一个 <code>action_</code>)</li>
<li>更新 <code>Q(s,a)</code> 的时候基于的是下一个 <code>Q(s_, a_)</code> (Q-Learning 是基于 <code>maxQ(s_)</code>)</li>
</ul>
<p>这种不同之处使得 Sarsa 相对于 Q-Learning, 更加的胆小。因为 Q-Learning 永远都是想着 <code>maxQ</code> 最大化，因为这个 <code>maxQ</code> 而变得贪婪，不考虑其他非 <code>maxQ</code> 的结果。我们可以理解成 Q-Learning 是一种贪婪、大胆、勇敢的算法。对于错误，死亡并不在乎。而 Sarsa 是一种保守的算法，他在乎每一步决策，对于错误和死亡比较铭感。这一点我们会在可视化的部分看出他们的不同。两种算法都有他们的好处，比如在实际中，你比较在乎机器的损害，用一种保守的算法，在训练时就能减少损坏的次数。</p>
<h2 id="10-3-算法的代码形式">10.3 算法的代码形式</h2>
<p>首先我们先 <code>import</code> 两个模块，<code>maze_env</code> 是我们的环境模块，已经编写好，大家可以直接在这里下载，<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境。</p>
<pre><code class="language-python">from maze_env import Maze
from RL_brain import SarsaTable
</code></pre>
<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Sarsa 最重要的迭代更新部分啦:</p>
<pre><code class="language-python">def update():
    for episode in range(100):
        # 初始化环境
        observation = env.reset()

        # Sarsa 根据 state 观测选择行为
        action = RL.choose_action(str(observation))

        while True:
            # 刷新环境
            env.render()

            # 在环境中采取行为, 获得下一个 state_ (obervation_), reward, 和是否终止
            observation_, reward, done = env.step(action)

            # 根据下一个 state (obervation_) 选取下一个 action_
            action_ = RL.choose_action(str(observation_))

            # 从 (s, a, r, s, a) 中学习, 更新 Q_tabel 的参数 ==&gt; Sarsa
            RL.learn(str(observation), action, reward, str(observation_), action_)

            # 将下一个当成下一步的 state (observation) and action
            observation = observation_
            action = action_

            # 终止时跳出循环
            if done:
                break

    # 大循环完毕
    print('game over')
    env.destroy()

if __name__ == "__main__":
    env = Maze()
    RL = SarsaTable(actions=list(range(env.n_actions)))

    env.after(100, update)
    env.mainloop()
</code></pre>
<h1 id="11-sarsa思维决策">11. Sarsa思维决策</h1>
<p>接着上节内容，我们来实现 <code>RL_brain</code> 的 <code>SarsaTable</code> 部分，这也是 RL 的大脑部分，负责决策和思考。</p>
<h2 id="11-1-代码主结构">11.1 代码主结构</h2>
<p>和之前定义 Q-Learning 中的 <code>QLearningTable</code> 一样，因为使用 <code>tabular</code> 方式的 <code>Sarsa</code> 和 <code>Qlearning</code> 的相似度极高:</p>
<pre><code class="language-python">class SarsaTable:
    # 初始化 (与之前一样)
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):

    # 选行为 (与之前一样)
    def choose_action(self, observation):

    # 学习更新参数 (有改变)
    def learn(self, s, a, r, s_):

    # 检测 state 是否存在 (与之前一样)
    def check_state_exist(self, state):
</code></pre>
<p>我们甚至可以定义一个 主 class <code>RL</code>，然后将 <code>QLearningTable</code> 和 <code>SarsaTable</code> 作为主 class <code>RL</code> 的衍生，这个主 <code>RL</code> 可以这样定义。所以我们将之前的 <code>__init__</code>、<code>check_state_exist</code>、 <code>choose_action</code>、<code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<pre><code class="language-python">import numpy as np
import pandas as pd


class RL(object):
    def __init__(self, action_space, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        pass  # 和 QLearningTable 中的代码一样

    def check_state_exist(self, state):
        pass  # 和 QLearningTable 中的代码一样

    def choose_action(self, observation):
        pass  # 和 QLearningTable 中的代码一样

    def learn(self, *args):
        pass  # 每种的都有点不同, 所以用 pass
</code></pre>
<p>如果是这样定义父类的 <code>RL</code> class，通过继承关系，那之子类 <code>QLearningTable</code> class 就能简化成这样:</p>
<pre><code class="language-python">class QLearningTable(RL):  # 继承了父类 RL
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        super(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)  # 表示继承关系

    def learn(self, s, a, r, s_):  # learn 的方法在每种类型中有不一样, 需重新定义
        self.check_state_exist(s_)
        q_predict = self.q_table.loc[s, a]
        if s_ != 'terminal':
            q_target = r + self.gamma * self.q_table.loc[s_, :].max()
        else:
            q_target = r
        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)
</code></pre>
<h2 id="11-2-学习">11.2 学习</h2>
<p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。这就是我们所有的 <code>SarsaTable</code> 于父类 <code>RL</code> 不同之处的代码。</p>
<pre><code class="language-python">class SarsaTable(RL):   # 继承 RL class

    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        super(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)  # 表示继承关系

    def learn(self, s, a, r, s_, a_):
        self.check_state_exist(s_)
        q_predict = self.q_table.loc[s, a]
        if s_ != 'terminal':
            # q_target 基于选好的 a_ 而不是 Q(s_) 的最大值
            q_target = r + self.gamma * self.q_table.loc[s_, a_]
        else:
            q_target = r  # 如果 s_ 是终止符
        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # 更新 q_table
</code></pre>
<h1 id="12-什么是sarsa-lambda">12. 什么是Sarsa-lambda</h1>
<p>今天我们会来说说强化学习中基于 Sarsa 的一种提速方法，叫做 sarsa-lambda。</p>
<h2 id="12-1-sarsa-n">12.1 Sarsa(n)</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113144051310.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>通过上个视频的介绍，我们知道这个 Sarsa 的算法是一种在线学习法 on-policy。但是这个 <code>lambda</code> 到底是什么。其实吧，Sarsa 是一种单步更新法，在环境中每走一步，更新一次自己的行为准则，我们可以在这样的 Sarsa 后面打一个括号，说他是 <code>Sarsa(0)</code>，因为他等走完这一步以后直接更新行为准则。如果延续这种想法，走完这步，再走一步，然后再更新，我们可以叫他 <code>Sarsa(1)</code>。同理，如果等待回合完毕我们一次性再更新呢，比如这回合我们走了 <code>n</code> 步，那我们就叫 <code>Sarsa(n)</code>。为了统一这样的流程，我们就有了一个 <code>lambda</code> 值来代替我们想要选择的步数，这也就是 <code>Sarsa(lambda)</code> 的由来。我们看看最极端的两个例子，对比单步更新和回合更新，看看回合更新的优势在哪里。</p>
<h2 id="12-2-单步更新和回合更新">12.2 单步更新和回合更新</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113144324841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>虽然我们每一步都在更新，但是在没有获取宝藏的时候，我们现在站着的这一步也没有得到任何更新，也就是直到获取宝藏时，我们才为获取到宝藏的上一步更新为: 这一步很好，和获取宝藏是有关联的，而之前为了获取宝藏所走的所有步都被认为和获取宝藏没关系。回合更新虽然我要等到这回合结束，才开始对本回合所经历的所有步都添加更新，但是这所有的步都是和宝藏有关系的，都是为了得到宝藏需要学习的步，所以每一个脚印在下回合被选则的几率又高了一些。在这种角度来看，回合更新似乎会有效率一些。</p>
<h2 id="12-3-有时迷茫">12.3 有时迷茫</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113145123114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们看看这种情况，还是使用单步更新的方法在每一步都进行更新，但是同时记下之前的寻宝之路。你可以想像，每走一步，插上一个小旗子，这样我们就能清楚的知道除了最近的一步，找到宝物时还需要更新哪些步了。不过，有时候情况可能没有这么乐观。开始的几次，因为完全没有头绪，我可能在原地打转了很久，然后才找到宝藏，那些重复的脚步真的对我拿到宝藏很有必要吗? 答案我们都知道。所以Sarsa(lambda)就来拯救你啦。</p>
<h2 id="12-4-lambda含义">12.4 Lambda含义</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113145309421.png" alt="在这里插入图片描述"></p>
<p>其实 <code>lambda</code> 就是一个衰变值，他可以让你知道离奖励越远的步可能并不是让你最快拿到奖励的步，所以我们想象我们站在宝藏的位置，回头看看我们走过的寻宝之，离宝藏越近的脚印越看得清，远处的脚印太渺小，我们都很难看清，那我们就索性记下离宝藏越近的脚印越重要，越需要被好好的更新。和之前我们提到过的<strong>奖励衰减值</strong> <code>gamma</code> 一样，<code>lambda</code> 是脚步衰减，都是一个在 0 和 1 之间的数。</p>
<h2 id="12-5-lambda取值">12.5 Lambda取值</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113145447804.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>当 <code>lambda</code> 取 0，就变成了 Sarsa 的单步更新，当 <code>lambda</code> 取 1，就变成了回合更新，对所有步更新的力度都是一样。当 <code>lambda</code> 在 0 和 1 之间，取值越大，离宝藏越近的步更新力度越大。这样我们就不用受限于单步更新的每次只能更新最近的一步，我们可以更有效率的更新所有相关步了。</p>
<h1 id="13-sarsa-lambda">13. Sarsa-lambda</h1>
<h2 id="13-1-要点">13.1 要点</h2>
<p>Sarsa-lambda 是基于 Sarsa 方法的升级版，他能更有效率地学习到怎么样获得好的 <code>reward</code>。如果说 Sarsa 和 Q-Learning 都是每次获取到 <code>reward</code>，只更新获取到 <code>reward</code> 的前一步。那 Sarsa-lambda 就是更新获取到 <code>reward</code> 的前 <code>lambda</code> 步。<code>lambda</code> 是在 <code>[0, 1]</code> 之间取值。</p>
<ul>
<li>
<p>如果 <code>lambda=0</code>，Sarsa-lambda 就是 Sarsa，只更新获取到 <code>reward</code> 前经历的最后一步。</p>
</li>
<li>
<p>如果 <code>lambda=1</code>，Sarsa-lambda 更新的是 获取到 <code>reward</code> 前所有经历的步。</p>
</li>
</ul>
<p>这样解释起来有点抽，还是建议大家观看我制作的什么是 Sarsa-lambda 短视频，用动画展示具体的区别。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20sarsa_lambda.mp4</p>
<h2 id="13-2-代码主结构">13.2 代码主结构</h2>
<p>使用 <code>SarsaLambdaTable</code> 在算法更新迭代的部分，是和之前的 <code>SarsaTable</code> 一样的，所以这一节，我们没有算法更新部分，直接变成 思维决策部分。</p>
<pre><code class="language-python">class SarsaLambdaTable:
    # 初始化 (有改变)
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):

    # 选行为 (与之前一样)
    def choose_action(self, observation):

    # 学习更新参数 (有改变)
    def learn(self, s, a, r, s_):

    # 检测 state 是否存在 (有改变)
    def check_state_exist(self, state):
</code></pre>
<p>同样, 我们选择继承的方，将 <code>SarsaLambdaTable</code> 继承到 <code>RL</code>, 所以我们将之前的 <code>__init__</code>、<code>check_state_exist</code>、<code>choose_action</code>、<code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<p>算法的相应更改请参考这个:</p>
<p><img src="https://img-blog.csdnimg.cn/20200113170454981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="13-3-预设值">13.3 预设值</h2>
<p>在预设值当中，我们添加了 <code>trace_decay=0.9</code> 这个就是 <code>lambda</code> 的值了。这个值将会使得拿到 <code>reward</code> 前的每一步都有价值。</p>
<pre><code class="language-python">class SarsaLambdaTable(RL): # 继承 RL class
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):
        super(SarsaLambdaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)
        # 后向观测算法, eligibility trace.
        self.lambda_ = trace_decay
        # 空的 eligibility trace 表
        self.eligibility_trace = self.q_table.copy()
        
</code></pre>
<h2 id="13-4-检测state是否存在">13.4 检测state是否存在</h2>
<p><code>check_state_exist</code> 和之前的是高度相似的，唯一不同的地方是我们考虑了 <code>eligibility_trace</code>。</p>
<pre><code class="language-python">class SarsaLambdaTable(RL): # 继承 RL class
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):
        # ...
    def check_state_exist(self, state):
        if state not in self.q_table.index:
            # append new state to q table
            to_be_append = pd.Series(
                    [0] * len(self.actions),
                    index=self.q_table.columns,
                    name=state,
                )
            self.q_table = self.q_table.append(to_be_append)

            # also update eligibility trace
            self.eligibility_trace = self.eligibility_trace.append(to_be_append)
</code></pre>
<h2 id="13-5-学习">13.5 学习</h2>
<p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaLambdaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。这就是我们所有的 <code>SarsaLambdaTable</code> 于父类 <code>RL</code> 不同之处的代码。</p>
<pre><code class="language-python">class SarsaLambdaTable(RL): # 继承 RL class
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):
        # ...
    def check_state_exist(self, state):
        # ...
    def learn(self, s, a, r, s_, a_):
        # 这部分和 Sarsa 一样
        self.check_state_exist(s_)
        q_predict = self.q_table.ix[s, a]
        if s_ != 'terminal':
            q_target = r + self.gamma * self.q_table.ix[s_, a_]
        else:
            q_target = r
        error = q_target - q_predict

        # 这里开始不同:
        # 对于经历过的 state-action, 我们让他+1, 
        # 证明他是得到 reward 路途中不可或缺的一环
        self.eligibility_trace.ix[s, a] += 1

        # Q table 更新
        self.q_table += self.lr * error * self.eligibility_trace

        # 随着时间衰减 eligibility trace 的值, 
        # 离获取 reward 越远的步, 他的"不可或缺性"越小
        self.eligibility_trace *= self.gamma * self.lambda_
</code></pre>
<p>除了图中和上面代码这种更新方式，还有一种会更加有效率。我们可以将上面的这一步替换成下面这样:</p>
<pre><code class="language-python"># 上面代码中的方式:
self.eligibility_trace.ix[s, a] += 1

# 更有效的方式:
self.eligibility_trace.ix[s, :] *= 0
self.eligibility_trace.ix[s, a] = 1

</code></pre>
<p>他们两的不同之处可以用这张图来概括:</p>
<p><img src="https://img-blog.csdnimg.cn/2020011317095162.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这是针对于一个 <code>state-action</code> 值按经历次数的变。最上面是经历 <code>state-action</code> 的时间点，第二张图是使用这种方式所带来的 “不可或缺性值”:</p>
<pre><code class="language-python">self.eligibility_trace.ix[s, a] += 1
</code></pre>
<p>下面图是使用这种方法带来的 “不可或缺性值”:</p>
<pre><code class="language-python">self.eligibility_trace.ix[s, :] *= 0
self.eligibility_trace.ix[s, a] = 1
</code></pre>
<p>实验证明选择下面这种方法会有更好的效果。大家也可以自己玩一玩，试试两种方法的不同表现。</p>
<p>最后不要忘了，<code>eligibility_trace</code> 只是记录每个回合的每一步, 新回合开始的时候需要将 <code>Trace</code> 清零。</p>
<pre><code class="language-python">for episode in range(100):
    # ...

    # 新回合, 清零
    RL.eligibility_trace *= 0

    while True: # 开始回合
        pass
</code></pre>
<h1 id="14-什么是dqn">14. 什么是DQN</h1>
<p>今天我们会来说说强化学习中的一种强大武器，Deep Q Network 简称为 DQN。Google Deep mind 团队就是靠着这 DQN 使计算机玩电动玩得比我们还厉害。</p>
<h2 id="14-1-强化学习与神经网络">14.1 强化学习与神经网络</h2>
<p><img src="https://img-blog.csdnimg.cn/20191211095231908.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>之前我们所谈论到的强化学习方法都是比较传统的方式，而如今随着机器学习在日常生活中的各种应用，各种机器学习方法也在融汇、合并、升级。而我们今天所要探讨的强化学习则是这么一种融合了神经网络和 Q-Learning 的方法，名字叫做 Deep-Q-Network。这种新型结构是为什么被提出来呢？原来，传统的表格形式的强化学习有这样一个瓶颈。</p>
<h2 id="14-2-神经网络的作用">14.2 神经网络的作用</h2>
<p><img src="https://img-blog.csdnimg.cn/2019121109570476.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们使用表格来存储每一个状态 <code>state</code>，和在这个 <code>state</code> 每个行为 <code>action</code> 所拥有的 Q 值。而当今问题是在太复杂，状态可以多到比天上的星星还多(比如下围棋)。如果全用表格来存储它们，恐怕我们的计算机有再大的内存都不够，而且每次在这么大的表格中搜索对应的状态也是一件很耗时的事。不过，在机器学习中有一种方法对这种事情很在行，那就是神经网络。我们可以将<strong>状态</strong>和<strong>动作</strong>当成神经网络的输入，然后经过神经网络分析后得到动作的 Q 值，这样我们就没必要在表格中记录 Q 值，而是直接使用神经网络生成 Q 值。还有一种形式是这样，我们也能只输入状态值，输出所有的动作值，然后按照 Q-Learning 的原则，直接选择拥有最大值的动作当做下一步要做的动作。我们可以想象，神经网络接受外部的信息，相当于眼睛鼻子耳朵收集信息，然后通过大脑加工输出每种动作的值，最后通过强化学习的方式选择动作。</p>
<h2 id="14-3-更新神经网络">14.3 更新神经网络</h2>
<p><img src="https://img-blog.csdnimg.cn/2019121110002138.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>接下来我们基于第二种神经网络来分析，我们知道，神经网络是要被训练才能预测出准确的值。那在强化学习中，神经网络是如何被训练的呢？首先，我们需要 <code>a1</code> 和 <code>a2</code> 正确的 Q 值，这个 Q 值我们就用之前在 Q-Learning 中的 <strong>Q 现实</strong>来代替。同样我们还需要一个 <strong>Q 估计</strong>来实现神经网络的更新。所以神经网络的参数就是老的 <strong>NN 参数</strong> 加上 <strong>学习率 alpha</strong> 乘以 <strong>Q 现实 和 Q 估计 的差距</strong>。我们整理一下。</p>
<p><img src="https://img-blog.csdnimg.cn/20191211100342564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们通过 NN 预测出 <code>Q(s2, a1)</code> 和 <code>Q(s2,a2)</code> 的值，这就是 Q 估计。然后我们选取 Q 估计中最大值的动作来换取环境中的奖励 reward。而 <strong>Q 现实</strong>中也包含从神经网络分析出来的两个 <strong>Q 估计值</strong>，不过这个 <strong>Q 估计</strong>是针对于下一步在 <code>s'</code> 的估计。最后再通过刚刚所说的算法更新神经网络中的参数，但是这并不是 DQN 会玩电动的根本原因。还有两大因素支撑着 DQN 使得它变得无比强大，这两大因素就是 <strong>Experience replay</strong> 和 <strong>Fixed Q-targets</strong>。</p>
<h2 id="14-4-dqn两大利器">14.4 DQN两大利器</h2>
<p><img src="https://img-blog.csdnimg.cn/20191211100609210.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>简单来说，DQN 有一个记忆库用于学习之前的经历。在之前的简介影片中提到过，Q-Learning 是一种 <code>off-policy</code> 离线学习法，它能学习当前经历着的，也能学习过去经历过的，甚至是学习别人的经历。所以每次 DQN 更新的时候，我们都可以随机抽取一些之前的经历进行学习。随机抽取这种做法打乱了经历之间的相关，也使得神经网络更新更有效率。<code>Fixed Q-targets</code> 也是一种打乱相关性的机理，如果使用 <code>fixed Q-targets</code>，我们就会在 DQN 中使用到两个<strong>结构相同</strong>但<strong>参数不同</strong>的神经网络，预测 <strong>Q 估计</strong> 的神经网络具备最新的参数，而预测 <strong>Q 现实</strong> 的神经网络使用的参数则是很久以前的。有了这两种提升手段，DQN 才能在一些游戏中超越人类。</p>
<h1 id="15-dqn算法更新-tensorflow">15. DQN算法更新—TensorFlow</h1>
<h2 id="15-1-要点">15.1 要点</h2>
<p>Deep Q Network 的简称叫 DQN，是将 Q-Learning 的优势和 Neural networks 结合了。如果我们使用 tabular Q-Learning，对于每一个 <code>state</code> 和 <code>action</code> 我们都需要存放在一张 <code>q_table</code> 的表中。如果像现实生活中，情况可就比那个迷宫的状况复杂多了，我们有千千万万个 <code>state</code>，如果将这千万个 <code>state</code> 的值都放在表中，受限于我们计算机硬件，这样从表中获取数据，更新数据是没有效率的，这就是 DQN 产生的原因了。我们可以使用神经网络来估算 这个 <code>state</code> 的值，这样就不需要一张表了。</p>
<p>这次的教程我们还是基于熟悉的迷宫环境，重点在实现 DQN 算法，之后我们再拿着做好的 DQN 算法去跑其他更有意思的环境。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20dqn.mp4</p>
<h2 id="15-2-算法">15.2 算法</h2>
<p><img src="https://img-blog.csdnimg.cn/20191211101344134.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>整个算法乍看起来很复杂，不过我们拆分一下，就变简单了。也就是个 Q-Learning 主框架上加了些装饰。</p>
<p>这些装饰包括:</p>
<ul>
<li>记忆库 (用于重复学习)</li>
<li>神经网络计算 Q 值</li>
<li>暂时冻结 <code>q_target</code> 参数 (切断相关性)</li>
</ul>
<h2 id="15-3-算法的代码形式">15.3 算法的代码形式</h2>
<p>接下来我们对应上面的算法，来实现主循环。首先 <code>import</code> 所需模块。</p>
<pre><code class="language-python">from maze_env import Maze
from RL_brain import DeepQNetwork

</code></pre>
<p>下面的代码，就是 DQN 于环境交互最重要的部分。</p>
<pre><code class="language-python">def run_maze():
    step = 0    # 用来控制什么时候学习
    for episode in range(300):
        # 初始化环境
        observation = env.reset()

        while True:
            # 刷新环境
            env.render()

            # DQN 根据观测值选择行为
            action = RL.choose_action(observation)

            # 环境根据行为给出下一个 state, reward, 是否终止
            observation_, reward, done = env.step(action)

            # DQN 存储记忆
            RL.store_transition(observation, action, reward, observation_)

            # 控制学习起始时间和频率 (先累积一些记忆再开始学习)
            if (step &gt; 200) and (step % 5 == 0):
                RL.learn()

            # 将下一个 state_ 变为 下次循环的 state
            observation = observation_

            # 如果终止, 就跳出循环
            if done:
                break
            step += 1   # 总步数

    # end of game
    print('game over')
    env.destroy()


if __name__ == "__main__":
    env = Maze()
    RL = DeepQNetwork(env.n_actions, env.n_features,
                      learning_rate=0.01,
                      reward_decay=0.9,
                      e_greedy=0.9,
                      # 每 200 步替换一次 target_net 的参数
                      replace_target_iter=200,
                      memory_size=2000, # 记忆上限
                      # output_graph=True  # 是否输出 tensorboard 文件
                      )
    env.after(100, run_maze)
    env.mainloop()
    RL.plot_cost()  # 观看神经网络的误差曲线
</code></pre>
<p>下一节我们会来讲解 Deep Q Network 这种算法具体要怎么编。</p>
<h1 id="16-dqn神经网络-tensorflow">16. DQN神经网络—TensorFlow</h1>
<h2 id="16-1-要点">16.1 要点</h2>
<p>接着上节内容，这节我们使用 Tensorflow，来搭建 DQN 当中的神经网络部分 (用来预测 Q 值)。</p>
<h2 id="16-2-两个神经网络">16.2 两个神经网络</h2>
<p>为了使用 Tensorflow 来实现 DQN，比较推荐的方式是搭建两个神经网，<code>target_net</code> 用于预测 <code>q_target</code> 值，它不会及时更新参数。<code>eval_net</code> 用于预测 <code>q_eval</code>，这个神经网络拥有最新的神经网络参数。不过这两个神经网络结构是完全一样的，只是里面的参数不一样。在这个短视频里，能找到我们为什么要建立两个不同参数的神经网络。</p>
<p><img src="https://img-blog.csdnimg.cn/20191211103056750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="16-3-神经网络结构">16.3 神经网络结构</h2>
<p>因为 DQN 的结构相比之前所讲的内容都不一样，所以我们不使用继承来实现这次的功能。这次我们创建一个 <code>DeepQNetwork</code> 的 <code>class</code>，以及神经网络部分的功能。下次再说强化学习的更新部分。</p>
<pre><code class="language-python">class DeepQNetwork:
    # 建立神经网络
    def _build_net(self):
</code></pre>
<h2 id="16-4-常见两个网络">16.4 常见两个网络</h2>
<p>两个神经网络是为了固定住一个神经网络 (<code>target_net</code>) 的参数，<code>target_net</code> 是 <code>eval_net</code> 的一个历史版本，拥有 <code>eval_net</code> 很久之前的一组参数，而且这组参数被固定一段时间，然后再被 <code>eval_net</code> 的新参数所替换。而 <code>eval_net</code> 是不断在被提升的，所以是一个可以被训练的网络 <code>trainable=True</code>，而 <code>target_net</code> 的 <code>trainable=False</code>。</p>
<pre><code class="language-python">class DeepQNetwork:
    def _build_net(self):
        # -------------- 创建 eval 神经网络, 及时提升参数 --------------
        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # 用来接收 observation
        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target') # 用来接收 q_target 的值, 这个之后会通过计算得到
        with tf.variable_scope('eval_net'):
            # c_names(collections_names) 是在更新 target_net 参数时会用到
            c_names, n_l1, w_initializer, b_initializer = \
                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, \
                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers

            # eval_net 的第一层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope('l1'):
                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)
                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)
                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)

            # eval_net 的第二层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope('l2'):
                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)
                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)
                self.q_eval = tf.matmul(l1, w2) + b2

        with tf.variable_scope('loss'): # 求误差
            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))
        with tf.variable_scope('train'):    # 梯度下降
            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)

        # ---------------- 创建 target 神经网络, 提供 target Q ---------------------
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # 接收下个 observation
        with tf.variable_scope('target_net'):
            # c_names(collections_names) 是在更新 target_net 参数时会用到
            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]

            # target_net 的第一层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope('l1'):
                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)
                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)
                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)

            # target_net 的第二层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope('l2'):
                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)
                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)
                self.q_next = tf.matmul(l1, w2) + b2
</code></pre>
<h1 id="17-dqn思维决策-tensorflow">17. DQN思维决策—TensorFlow</h1>
<p>接着上节内容，我们来定义 <code>DeepQNetwork</code> 的决策和思考部分。</p>
<h2 id="17-1-代码主结构">17.1 代码主结构</h2>
<p>定义完上次的神经网络部分以后，这次我们来定义其他部分，包括:</p>
<pre><code class="language-python">class DeepQNetwork:
    # 上次的内容
    def _build_net(self):

    # 这次的内容:
    # 初始值
    def __init__(self):

    # 存储记忆
    def store_transition(self, s, a, r, s_):

    # 选行为
    def choose_action(self, observation):

    # 学习
    def learn(self):

    # 看看学习效果 (可选)
    def plot_cost(self):
</code></pre>
<h2 id="17-2-初始值">17.2 初始值</h2>
<pre><code class="language-python">class DeepQNetwork:
    def __init__(
            self,
            n_actions,
            n_features,
            learning_rate=0.01,
            reward_decay=0.9,
            e_greedy=0.9,
            replace_target_iter=300,
            memory_size=500,
            batch_size=32,
            e_greedy_increment=None,
            output_graph=False,
    ):
        self.n_actions = n_actions
        self.n_features = n_features
        self.lr = learning_rate
        self.gamma = reward_decay
        self.epsilon_max = e_greedy     # epsilon 的最大值
        self.replace_target_iter = replace_target_iter  # 更换 target_net 的步数
        self.memory_size = memory_size  # 记忆上限
        self.batch_size = batch_size    # 每次更新时从 memory 里面取多少记忆出来
        self.epsilon_increment = e_greedy_increment # epsilon 的增量
        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max # 是否开启探索模式, 并逐步减少探索次数

        # 记录学习次数 (用于判断是否更换 target_net 参数)
        self.learn_step_counter = 0

        # 初始化全 0 记忆 [s, a, r, s_]
        self.memory = np.zeros((self.memory_size, n_features*2+2)) # 和视频中不同, 因为 pandas 运算比较慢, 这里改为直接用 numpy

        # 创建 [target_net, evaluate_net]
        self._build_net()

        # 替换 target net 的参数
        t_params = tf.get_collection('target_net_params')  # 提取 target_net 的参数
        e_params = tf.get_collection('eval_net_params')   # 提取  eval_net 的参数
        self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)] # 更新 target_net 参数

        self.sess = tf.Session()

        # 输出 tensorboard 文件
        if output_graph:
            # $ tensorboard --logdir=logs
            tf.summary.FileWriter("logs/", self.sess.graph)

        self.sess.run(tf.global_variables_initializer())
        self.cost_his = []  # 记录所有 cost 变化, 用于最后 plot 出来观看
</code></pre>
<h2 id="17-3-存储记忆">17.3 存储记忆</h2>
<p>DQN 的精髓部分之一: 记录下所有经历过的步，这些步可以进行反复的学习，所以是一种 <code>off-policy</code> 方法，你甚至可以自己玩，然后记录下自己玩的经历，让这个 DQN 学习你是如何通关的。</p>
<pre><code class="language-python">class DeepQNetwork:
    def __init__(self):
        pass
    def store_transition(self, s, a, r, s_):
        if not hasattr(self, 'memory_counter'):
            self.memory_counter = 0

        # 记录一条 [s, a, r, s_] 记录
        transition = np.hstack((s, [a, r], s_))

        # 总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换
        index = self.memory_counter % self.memory_size
        self.memory[index, :] = transition # 替换过程

        self.memory_counter += 1

</code></pre>
<h2 id="17-4-选行为">17.4 选行为</h2>
<p>和之前的 <code>QLearningTable</code> 和 <code>SarsaTable</code> 等一样，都需要一个选行为的功能。</p>
<pre><code class="language-python">class DeepQNetwork:
    def __init__(self):
        pass
    def store_transition(self, s, a, r, s_):
        pass
    def choose_action(self, observation):
        # 统一 observation 的 shape (1, size_of_observation)
        observation = observation[np.newaxis, :]

        if np.random.uniform() &lt; self.epsilon:
            # 让 eval_net 神经网络生成所有 action 的值, 并选择值最大的 action
            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})
            action = np.argmax(actions_value)
        else:
            action = np.random.randint(0, self.n_actions)   # 随机选择
        return action
</code></pre>
<h2 id="17-5-学习">17.5 学习</h2>
<p>最重要的一步来了，就是在 <code>DeepQNetwork</code> 中，是如何学习以及更新参数的。这里涉及了 <code>target_net</code> 和 <code>eval_net</code> 的交互使用。</p>
<pre><code class="language-python">class DeepQNetwork:
    def __init__(self):
        ...
    def store_transition(self, s, a, r, s_):
        ...
    def choose_action(self, observation):
        ...
    def _replace_target_params(self):
        ...
    def learn(self):
        # 检查是否替换 target_net 参数
        if self.learn_step_counter % self.replace_target_iter == 0:
            self.sess.run(self.replace_target_op)
            print('\ntarget_params_replaced\n')

        # 从 memory 中随机抽取 batch_size 这么多记忆
        if self.memory_counter &gt; self.memory_size:
            sample_index = np.random.choice(self.memory_size, size=self.batch_size)
        else:
            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)
        batch_memory = self.memory[sample_index, :]

        # 获取 q_next (target_net 产生了 q) 和 q_eval(eval_net 产生的 q)
        q_next, q_eval = self.sess.run(
            [self.q_next, self.q_eval],
            feed_dict={
                self.s_: batch_memory[:, -self.n_features:],
                self.s: batch_memory[:, :self.n_features]
            })

        """下面这几步十分重要. q_next, q_eval 包含所有 action 的值,
        而我们需要的只是已经选择好的 action 的值, 其他的并不需要.
        所以我们将其他的 action 值全变成 0, 
        将用到的 action 误差值反向传递回去, 作为更新凭据.
        这是我们最终要达到的样子, 
        比如 q_target - q_eval = [1, 0, 0] - [-1, 0, 0] = [2, 0, 0]
        q_eval = [-1, 0, 0] 表示这一个记忆中有我选用过 action 0, 
        而 action 0 带来的 Q(s, a0) = -1, 所以其他的 Q(s, a1) = Q(s, a2) = 0.
        q_target = [1, 0, 0] 表示这个记忆中的 r+gamma*maxQ(s_) = 1,
        而且不管在 s_ 上我们取了哪个 action,
        我们都需要对应上 q_eval 中的 action 位置, 所以就将 1 放在了 action 0 的位置.

        下面也是为了达到上面说的目的, 不过为了更方面让程序运算, 达到目的的过程有点不同.
        是将 q_eval 全部赋值给 q_target, 这时 q_target-q_eval 全为 0,
        不过我们再根据 batch_memory 当中的 action 这个 column 来给 q_target 中的对应的 memory-action 位置来修改赋值.
        使新的赋值为 reward + gamma * maxQ(s_), 这样 q_target-q_eval 就可以变成我们所需的样子.
        具体在下面还有一个举例说明.
        """

        q_target = q_eval.copy()
        batch_index = np.arange(self.batch_size, dtype=np.int32)
        eval_act_index = batch_memory[:, self.n_features].astype(int)
        reward = batch_memory[:, self.n_features + 1]

        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)

        """
        假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:
        q_eval =
        [[1, 2, 3],
         [4, 5, 6]]

        q_target = q_eval =
        [[1, 2, 3],
         [4, 5, 6]]

        然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:
        比如在:
            记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;
            记忆 1 的 q_target 计算值是 -2, 而且我用了 action 2:
        q_target =
        [[-1, 2, 3],
         [4, 5, -2]]

        所以 (q_target - q_eval) 就变成了:
        [[(-1)-(1), 0, 0],
         [0, 0, (-2)-(6)]]

        最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络.
        所有为 0 的 action 值是当时没有选择的 action, 
        之前有选择的 action 才有不为0的值.
        我们只反向传递之前选择的 action 的值,
        """

        # 训练 eval_net
        _, self.cost = self.sess.run([self._train_op, self.loss],
				feed_dict={self.s: batch_memory[:, :self.n_features],
						   self.q_target: q_target})
        self.cost_his.append(self.cost) # 记录 cost 误差

        # 逐渐增加 epsilon, 降低行为的随机性
        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max
        self.learn_step_counter += 1

</code></pre>
<h2 id="17-6-学习效果">17.6 学习效果</h2>
<p>为了看看学习效果，我们在最后输出学习过程中的 <code>cost</code> 变化曲线。</p>
<pre><code class="language-python">class DeepQNetwork:
    def __init__(self):
        ...
    def store_transition(self, s, a, r, s_):
        ...
    def choose_action(self, observation):
        ...
    def _replace_target_params(self):
        ...
    def learn(self):
        ...
    def plot_cost(self):
        import matplotlib.pyplot as plt
        plt.plot(np.arange(len(self.cost_his)), self.cost_his)
        plt.ylabel('Cost')
        plt.xlabel('training steps')
        plt.show()
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20191211105754234.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>可以看出曲线并不是平滑下降的，这是因为 DQN 中的 <code>input</code> 数据是一步步改变的，而且会根据学习情况，获取到不同的数据。所以这并不像一般的监督学习，DQN 的 <code>cost</code> 曲线就有所不同了。</p>
<h2 id="17-7-修改版的-dqn">17.7 修改版的 DQN</h2>
<p><img src="https://img-blog.csdnimg.cn/20191211105915178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>最后提供一种修改版的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/DQN_modified.py" target="_blank" rel="noopener">DQN 代码</a>，这是录制完视频以后做的，这是将 <code>q_target</code> 的计算也加在了 Tensorflow 的 <code>graph</code> 里面。这种结构还是有好处的, 作为学习样本的话，计算结构全部在 <code>tensorboard</code> 上，就更好理解，代码结构也更好理解。</p>
<p>我只在原本的 DQN 代码上改了一点点东西，大家应该可以很容易辨别。</p>
<h1 id="18-openai-gym环境库">18. OpenAI gym环境库</h1>
<h2 id="18-1-要点">18.1 要点</h2>
<p>手动编环境是一件很耗时间的事情，所以如果有能力使用别人已经编好的环境，可以节约我们很多时间。OpenAI gym 就是这样一个模块，它提供了我们很多优秀的模拟环境。我们的各种 RL 算法都能使用这些环境，不过 OpenAI gym 暂时只支持 MacOS 和 Linux 系统，Windows 已经支持，但是听说还没有全面支持，大家时不时查看下官网，可能。是在等不及更新了，也行用 tkinter 来手动编写一下环境。之前的 maze 环境是用 <code>tkinter</code> 编出来的，实在不行可以使用 <code>tkinter</code> 编写环境。或者还可以玩玩更厉害的，想 OpenAI 一样，使用 <code>pyglet</code> 模块来编写，莫烦做了一个从环境开始编写的<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1/" target="_blank" rel="noopener">强化学习实战</a>。</p>
<h2 id="18-2-安装gym">18.2 安装gym</h2>
<p>在 MacOS 和 Linux 系统下，安装 <code>gym</code> 很方便，首先确定你是 python 2.7 或者 python 3.5 版本。然后在你的 terminal 中复制下面这些，但是 gym 暂时还不完全支持 Windows，不过有些虚拟环境已经的到了支持，想立杆子那个已经支持了。 所以接下来要说的安装方法只有 MacOS 和 Linux 的。Windows 用户的安装方式应该也差不多，如果 Windows 用户遇到了问题，欢迎在留言区分享解决的方法。</p>
<pre><code class="language-bash"># python 2.7, 复制下面
→ pip install gym

# python 3.5, 复制下面
→ pip3 install gym

</code></pre>
<p>如果没有报错，恭喜你，这样你就装好了 gym 的最基本款，可以开始玩以下游戏啦:</p>
<ul>
<li><a href="https://gym.openai.com/envs#algorithmic" target="_blank" rel="noopener">algorithmic</a></li>
<li><a href="https://gym.openai.com/envs#toy_text" target="_blank" rel="noopener">toy_text</a></li>
<li><a href="https://gym.openai.com/envs#classic_control" target="_blank" rel="noopener">classic_control</a> (这个需要 pyglet 模块)</li>
</ul>
<p>如果在安装中遇到问题。可能是缺少了一些必要模块，可以使用下面语句来安装这些模块(安装时间可能有点久):</p>
<pre><code class="language-bash"># MacOS:
$ brew install cmake boost boost-python sdl2 swig wget

# Ubuntu 14.04:
$ apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig

</code></pre>
<p>如果想要玩 gym 提供的全套游戏，下面这几句就能满足你:</p>
<pre><code class="language-bash"># python 2.7, 复制下面
→ pip install gym[all]

# python 3.5, 复制下面
→ pip3 install gym[all]

</code></pre>
<h2 id="18-3-cartpole例子">18.3 CartPole例子</h2>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/cartpole%20dqn.mp4</p>
<p>之前我编写的 <code>maze_env</code> 基本上是按照 <code>gym</code> 环境格式写的，所以你换成 <code>gym</code> 格式会很简单。</p>
<p>接下来我们对应上面的算法，来实现主循环，首先 import 所需模。</p>
<pre><code class="language-python">import gym
from RL_brain import DeepQNetwork

env = gym.make('CartPole-v0')   # 定义使用 gym 库中的那一个环境
env = env.unwrapped # 不做这个会有很多限制

print(env.action_space) # 查看这个环境中可用的 action 有多少个
print(env.observation_space)  # 查看这个环境中可用的 state 的 observation 有多少个
print(env.observation_space.high)  # 查看 observation 最高取值
print(env.observation_space.low)   # 查看 observation 最低取值
</code></pre>
<p>于之前使用 tkinter 定义的环境有点不一样，我们可以不使用 <code>if __name__ == "__main__"</code> 的方式，下面是一种类似，却更简单的写法。之中我们会用到里面的 <code>reward</code>，但是 <code>env.step()</code> 说提供的 <code>reward</code> 不一定是最有效率的 <code>reward</code>，我们大可对这些进行修改，使 DQN 学得更有效率。你可以自己对比一下不修改 reward 和 按我这样修改，他们学习过程的不同。</p>
<pre><code class="language-python"># 定义使用 DQN 的算法
RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0],
                  learning_rate=0.01, e_greedy=0.9,
                  replace_target_iter=100, memory_size=2000,
                  e_greedy_increment=0.0008,)

total_steps = 0 # 记录步数

for i_episode in range(100):

    # 获取回合 i_episode 第一个 observation
    observation = env.reset()
    ep_r = 0
    while True:
        env.render()    # 刷新环境

        action = RL.choose_action(observation)  # 选行为

        observation_, reward, done, info = env.step(action) # 获取下一个 state

        x, x_dot, theta, theta_dot = observation_  # 细分开,为了修改原配的 reward

        # x 是车的水平位移, 所以 r1 是车越偏离中心, 分越少
        # theta 是棒子离垂直的角度, 角度越大, 越不垂直. 所以 r2 是棒越垂直, 分越高

        x, x_dot, theta, theta_dot = observation_
        r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8
        r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5
        # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度, 这样 DQN 学习更有效率
        reward = r1 + r2   

        # 保存这一组记忆
        RL.store_transition(observation, action, reward, observation_)

        if total_steps &gt; 1000:
            RL.learn()  # 学习

        ep_r += reward
        if done:
            print('episode: ', i_episode,
                  'ep_r: ', round(ep_r, 2),
                  ' epsilon: ', round(RL.epsilon, 2))
            break

        observation = observation_
        total_steps += 1

# 最后输出 cost 曲线
RL.plot_cost()
</code></pre>
<p>这是更为典型的 RL cost 曲线:</p>
<p><img src="https://img-blog.csdnimg.cn/20191211111927696.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="18-4-mountaincar例子">18.4 MountainCar例子</h2>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/mountaincar%20dqn.mp4</p>
<p>代码基本和上述代码相同，就只是在 reward 上动了下手脚。</p>
<pre><code class="language-python">import gym
from RL_brain import DeepQNetwork

env = gym.make('MountainCar-v0')
env = env.unwrapped

print(env.action_space)
print(env.observation_space)
print(env.observation_space.high)
print(env.observation_space.low)

RL = DeepQNetwork(n_actions=3, n_features=2, learning_rate=0.001, e_greedy=0.9,
                  replace_target_iter=300, memory_size=3000,
                  e_greedy_increment=0.0001,)

total_steps = 0


for i_episode in range(10):

    observation = env.reset()
    ep_r = 0
    while True:
        env.render()

        action = RL.choose_action(observation)

        observation_, reward, done, info = env.step(action)

        position, velocity = observation_

        # 车开得越高 reward 越大
        reward = abs(position - (-0.5))

        RL.store_transition(observation, action, reward, observation_)

        if total_steps &gt; 1000:
            RL.learn()

        ep_r += reward
        if done:
            get = '| Get' if observation_[0] &gt;= env.unwrapped.goal_position else '| ----'
            print('Epi: ', i_episode,
                  get,
                  '| Ep_r: ', round(ep_r, 4),
                  '| Epsilon: ', round(RL.epsilon, 2))
            break

        observation = observation_
        total_steps += 1

RL.plot_cost()
</code></pre>
<p>出来的 cost 曲线是这样:</p>
<p><img src="https://img-blog.csdnimg.cn/20191211112627768.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这两个都只是例子而已，具体的实施你也可以大动手脚，比如你的 <code>reward</code> 定义得更好，你的神经网络结构更好，使得他们学的更快，这些都是自己定义的。</p>
<h1 id="19-double-dqn-tensorflow">19. Double DQN—TensorFlow</h1>
<h2 id="19-1-要点">19.1 要点</h2>
<p>接下来我们说说为什么会有 Double DQN 这种算法。所以我们从 Double DQN 相对于 Natural DQN (传统 DQN) 的优势说起。</p>
<p>一句话概括，DQN 基于 Q-Learning，Q-Learning 中有 <code>Qmax</code>, <code>Qmax</code> 会导致 <code>Q现实</code> 当中的<strong>过估计</strong> (overestimate)，而 Double DQN 就是用来解决过估计的。在实际问题中，如果你输出你的 DQN 的 Q 值，可能就会发现，Q 值都超级大，这就是出现了 overestimate。</p>
<p>这次的 Double DQN 的算法基于的是 OpenAI Gym 中的 <code>Pendulum</code> 环境。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/Pendulum%20DQN.mp4</p>
<h2 id="19-2-double-dqn算法">19.2 Double DQN算法</h2>
<p>我们知道 DQN 的神经网络部分可以看成一个 <code>最新的神经网络</code> + <code>老神经网络</code>，他们有相同的结构，但内部的参数更新却有时差，而它的 <code>Q现实</code> 部分是这样的:</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Y</mi><mi>t</mi><mrow><mi>D</mi><mi>Q</mi><mi>N</mi></mrow></msubsup><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mtext>  </mtext><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>a</mi></msub><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>a</mi><mo separator="true">;</mo><msubsup><mi>θ</mi><mi>t</mi><mo>−</mo></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y_t^{DQN} = R_{t+1} + \gamma \; max_a Q(S_{t+1}, a; \theta_t^-)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2049949999999998em;vertical-align:-0.24575599999999997em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.959239em;"><span style="top:-2.454244em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.1809080000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span><span class="mord mathdefault mtight">Q</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0713309999999998em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8213309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>因为我们的神经网络预测 <code>Qmax</code> 本来就有误差，每次也向着最大误差的 <code>Q现实</code> 改进神经网络，就是因为这个 <code>Qmax</code> 导致了 overestimate，所以 Double DQN 的想法就是引入另一个神经网络来打消一些最大误差的影响。而 DQN 中本来就有两个神经网络，我们何不利用一下这个地理优势呢。所以，我们用 <code>Q估计</code> 的神经网络估计 <code>Q现实</code> 中 <code>Qmax(s', a')</code> 的最大动作值。然后用这个被 <code>Q估计</code> 估计出来的动作来选择 <code>Q现实</code> 中的 <code>Q(s')</code>，总结一下:</p>
<p>有两个神经网络: <code>Q_eval</code> (Q估计中的) 和<code>Q_next</code> (Q现实中的)。原本的 <code>Q_next = max(Q_next(s', a_all))</code>。Double DQN 中的 <code>Q_next = Q_next(s', argmax(Q_eval(s', a_all)))</code>，也可以表达成下面那样:</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Y</mi><mi>t</mi><mrow><mi>D</mi><mi>o</mi><mi>u</mi><mi>b</mi><mi>l</mi><mi>e</mi><mi>D</mi><mi>Q</mi><mi>N</mi></mrow></msubsup><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mtext>  </mtext><mi>Q</mi><mo stretchy="false">(</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>a</mi></msub><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>a</mi><mo separator="true">;</mo><msub><mi>θ</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><msubsup><mi>θ</mi><mi>t</mi><mo>−</mo></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y_t^{DoubleDQN} = R_{t+1} + \gamma \; Q(argmax_a Q(S_{t+1}, a; \theta_t), \theta_t^-)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2127719999999997em;vertical-align:-0.24575599999999997em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9670159999999999em;"><span style="top:-2.454244em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.1809080000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span><span class="mord mathdefault mtight">Q</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0713309999999998em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8213309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<h2 id="19-3-更新方法">19.3 更新方法</h2>
<p>这里的代码都是基于之前 DQN 教程中的代码 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/RL_brain.py" target="_blank" rel="noopener">(github)</a>，在 <code>RL_brain</code> 中我们将 <code>class</code> 的名字改成 <code>DoubleDQN</code>，为了对比 Natural DQN，我们也保留原来大部分的 DQN 的代码。我们在 <code>__init__</code> 中加一个 <code>double_q</code> 参数来表示使用的是 Natural DQN 还是 Double DQN。为了对比的需要，我们的 <code>tf.Session()</code> 也单独传入，并移除原本在 DQN 代码中的这一句 <code>self.sess.run(tf.global_variables_initializer())</code>。</p>
<pre><code class="language-python">class DoubleDQN:
    def __init__(..., double_q=True, sess=None):
        ...
        self.double_q = double_q
        if sess is None:
            self.sess = tf.Session()
            self.sess.run(tf.global_variables_initializer())
        else:
            self.sess = sess
        ...
</code></pre>
<p>接着我们来修改 <code>learn()</code> 中的代码，我们对比 Double DQN 和 Natural DQN 在 tensorboard 中的图，发现他们的结构并没有不同，但是在计算 <code>q_target</code> 的时候，方法是不同的。</p>
<p><img src="https://img-blog.csdnimg.cn/20191211122057506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<pre><code class="language-python">class DoubleDQN:
    def learn(self):
        # 这一段和 DQN 一样:
        if self.learn_step_counter % self.replace_target_iter == 0:
            self.sess.run(self.replace_target_op)
            print('\ntarget_params_replaced\n')

        if self.memory_counter &gt; self.memory_size:
            sample_index = np.random.choice(self.memory_size, size=self.batch_size)
        else:
            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)
        batch_memory = self.memory[sample_index, :]

        # 这一段和 DQN 不一样
        q_next, q_eval4next = self.sess.run(
            [self.q_next, self.q_eval],
            feed_dict={self.s_: batch_memory[:, -self.n_features:],    # next observation
                       self.s: batch_memory[:, -self.n_features:]})    # next observation
        q_eval = self.sess.run(self.q_eval, {self.s: batch_memory[:, :self.n_features]})
        q_target = q_eval.copy()
        batch_index = np.arange(self.batch_size, dtype=np.int32)
        eval_act_index = batch_memory[:, self.n_features].astype(int)
        reward = batch_memory[:, self.n_features + 1]

        if self.double_q:   # 如果是 Double DQN
            # q_eval 得出的最高奖励动作
            max_act4next = np.argmax(q_eval4next, axis=1)
            # Double DQN 选择 q_next 依据 q_eval 选出的动作
            selected_q_next = q_next[batch_index, max_act4next]
        else:       # 如果是 Natural DQN
            selected_q_next = np.max(q_next, axis=1)  # natural DQN

        q_target[batch_index, eval_act_index] = reward + self.gamma * selected_q_next

        # 这下面和 DQN 一样:
        _, self.cost = self.sess.run([self._train_op, self.loss],
				feed_dict={self.s: batch_memory[:, :self.n_features],
						   self.q_target: q_target})
        self.cost_his.append(self.cost)
        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max
        self.learn_step_counter += 1
</code></pre>
<h2 id="19-4-记录-q-值">19.4 记录 Q 值</h2>
<p>为了记录下我们选择动作时的 Q 值，接下来我们就修改 <code>choose_action()</code> 功能，让它记录下每次选择的 Q 值。</p>
<pre><code class="language-python">class DoubleDQN:
    def choose_action(self, observation):
        observation = observation[np.newaxis, :]
        actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})
        action = np.argmax(actions_value)

        if not hasattr(self, 'q'):  # 记录选的 Qmax 值
            self.q = []
            self.running_q = 0
        self.running_q = self.running_q*0.99 + 0.01 * np.max(actions_value)
        self.q.append(self.running_q)

        if np.random.uniform() &gt; self.epsilon:  # 随机选动作
            action = np.random.randint(0, self.n_actions)
        return action
</code></pre>
<h2 id="19-5-对比结果">19.5 对比结果</h2>
<p>接着我们就来对比 Natural DQN 和 Double DQN 带来的不同结果啦，<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/run_Pendulum.py" target="_blank" rel="noopener">代码在这</a></p>
<pre><code class="language-python">import gym
from RL_brain import DoubleDQN
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf


env = gym.make('Pendulum-v0')
env.seed(1) # 可重复实验
MEMORY_SIZE = 3000
ACTION_SPACE = 11    # 将原本的连续动作分离成 11 个动作

sess = tf.Session()
with tf.variable_scope('Natural_DQN'):
    natural_DQN = DoubleDQN(
        n_actions=ACTION_SPACE, n_features=3, memory_size=MEMORY_SIZE,
        e_greedy_increment=0.001, double_q=False, sess=sess
    )

with tf.variable_scope('Double_DQN'):
    double_DQN = DoubleDQN(
        n_actions=ACTION_SPACE, n_features=3, memory_size=MEMORY_SIZE,
        e_greedy_increment=0.001, double_q=True, sess=sess, output_graph=True)

sess.run(tf.global_variables_initializer())


def train(RL):
    total_steps = 0
    observation = env.reset()
    while True:
        # if total_steps - MEMORY_SIZE &gt; 8000: env.render()

        action = RL.choose_action(observation)

        f_action = (action-(ACTION_SPACE-1)/2)/((ACTION_SPACE-1)/4)   # 在 [-2 ~ 2] 内离散化动作

        observation_, reward, done, info = env.step(np.array([f_action]))

        reward /= 10     # normalize 到这个区间 (-1, 0). 立起来的时候 reward = 0.
        # 立起来以后的 Q target 会变成 0, 因为 Q_target = r + gamma * Qmax(s', a') = 0 + gamma * 0
        # 所以这个状态时的 Q 值大于 0 时, 就出现了 overestimate.

        RL.store_transition(observation, action, reward, observation_)

        if total_steps &gt; MEMORY_SIZE:   # learning
            RL.learn()

        if total_steps - MEMORY_SIZE &gt; 20000:   # stop game
            break

        observation = observation_
        total_steps += 1
    return RL.q # 返回所有动作 Q 值

# train 两个不同的 DQN
q_natural = train(natural_DQN)
q_double = train(double_DQN)

# 出对比图
plt.plot(np.array(q_natural), c='r', label='natural')
plt.plot(np.array(q_double), c='b', label='double')
plt.legend(loc='best')
plt.ylabel('Q eval')
plt.xlabel('training steps')
plt.grid()
plt.show()
</code></pre>
<p>所以这个出来的图是这样:</p>
<p><img src="https://img-blog.csdnimg.cn/20191211122328325.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>可以看出, Natural DQN 学得差不多后，在立起来时，大部分时间都是 估计的 <strong>Q值</strong> 要大于0， 这时就出现了 overestimate，而 Double DQN 的 <strong>Q值</strong> 就消除了一些 overestimate， 将估计值保持在 0 左右。</p>
<h1 id="20-prioritized-experience-replay-dqn-tensorflow">20. Prioritized Experience Replay(DQN)—TensorFlow</h1>
<h2 id="20-1-要点">20.1 要点</h2>
<p>这一次还是使用 MountainCar 来进行实验，因为这次我们不需要重度改变它的 reward 了。所以只要是没有拿到小旗子，<code>reward=-1</code>，拿到小旗子时，我们定义它获得了 <code>+10</code> 的 reward。比起之前 DQN 中，这个 reward 定义更加准。如果使用这种 reward 定义方式，可以想象 Natural DQN 会花很久的时间学习，因为记忆库中只有很少很少的 <code>+10</code> reward 可以学习。正负样本不一样，而使用 Prioritized replay，就会重视这种少量的，但值得学习的样本。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/mountaincar%20dqn.mp4</p>
<h2 id="20-2-prioritized-replay算法">20.2 Prioritized Replay算法</h2>
<p><img src="https://img-blog.csdnimg.cn/20191211122754539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这一套算法重点就在我们 <code>batch</code> 抽样的时候并不是随机抽样，而是按照 Memory 中的样本优先级来抽，所以这能更有效地找到我们需要学习的样本。</p>
<p>那么样本的优先级是怎么定的呢？原来我们可以用到 <code>TD-error</code>，也就是 <code>Q现实 - Q估计</code> 来规定优先学习的程度。如果 <code>TD-error</code> 越大，就代表我们的预测精度还有很多上升空间，那么这个样本就越需要被学习，也就是优先级 <code>p</code> 越高。</p>
<p>有了 <code>TD-error</code> 就有了优先级 <code>p</code>，那我们如何有效地根据 <code>p</code> 来抽样呢？如果每次抽样都需要针对 <code>p</code> 对所有样本排序，这将会是一件非常消耗计算能力的事。好在我们还有其他方法，这种方法不会对得到的样本进行排序，这就是这篇 <a href="https://arxiv.org/abs/1511.05952" target="_blank" rel="noopener">paper</a> 中提到的 <code>SumTree</code>.</p>
<p>SumTree 是一种树形结构，每片树叶存储每个样本的优先级 <code>p</code>，每个树枝节点只有两个分叉，节点的值是两个分叉的合，所以 SumTree 的顶端就是所有 <code>p</code> 的和。正如下面<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" target="_blank" rel="noopener">图片(来自Jaromír Janisch)</a>，最下面一层树叶存储样本的 <code>p</code>，叶子上一层最左边的 13 = 3 + 10，按这个规律相加，顶层的 root 就是全部 <code>p</code> 的合了。</p>
<p><img src="https://img-blog.csdnimg.cn/20191211123132523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>抽样时，我们会将 <code>p</code> 的总合除以 <code>batch size</code>，分成 batch size 那么多区间，<code>(n=sum(p)/batch_size)</code>。如果将所有 node 的 <code>priority</code> 加起来是42的，我们如果抽6个样本，这时的区间拥有的 priority 可能是这样。</p>
<pre><code>[0-7], [7-14], [14-21], [21-28], [28-35], [35-42]
</code></pre>
<p>然后在每个区间里随机选取一个数。比如在第区间 <code>[21-28]</code> 里选到了24，就按照这个 24 从最顶上的 42 开始向下搜索。首先看到最顶上 <code>42</code> 下面有两个 child nodes，拿着手中的 24 对比左边的 child <code>29</code>，如果 左边的 child 比自己手中的值大，那我们就走左边这条路，接着再对比 <code>29</code> 下面的左边那个点 <code>13</code>，这时手中的 24 比 <code>13</code> 大，那我们就走右边的路，并且将手中的值根据 <code>13</code> 修改一下，变成 <code>24-13=11</code>。 接着拿着 11 和 <code>13</code> 左下角的 <code>12</code> 比，结果 <code>12</code> 比 11 大，那我们就选 12 当做这次选到的 priority，并且也选择 12 对应的数据。</p>
<h2 id="20-3-sumtree有效抽样">20.3 SumTree有效抽样</h2>
<p>首先要提的是, 这个 SumTree 的算法是对于 <a href="https://github.com/jaara/AI-blog/blob/master/SumTree.py" target="_blank" rel="noopener">Jaromír Janisch 写的 Sumtree</a> 的修改版。Jaromír Janisch 的代码在更新 sumtree 的时候和抽样的时候多次用到了 recursive 递归结构，我使用的是 while 循环，测试要比递归结构运行快。在 <code>class</code> 中的功能也比它的代码少几个，我优化了一下。</p>
<pre><code class="language-python">class SumTree(object):
    # 建立 tree 和 data,
    # 因为 SumTree 有特殊的数据结构,
    # 所以两者都能用一个一维 np.array 来存储
    def __init__(self, capacity):

    # 当有新 sample 时, 添加进 tree 和 data
    def add(self, p, data):

    # 当 sample 被 train, 有了新的 TD-error, 就在 tree 中更新
    def update(self, tree_idx, p):

    # 根据选取的 v 点抽取样本
    def get_leaf(self, v):

    # 获取 sum(priorities)
    @property
    def totoal_p(self):
</code></pre>
<p>具体的抽要和更新值的规则和上面说的类似. 具体的代码在这里呈现的话比较累赘, 详细代码请去往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py#L18-L86" target="_blank" rel="noopener">Github对应的位置</a></p>
<h2 id="20-4-memory类">20.4 Memory类</h2>
<p>这个 Memory 类也是基于 <a href="https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py" target="_blank" rel="noopener">Jaromír Janisch 所写的 Memory</a> 进行了修改和优化。</p>
<pre><code class="language-python">class Memory(object):
    # 建立 SumTree 和各种参数
    def __init__(self, capacity):

    # 存储数据, 更新 SumTree
    def store(self, transition):

    # 抽取 sample
    def sample(self, n):

    # train 完被抽取的 samples 后更新在 tree 中的 sample 的 priority
    def batch_update(self, tree_idx, abs_errors):
</code></pre>
<p>具体的代码在这里呈现的话比较累赘，详细代码请去往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py#L89-L129" target="_blank" rel="noopener">Github对应的位置</a> 下面有很多朋友经常问的一个问题，这个 <code>ISweight</code> 到底怎么算。需要提到的一点是，代码中的计算方法是经过了简化的，将 paper 中的步骤合并了一些。比如 <code>prob = p / self.tree.total_p; ISWeights = np.power(prob/min_prob, -self.beta)</code></p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Compute&nbsp;importance-sampling&nbsp;weight&nbsp;</mtext><msub><mi>w</mi><mi>j</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mi>N</mi><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mi>β</mi></mrow></msup><mi mathvariant="normal">/</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>i</mi></msub><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\text {Compute importance-sampling weight } w_j=(N\cdot P(j))^{-\beta} / max_iw_i
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord text"><span class="mord">Compute&nbsp;importance-sampling&nbsp;weight&nbsp;</span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.149108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight" style="margin-right:0.05278em;">β</span></span></span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>下面是我的推导，如果有不正确还请指出。在paper 中，<code>ISWeight = (N*Pj)^(-beta) / maxi_wi</code> 里面的 <code>maxi_wi</code> 是为了 <code>normalize ISWeight</code>，所以我们先把他放在一边。所以单纯的 importance sampling 就是 <code>(N*Pj)^(-beta)</code>，那 <code>maxi_wi = maxi[(N*Pi)^(-beta)]</code>。</p>
<p>如果将这两个式子合并:</p>
<pre><code>ISWeight = (N*Pj)^(-beta) / maxi[ (N*Pi)^(-beta) ]
</code></pre>
<p>而且如果将 <code>maxi[ (N*Pi)^(-beta) ]</code> 中的 (-beta) 提出来, 这就变成了 <code>mini[ (N*Pi) ] ^ (-beta)</code>。看出来了吧，有的东西可以抵消掉的。最后</p>
<pre><code>ISWeight = (Pj / mini[Pi])^(-beta)
</code></pre>
<p>这样我们就有了代码中的样子。还有代码中的 <code>alpha</code> 是一个决定我们要使用多少 ISweight 的影响，如果 <code>alpha=0</code>，我们就没使用到任何 Importance Sampling。</p>
<h2 id="20-5-更新方法">20.5 更新方法</h2>
<p>基于之前的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/RL_brain.py" target="_blank" rel="noopener">DQN 代码</a>，我们做出以下修改。我们将 class 的名字改成 <code>DQNPrioritiedReplay</code>，为了对比 Natural DQN，我们也保留原来大部分的 DQN 的代码。我们在 <code>__init__</code> 中加一个 <code>prioritized</code> 参数来表示 DQN 是否具备 <code>prioritized</code> 能力。为了对比的需要，我们的 <code>tf.Session()</code>也单独传入，并移除原本在 DQN 代码中的这一句: <code>self.sess.run(tf.global_variables_initializer())</code></p>
<pre><code class="language-python">class DQNPrioritiedReplay:
    def __init__(..., prioritized=True, sess=None)
        self.prioritized = prioritized
        # ...
        if self.prioritized:
            self.memory = Memory(capacity=memory_size)
        else:
            self.memory = np.zeros((self.memory_size, n_features*2+2))

        if sess is None:
            self.sess = tf.Session()
            self.sess.run(tf.global_variables_initializer())
        else:
            self.sess = sess
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20191211124238498.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>搭建神经网络时，我们发现 DQN with Prioritized replay 只多了一个 <code>ISWeights</code>，这个正是<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-6-prioritized-replay/#algorithm" target="_blank" rel="noopener">刚刚算法中</a>提到的 <code>Importance-Sampling Weights</code>，用来恢复被 Prioritized replay 打乱的抽样概率分布。</p>
<pre><code class="language-python">class DQNPrioritizedReplay:
    def _build_net(self)
        ...
        # self.prioritized 时 eval net 的 input 多加了一个 ISWeights
        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input
        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # for calculating loss
        if self.prioritized:
            self.ISWeights = tf.placeholder(tf.float32, [None, 1], name='IS_weights')

        ...
        # 为了得到 abs 的 TD error 并用于修改这些 sample 的 priority, 我们修改如下
        with tf.variable_scope('loss'):
            if self.prioritized:
                self.abs_errors = tf.reduce_sum(tf.abs(self.q_target - self.q_eval), axis=1)    # for updating Sumtree
                self.loss = tf.reduce_mean(self.ISWeights * tf.squared_difference(self.q_target, self.q_eval))
            else:
                self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))
</code></pre>
<p>因为和 Natural DQN 使用的 Memory 不一样，所以在存储 transition 的时候方式也略不相同。</p>
<pre><code class="language-python">class DQNPrioritizedReplay:
    def store_transition(self, s, a, r, s_):
        if self.prioritized:    # prioritized replay
            transition = np.hstack((s, [a, r], s_))
            self.memory.store(transition)
        else:       # random replay
            if not hasattr(self, 'memory_counter'):
                self.memory_counter = 0
            transition = np.hstack((s, [a, r], s_))
            index = self.memory_counter % self.memory_size
            self.memory[index, :] = transition
            self.memory_counter += 1
</code></pre>
<p>接下来是相对于 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py" target="_blank" rel="noopener">Natural DQN 代码</a>，我们在 <code>learn()</code> 改变的部分也在如下展示。</p>
<pre><code class="language-python">class DQNPrioritizedReplay:
    def learn(self):
        ...
        # 相对于 DQN 代码, 改变的部分
        if self.prioritized:
            tree_idx, batch_memory, ISWeights = self.memory.sample(self.batch_size)
        else:
            sample_index = np.random.choice(self.memory_size, size=self.batch_size)
            batch_memory = self.memory[sample_index, :]

        ...

        if self.prioritized:
            _, abs_errors, self.cost = self.sess.run([self._train_op, self.abs_errors, self.loss],
					feed_dict={self.s: batch_memory[:, :self.n_features],
							   self.q_target: q_target,
                               self.ISWeights: ISWeights})
            self.memory.batch_update(tree_idx, abs_errors)   # update priority
        else:
            _, self.cost = self.sess.run([self._train_op, self.loss],
					feed_dict={self.s: batch_memory[:, :self.n_features],
							   self.q_target: q_target})

        ...
</code></pre>
<h2 id="20-6-对比结果">20.6 对比结果</h2>
<p><img src="https://img-blog.csdnimg.cn/20191211124009684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>运行我 Github 中的这个 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/run_MountainCar.py" target="_blank" rel="noopener">MountainCar 脚本</a>，我们就不难发现，我们都从两种方法最初拿到第一个 <code>R=+10</code> 奖励的时候算起，看看经历过一次 <code>R=+10</code> 后，他们有没有好好利用这次的奖励。可以看出，有 Prioritized replay 的可以高效的利用这些不常拿到的奖励，并好好学习他们。所以 <code>Prioritized replay</code> 会更快结束每个 <code>episode</code>， 很快就到达了小旗子。</p>
<h1 id="21-dueling-dqn-tensorflow">21. Dueling DQN—TensorFlow</h1>
<h2 id="21-1-要点">21.1 要点</h2>
<p>只要稍稍修改 DQN 中神经网络的结构，就能大幅提升学习效果，加速收敛。这种新方法叫做 <code>Dueling DQN</code>。用一句话来概括 Dueling DQN 就是。它将每个动作的 <code>Q</code> 拆分成了 <code>state</code> 的 <code>Value </code> 加上每个动作的 <code>Advantage</code>。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/Pendulum%20DQN.mp4</p>
<h2 id="21-2-dueling算法">21.2 Dueling算法</h2>
<p>上一个 Paper 中的经典解释图片，上者是一般的 DQN 的 Q值神经网络。下者就是 Dueling DQN 中的 Q值神经网络了。那具体是哪里不同了呢?</p>
<p><img src="https://img-blog.csdnimg.cn/20200113172746925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>下面这个公式解释了不同之处。原来 DQN 神经网络直接输出的是每种动作的 Q值，而 Dueling DQN 每个动作的 Q值是有下面的公式确定的。</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">;</mo><mi>θ</mi><mo separator="true">,</mo><mi>α</mi><mo separator="true">,</mo><mi>β</mi><mo stretchy="false">)</mo><mo>=</mo><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">;</mo><mi>θ</mi><mo separator="true">,</mo><mi>β</mi><mo stretchy="false">)</mo><mo>+</mo><mi>A</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">;</mo><mi>a</mi><mo separator="true">;</mo><mi>θ</mi><mo separator="true">,</mo><mi>α</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(s, a; \theta, \alpha, \beta) = V(s;\theta, \beta) + A(s;a;\theta,\alpha)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">A</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mclose">)</span></span></span></span></span></p>
<p>它分成了这个 <code>state</code> 的值，加上每个动作在这个 <code>state</code> 上的 <code>advantage</code>。因为有时候在某种 <code>state</code>，无论做什么动作，对下一个 <code>state</code> 都没有多大影响。比如 paper 中的这张图。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113173004298.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这是开车的游戏，左边是 <code>state value</code>，发红的部分证明了 <code>state value</code> 和前面的路线有关，右边是 <code>advantage</code>，发红的部分说明了 <code>advantage</code> 很在乎旁边要靠近的车子，这时的动作会受更多 <code>advantage</code> 的影响，发红的地方左右了自己车子的移动原则。</p>
<h2 id="21-3-更新方法">21.3 更新方法</h2>
<p>下面的修改都是基于我之前写的 DQN 代码，这次修改的部分比较少。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113173248438.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<pre><code class="language-python">class DuelingDQN:
    def __init__(..., dueling=True, sess=None)
        # ...
        self.dueling = dueling  # 会建立两个 DQN, 其中一个是 Dueling DQN
        # ...
        if sess is None:  # 针对建立两个 DQN 的模式修改了 tf.Session() 的建立方式
            self.sess = tf.Session()
            self.sess.run(tf.global_variables_initializer())
        else:
            self.sess = sess
        # ...

    def _build_net(self):
        def build_layers(s, c_names, n_l1, w_initializer, b_initializer):
            with tf.variable_scope('l1'):  # 第一层, 两种 DQN 都一样
                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)
                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)
                l1 = tf.nn.relu(tf.matmul(s, w1) + b1)

            if self.dueling:
                # Dueling DQN
                with tf.variable_scope('Value'):  # 专门分析 state 的 Value
                    w2 = tf.get_variable('w2', [n_l1, 1], initializer=w_initializer, collections=c_names)
                    b2 = tf.get_variable('b2', [1, 1], initializer=b_initializer, collections=c_names)
                    self.V = tf.matmul(l1, w2) + b2

                with tf.variable_scope('Advantage'):  # 专门分析每种动作的 Advantage
                    w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)
                    b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)
                    self.A = tf.matmul(l1, w2) + b2

                with tf.variable_scope('Q'):  # 合并 V 和 A, 为了不让 A 直接学成了 Q, 我们减掉了 A 的均值
                    out = self.V + (self.A - tf.reduce_mean(self.A, axis=1, keep_dims=True))     # Q = V(s) + A(s,a)
            else:
                with tf.variable_scope('Q'):  # 普通的 DQN 第二层
                    w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)
                    b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)
                    out = tf.matmul(l1, w2) + b2

            return out
        # ...
</code></pre>
<h2 id="21-4-对比结果">21.4 对比结果</h2>
<p>这次我们看看累积奖励 <code>reward</code>，杆子立起来的时候奖励 <code>reward=0</code>，其他时候都是负值，所以当累积奖励没有在降低时，说明杆子已经被成功立了很久了。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113173453263.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们发现当可用动作越高，学习难度就越大，不过 Dueling DQN 还是会比 Natural DQN 学习得更快，收敛效果更好。</p>
<h1 id="22-什么是policy-gradients">22. 什么是Policy Gradients</h1>
<h2 id="22-1-和以往的强化学习方法不同">22.1 和以往的强化学习方法不同</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113173649311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>强化学习是一个通过奖惩来学习正确行为的机。家族中有很多种不一样的成，有学习奖惩值，根据自己认为的高价值选行为，比如 Q-Learning、Deep Q Network，也有不通过分析奖励值，直接输出行为的方法，这就是今天要说的 Policy Gradients 了。甚至我们可以为 Policy Gradients 加上一个神经网络来输出预测的动，对比起以值为基础的方法，Policy Gradients 直接输出动作的最大好处就是，它能在一个连续区间内挑选动作，而基于值的，比如 Q-learning，它如果在无穷多的动作中计算价，从而选择行，这它可吃不消。</p>
<h2 id="22-2-更新不同之处">22.2 更新不同之处</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113173858233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>有了神经网络当然方便，但是我们怎么进行神经网络的误差反向传递呢？Policy Gradients 的误差又是什么呢？答案是没有误差! 但是他的确是在进行某一种的反向传递。这种反向传递的目的是让这次被选中的行为更有可能在下次发生。但是我们要怎么确定这个行为是不是应当被增加被选的概率呢？这时候我们的老朋友，<code>reward</code> 奖惩正可以在这时候派上用场。</p>
<h2 id="22-3-具体更新步骤">22.3 具体更新步骤</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113174024802.png" alt="在这里插入图片描述"></p>
<p>现在我们来演示一遍，观测的信息通过神经网络分析，选出了左边的行为，我们直接进行反向传递，使之下次被选的可能性增加，但是奖惩信息却告诉我们，这次的行为是不好的，那我们的动作可能性增加的幅度随之被减低。这样就能靠奖励来左右我们的神经网络反向传递。我们再来举个例子，假如这次的观测信息让神经网络选择了右边的行为，右边的行为随之想要进行反向传递，使右边的行为下次被多选一点，这时奖惩信息也来，告诉我们这是好行为，那我们就在这次反向传递的时候加大力度，让它下次被多选的幅度更猛烈！这就是 Policy Gradients 的核心思想了。</p>
<h1 id="23-policy-gradients算法更新-tensorflow">23. Policy Gradients算法更新—TensorFlow</h1>
<h2 id="23-1-要点">23.1 要点</h2>
<p>Policy gradient 是 RL 中另外一个大家族，他不像 Value-based 方法 (Q-Learnin、Sarsa)，但他也要接受<strong>环境信息</strong>(observation)，不同的是他要输出不是 <code>action</code> 的 <code>value</code>，而是具体的那一个 <code>action</code>， 这样 policy gradient 就跳过了 <code>value</code> 这个阶段。而且个人认为 Policy gradient 最大的一个优势是: 输出的这个 action 可以是一个连续的值，之前我们说到的 value-based 方法输出的都是不连续的值，然后再选择值最大的 <code>action</code>。而 policy gradient 可以在一个连续分布上选取 <code>action</code>。</p>
<p>视频传送门：</p>
<ul>
<li>https://morvanzhou.github.io/static/results/reinforcement-learning/cartpole%20policy%20gradient%20softmax.mp4</li>
<li>https://morvanzhou.github.io/static/results/reinforcement-learning/mountaincar%20policy%20gradient%20softmax.mp4</li>
</ul>
<h2 id="23-2-算法">23.2 算法</h2>
<p>我们介绍的 policy gradient 的第一个算法是一种基于 <strong>整条回合数据</strong> 的更新，也叫 <strong>REINFORCE</strong> 方法。 这种方法是 policy gradient 的最基本方法，有了这个的基，我们再来做更高级的。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113174529935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><code>delta(log(Policy(s,a)) * V)</code> 表示在 状态 <code>s</code> 对所选动作 <code>a</code> 的吃惊度，如果 <code>Policy(s,a)</code> 概率越小，反向的 <code>log(Policy(s,a))</code> (即 <code>-log(P)</code>) 反而越大。如果在 <code>Policy(s,a)</code> 很小的情况下，拿到了一个 大的 <code>R</code>，也就是 大的 <code>V</code>，那 <code>-delta(log(Policy(s, a)) * V)</code> 就更大，表示更吃惊，(我选了一个不常选的动作，却发现原来它能得到了一个好的 <code>reward</code>，那我就得对我这次的参数进行一个大幅修改)。这就是吃惊度的物理意义啦。</p>
<h2 id="23-3-算法代码形式">23.3 算法代码形式</h2>
<p>和以前类，我们先定义主更新的循环，然后下节内容讲如何用 Tensorflow 定义 <code>PolicyGradient()</code> 的算法:</p>
<pre><code class="language-python">import gym
from RL_brain import PolicyGradient
import matplotlib.pyplot as plt

RENDER = False  # 在屏幕上显示模拟窗口会拖慢运行速度, 我们等计算机学得差不多了再显示模拟
DISPLAY_REWARD_THRESHOLD = 400  # 当回合总 reward 大于 400 时显示模拟窗口

env = gym.make('CartPole-v0')   # CartPole 这个模拟
env = env.unwrapped  # 取消限制
# 普通的 Policy gradient 方法，使得回合的 variance 比较大, 
# 所以我们选了一个好点的随机种子
env.seed(1)  

print(env.action_space)  # 显示可用 action
print(env.observation_space)  # 显示可用 state 的 observation
print(env.observation_space.high)  # 显示 observation 最高值
print(env.observation_space.low)   # 显示 observation 最低值

# 定义
RL = PolicyGradient(
    n_actions=env.action_space.n,
    n_features=env.observation_space.shape[0],
    learning_rate=0.02,
    reward_decay=0.99,  # gamma
    # output_graph=True,  # 输出 tensorboard 文件
)
</code></pre>
<p>主循环在这，这节介绍的内容是让计算机跑完一整个回合才更新一次。之前的 Q-Leanring 等在回合中每一步都可以更新参数。</p>
<pre><code class="language-python">for i_episode in range(3000):

    observation = env.reset()

    while True:
        if RENDER: env.render()

        action = RL.choose_action(observation)

        observation_, reward, done, info = env.step(action)
		# 存储这一回合的 transition
        RL.store_transition(observation, action, reward)

        if done:
            ep_rs_sum = sum(RL.ep_rs)

            if 'running_reward' not in globals():
                running_reward = ep_rs_sum
            else:
                running_reward = running_reward * 0.99 + ep_rs_sum * 0.01
             # 判断是否显示模拟
            if running_reward &gt; DISPLAY_REWARD_THRESHOLD: RENDER = True   
            	print("episode:", i_episode, "  reward:", int(running_reward))

            vt = RL.learn() # 学习, 输出 vt, 我们下节课讲这个 vt 的作用

            if i_episode == 0:
                plt.plot(vt)  # plot 这个回合的 vt
                plt.xlabel('episode steps')
                plt.ylabel('normalized state-action value')
                plt.show()
            break

        observation = observation_
</code></pre>
<p>另外一个 Mountain Car 模拟代码在我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/7_Policy_gradient_softmax/run_MountainCar.py" target="_blank" rel="noopener">Github 中</a>，和上面那些代码类似，只改动了一些大写的参数。</p>
<h1 id="24-policy-gradients思维决策-tensorflow">24. Policy Gradients思维决策—TensorFlow</h1>
<p>接着上节内容，我们来实现 <code>RL_brain</code> 的 <code>PolicyGradient</code> 部分，这也是 RL 的大脑部分，负责决策和思考。</p>
<h2 id="24-1-主要代码结构">24.1 主要代码结构</h2>
<p>用基本的 Policy gradient 算，和之前的 value-based 算法看上去很类似。</p>
<pre><code class="language-python">class PolicyGradient:
    # 初始化 (有改变)
    def __init__(self, n_actions, n_features, learning_rate=0.01, reward_decay=0.95, output_graph=False):

    # 建立 policy gradient 神经网络 (有改变)
    def _build_net(self):

    # 选行为 (有改变)
    def choose_action(self, observation):

    # 存储回合 transition (有改变)
    def store_transition(self, s, a, r):

    # 学习更新参数 (有改变)
    def learn(self, s, a, r, s_):

    # 衰减回合的 reward (新内容)
    def _discount_and_norm_rewards(self):
</code></pre>
<h2 id="24-2-初始化">24.2 初始化</h2>
<p>初始化时，我们需要给出这些参数，并创建一个神经网络。</p>
<pre><code class="language-python">class PolicyGradient:
    def __init__(self, n_actions, n_features, learning_rate=0.01, reward_decay=0.95, output_graph=False):
        self.n_actions = n_actions
        self.n_features = n_features
        self.lr = learning_rate    # 学习率
        self.gamma = reward_decay  # reward 递减率
		# 这是我们存储 回合信息的 list
        self.ep_obs, self.ep_as, self.ep_rs = [], [], []

        self._build_net()   # 建立 policy 神经网络

        self.sess = tf.Session()

        if output_graph:    # 是否输出 tensorboard 文件
            # $ tensorboard --logdir=logs
            # http://0.0.0.0:6006/
            # tf.train.SummaryWriter soon be deprecated, use following
            tf.summary.FileWriter("logs/", self.sess.graph)

        self.sess.run(tf.global_variables_initializer())
</code></pre>
<h2 id="24-3-建立policy神经网络">24.3 建立Policy神经网络</h2>
<p>这次我们要建立的神经网络是这样的:</p>
<p><img src="https://img-blog.csdnimg.cn/20200113175233323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>因为这是强化学习，所以神经网络中并没有我们熟知的监督学习中的 <code>y</code>类标。取而代之的是我们选的 <code>action</code>。</p>
<pre><code class="language-python">class PolicyGradient:
    def __init__(self, n_actions, n_features, learning_rate=0.01, reward_decay=0.95, output_graph=False):
        ...
    def _build_net(self):
        with tf.name_scope('inputs'):
            # 接收 observation
            self.tf_obs = tf.placeholder(tf.float32, [None, self.n_features], name="observations")
            # 接收我们在这个回合中选过的 actions
            self.tf_acts = tf.placeholder(tf.int32, [None, ], name="actions_num") 
            # 接收每个 state-action 所对应的 value (通过 reward 计算)
            self.tf_vt = tf.placeholder(tf.float32, [None, ], name="actions_value")

        # fc1
        layer = tf.layers.dense(
            inputs=self.tf_obs,
            units=10,   # 输出个数
            activation=tf.nn.tanh,  # 激励函数
            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),
            bias_initializer=tf.constant_initializer(0.1),
            name='fc1'
        )
        # fc2
        all_act = tf.layers.dense(
            inputs=layer,
            units=self.n_actions,  # 输出个数
            activation=None,  # 之后再加 Softmax
            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),
            bias_initializer=tf.constant_initializer(0.1),
            name='fc2'
        )
		# 激励函数 softmax 出概率
        self.all_act_prob = tf.nn.softmax(all_act, name='act_prob')

        with tf.name_scope('loss'):
            # 最大化 总体 reward (log_p * R) 就是在最小化 -(log_p * R), 
            # 而 tf 的功能里只有最小化 loss
            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_act, labels=self.tf_acts) # 所选 action 的概率 -log 值
            # 下面的方式是一样的:
            # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1)
            # (vt = 本reward + 衰减的未来reward) 引导参数的梯度下降
            loss = tf.reduce_mean(neg_log_prob * self.tf_vt)

        with tf.name_scope('train'):
            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)
</code></pre>
<p>这里有必要解释一下为什么我们使用的 <code>loss= -log(prob)*vt</code> 当做 loss，因为下面有很多评论说这里不理解。简单来说，上面提到了两种形式来计算 <code>neg_log_prob</code>，这两种形式是一模一样的，只是第二个是第一个的展开。如果你仔细看第一个形式，这不就是在神经网络分类问题中的 cross-entropy 嘛！ 使用 <code>softmax</code> 和神经网络的最后一层 <code>logits</code> 输出和真实标签 (<code>self.tf_acts</code>) 对比的误差。并将神经网络的参数按照这个真实标签改进。这显然和一个分类问题没有太多区别。我们能将这个 <code>neg_log_prob</code> 理解成 cross-entropy 的分类误差。分类问题中的标签是真实 <code>x</code> 对应的 <code>y</code>，而我们 Policy gradient 中，<code>x</code> 是 state，<code>y</code> 就是它按照这个 <code>x</code> 所做的动作号码。所以也可以理解成，它按照 <code>x</code> 做的动作永远是对的 (出来的动作永远是正确标签)，它也永远会按照这个 “正确标签” 修改自己的参数。可是事实却不是这样，他的动作不一定都是 “正确标签”，这就是强化学习(Policy gradient)和监督学习(classification)的不同。</p>
<p>为了确保这个动作真的是 “正确标签”，我们的 <code>loss</code> 在原本的 cross-entropy 形式上乘以 <code>vt</code>，用 <code>vt</code> 来告诉这个 cross-entropy 算出来的梯度是不是一个值得信任的梯度。如果 <code>vt</code> 小，或者是负的，就说明这个梯度下降是一个错误的方向，我们应该向着另一个方向更新参数，如果这个 <code>vt</code> 是正的，或很大，<code>vt</code> 就会称赞 cross-entropy 出来的梯度，并朝着这个方向梯度下。下面有一张从 <a href="http://karpathy.github.io/2016/05/31/rl/" target="_blank" rel="noopener">karpathy 大神</a> 网页上扣下来的图，也正是阐述的这个思想。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113175806448.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>而不明白为什么是 <code>loss=-log(prob)*vt</code> 而不是 <code>loss=-prob*vt</code> 的朋友们，下面留言有很多问道这个问题。原因是这里的 <code>prob</code> 是从 <code>softmax</code> 出来的，而计算神经网络里的所有参数梯度，使用到的就是 cross-entropy，然后将这个梯度乘以 <code>vt</code> 来控制梯度下降的方向和力度。而我上面使用 <code>neg_log_prob</code> 这个名字只是为了区分这不是真正意义上的 cross-entropy，因为标签不是真标签。 我在下面提供一些扩展链接。</p>
<h2 id="24-4-选行为">24.4 选行为</h2>
<p>这个行为不再是用 Q value 来选定的，而是用概率来选定。即使不用 <code>epsilon-greedy</code>，也具有一定的随机性。</p>
<pre><code class="language-python">class PolicyGradient:
    def __init__(self, n_actions, n_features, learning_rate=0.01, reward_decay=0.95, output_graph=False):
        # ...
    def _build_net(self):
        # ...
    def choose_action(self, observation):
        prob_weights = self.sess.run(self.all_act_prob, feed_dict={self.tf_obs: observation[np.newaxis, :]})  # 所有 action 的概率
        action = np.random.choice(range(prob_weights.shape[1]), p=prob_weights.ravel())  # 根据概率来选 action
        return action
</code></pre>
<h2 id="24-5-存储回合">24.5 存储回合</h2>
<p>这一部很简单，就是将这一步的 <code>observation</code>、<code>action</code>、<code>reward</code> 加到列表中去。因为本回合完毕之后要清空列表，然后存储下一回合的数据，所以我们会在 <code>learn()</code> 当中进行清空列表的动作。</p>
<pre><code class="language-python">class PolicyGradient:
    def __init__(self, n_actions, n_features, learning_rate=0.01, reward_decay=0.95, output_graph=False):
        # ...
    def _build_net(self):
        # ...
    def choose_action(self, observation):
        # ...
    def store_transition(self, s, a, r):
        self.ep_obs.append(s)
        self.ep_as.append(a)
        self.ep_rs.append(r)
</code></pre>
<h2 id="24-6-学习">24.6 学习</h2>
<p>本节的 <code>learn()</code> 很简单，首先我们要对这回合的所有 <code>reward</code> 动动手脚，使他变得更适合被学习。第一就是随着时间推进，用 <code>gamma</code> 衰减未来的 <code>reward</code>，然后为了一定程度上减小 policy gradient 回合 <code>variance</code>，我们标准化回合的 state-action value <a href="http://karpathy.github.io/2016/05/31/rl/" target="_blank" rel="noopener">依据在 Andrej Karpathy 的 blog</a>。</p>
<pre><code class="language-python">class PolicyGradient:
    def __init__(self, n_actions, n_features, learning_rate=0.01, reward_decay=0.95, output_graph=False):
        ...
    def _build_net(self):
        # ...
    def choose_action(self, observation):
        # ...
    def store_transition(self, s, a, r):
        # ...
    def learn(self):
        # 衰减, 并标准化这回合的 reward
        discounted_ep_rs_norm = self._discount_and_norm_rewards()  # 功能再面

        # train on episode
        self.sess.run(self.train_op, feed_dict={
             self.tf_obs: np.vstack(self.ep_obs),  # shape=[None, n_obs]
             self.tf_acts: np.array(self.ep_as),  # shape=[None, ]
             self.tf_vt: discounted_ep_rs_norm,  # shape=[None, ]
        })

        self.ep_obs, self.ep_as, self.ep_rs = [], [], []  # 清空回合 data
        return discounted_ep_rs_norm  # 返回这一回合的 state-action value
</code></pre>
<p>我们再来看看这个 <code>discounted_ep_rs_norm</code> 到底长什么，不知道大家还记不记得上节内容的这一段:</p>
<pre><code class="language-python">vt = RL.learn()  # 学习,输出 vt,我们下节课讲这个 vt 的作用

if i_episode == 0:
    plt.plot(vt)  # plot 这个回合的 vt
    plt.xlabel('episode steps')
    plt.ylabel('normalized state-action value')
    plt.show()
</code></pre>
<p>我们看看这一段的输出，<code>vt</code> 也就是 <code>discounted_ep_rs_norm</code>，看他是怎么样诱导我们的 gradient descent。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113180322855.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>可以看出，左边一段的 <code>vt</code> 有较高的值, 右边较低，这就是 <code>vt</code> 在说:</p>
<p><strong>“请重视我这回合开始时的一系列动作，因为前面一段时间杆子还没有掉下来。而且请惩罚我之后的一系列动作，因为后面的动作让杆子掉下来了”</strong> 或者是</p>
<p><strong>“我每次都想让这个动作在下一次增加被做的可能性 (<code>grad(log(Policy))</code>)，但是增加可能性的这种做法是好还是坏呢? 这就要由 <code>vt</code> 告诉我了，所以后段时间的 <code>增加可能性</code> 做法并没有被提倡，而前段时间的 <code>增加可能性</code> 做法是被提倡的。”</strong></p>
<p>这样 <code>vt</code> 就能在这里 <code>loss = tf.reduce_mean(log_prob * self.tf_vt)</code> 诱导 gradient descent 朝着正确的方向发展了。</p>
<p>如果你玩了下 <code>MountainCar</code> 的模拟程序，你会发现 <code>MountainCar</code> 模拟程序中的 <code>vt</code> 长这样:</p>
<p><img src="https://img-blog.csdnimg.cn/20200113180530228.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这张图在说: <strong>“请重视我这回合最后的一系列动作，因为这一系列动作让我爬上了山。而且请惩罚我开始的一系列动作，因为这些动作没能让我爬上山”。</strong></p>
<p>也是通过这些 <code>vt</code> 来诱导梯度下降的方向。最后是如何用算法实现对未来 reward 的衰减。</p>
<pre><code class="language-python">class PolicyGradient:
    def __init__(self, n_actions, n_features, learning_rate=0.01, reward_decay=0.95, output_graph=False):
        # ...
    def _build_net(self):
        # ...
    def choose_action(self, observation):
        # ...
    def store_transition(self, s, a, r):
        # ...
    def learn(self):
        # ...
    def _discount_and_norm_rewards(self):
        # discount episode rewards
        discounted_ep_rs = np.zeros_like(self.ep_rs)
        running_add = 0
        for t in reversed(range(0, len(self.ep_rs))):
            running_add = running_add * self.gamma + self.ep_rs[t]
            discounted_ep_rs[t] = running_add

        # normalize episode rewards
        discounted_ep_rs -= np.mean(discounted_ep_rs)
        discounted_ep_rs /= np.std(discounted_ep_rs)
        return discounted_ep_rs
</code></pre>
<h1 id="25-什么是actor-critic">25. 什么是Actor Critic</h1>
<p>今天我们会来说说强化学习中的一种结合体 Actor Critic (演员评判家)，它合并了<strong>以值为基础</strong> (比如 Q-Learning) 和<strong>以动作概率为基础</strong> (比如 Policy Gradients) 两类强化学习算法。</p>
<h2 id="25-1-为什么要有actor和critic">25.1 为什么要有Actor和Critic</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113180804874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们有了像 Q-learning 这么伟大的算法，为什么还要瞎折腾出一个 Actor-Critic？原来 Actor-Critic 的 Actor 的前生是 Policy Gradients，这能让它毫不费力地在连续动作中选取合适的动作，而 Q-Learning 做这件事会瘫痪。那为什么不直接用 Policy Gradients 呢？原来 Actor-Critic 中的 <code>Critic</code> 的前生是 Q-Learning 或者其他的以值为基础的学习法，能进行单步更新，而传统的 Policy Gradients 则是回合更新，这降低了学习效率。</p>
<h2 id="25-2-actor和critic">25.2 Actor和Critic</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113180959783.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>现在我们有两套不同的体系，<code>Actor</code> 和 <code>Critic</code>，他们都能用不同的神经网络来代替。在 Policy Gradients 的影片中提到过，现实中的奖惩会左右 <code>Actor</code> 的更新情况。Policy Gradients 也是靠着这个来获取适宜的更。那么何时会有奖惩这种信息能不能被学习呢？这看起来不就是以值为基础的强化学习方法做过的事吗。那我们就拿一个 <code>Critic</code> 去学习这些奖惩机制，学习完了以后。由 <code>Actor</code> 来指手画脚，由 Critic 来告诉 Actor 你的那些指手画脚哪些指得好，哪些指得差，Critic 通过学习环境和奖励之间的关系，能看到现在所处状态的潜在，所以用它来指点 Actor 便能使 Actor 每一步都在更新，如果使用单纯的 Policy Gradients，Actor 只能等到回合结束才能开始更新。</p>
<h2 id="25-3-增加单步更新属性">25.3 增加单步更新属性</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113181224554.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>但是事物终有它坏的一面，Actor-Critic 涉及到了两个神经网络，而且每次都是在连续状态中更新参数，每次参数更新前后都存在相关性，导致神经网络只能片面的看待问题，甚至导致神经网络学不到东西。Google DeepMind 为了解决这个，修改了 Actor-Critic 的算法。</p>
<h2 id="25-4-改进版deep-deterministic-policy-gradient-ddpg">25.4 改进版Deep Deterministic Policy Gradient(DDPG)</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113181322905.png" alt="在这里插入图片描述"></p>
<p>将之前在电动游戏 Atari 上获得成功的 DQN 网络加入进 Actor Critic 系统中，这种新算法叫做 Deep Deterministic Policy Gradient，成功的解决的在连续动作预测上的学不到东西问题。所以之后，我们再来说说什么是这种高级版本的 Deep Deterministic Policy Gradient 吧。</p>
<h1 id="26-actot-critic-tensorflow">26. Actot Critic—TensorFlow</h1>
<h2 id="26-1-要点">26.1 要点</h2>
<p><strong>一句话概括 Actor Critic 方法</strong>:</p>
<p>结合了 Policy Gradient (Actor) 和 Function Approximation (Critic) 的方法。<code>Actor</code> 基于概率选行， <code>Critic</code> 基于 <code>Actor</code> 的行为评判行为的得分，<code>Actor</code> 根据 <code>Critic</code> 的评分修改选行为的概率。</p>
<p><strong>Actor Critic 方法的优势</strong>: 可以进行单步更新，比传统的 Policy Gradient 要快。</p>
<p><strong>Actor Critic 方法的劣势</strong>: 取决于 Critic 的价值判断，但是 Critic 难收敛，再加上 Actor 的更新，就更难收敛。为了解决收敛问题，Google Deepmind 提出了 <code>Actor Critic</code> 升级版 <code>Deep Deterministic Policy Gradient</code>。后者融合了 DQN 的优势，解决了收敛难的问。我们之后也会要讲到 Deep Deterministic Policy Gradient。不过那个是要以 <code>Actor Critic</code> 为基础，懂了 <code>Actor Critic</code>，后面那个就好懂了。</p>
<p>下面是基于 Actor Critic 的 Gym Cartpole 实验: https://morvanzhou.github.io/static/results/reinforcement-learning/cartpole%20actor%20critic.mp4</p>
<h2 id="26-2-算法">26.2 算法</h2>
<p>这套算法是在普通的 Policy gradient 算法上面修改的，对这套算法打个比方如下:</p>
<p><strong><code>Actor</code> 修改行为时就像蒙着眼睛一直向前开，<code>Critic</code> 就是那个扶方向盘改变 <code>Actor</code> 开车方向的.</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200113191657394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>或者说详细点，就是 <code>Actor</code> 在运用 Policy Gradient 的方法进行 Gradient ascent 的时候，由 <code>Critic</code> 来告诉他，这次的 Gradient ascent 是不是一次正确的 ascent, 如果这次的得分不好，那么就不要 ascent 那么多。</p>
<h2 id="26-3-代码主结构">26.3 代码主结构</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113191747920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>上图是 <code>Actor</code> 的神经网络结果, 代码结构在下面:</p>
<pre><code class="language-python">class Actor(object):
    def __init__(self, sess, n_features, n_actions, lr=0.001):
        # 用 tensorflow 建立 Actor 神经网络,
        # 搭建好训练的 Graph.

    def learn(self, s, a, td):
        # s, a 用于产生 Gradient ascent 的方向,
        # td 来自 Critic, 用于告诉 Actor 这方向对不对.

    def choose_action(self, s):
        # 根据 s 选 行为 a

</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20200113191846369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>上图是 <code>Critic</code> 的神经网络结果，代码结构在下面:</p>
<pre><code class="language-python">class Critic(object):
    def __init__(self, sess, n_features, lr=0.01):
        # 用 tensorflow 建立 Critic 神经网络,
        # 搭建好训练的 Graph.

    def learn(self, s, r, s_):
        # 学习 状态的价值 (state value), 不是行为的价值 (action value),
        # 计算 TD_error = (r + v_) - v,
        # 用 TD_error 评判这一步的行为有没有带来比平时更好的结果,
        # 可以把它看做 Advantage
        return # 学习时产生的 TD_error

</code></pre>
<h2 id="26-4-两者学习方式">26.4 两者学习方式</h2>
<p><code>Actor</code> 想要最大化期望的 <code>reward</code>，在 <code>Actor Critic</code> 算法中，我们用 “比平时好多少” (<code>TD error</code>) 来当做 <code>reward</code>, 所以就是:</p>
<pre><code class="language-python">with tf.variable_scope('exp_v'):
    log_prob = tf.log(self.acts_prob[0, self.a])  # log 动作概率
    self.exp_v = tf.reduce_mean(log_prob * self.td_error)  # log 概率 * TD 方向
with tf.variable_scope('train'):
    # 因为我们想不断增加这个 exp_v (动作带来的额外价值),
    # 所以我们用过 minimize(-exp_v) 的方式达到
    # maximize(exp_v) 的目的
    self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)

</code></pre>
<p><code>Critic</code> 的更新很简单，就是像 Q-Learning 那样更新现实和估计的误差 (TD error) 就好了。</p>
<pre><code class="language-python">with tf.variable_scope('squared_TD_error'):
    self.td_error = self.r + GAMMA * self.v_ - self.v
    # TD_error = (r+gamma*V_next) - V_eval
    self.loss = tf.square(self.td_error)

with tf.variable_scope('train'):
    self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)
</code></pre>
<h2 id="26-5-每回合算法">26.5 每回合算法</h2>
<pre><code class="language-python">for i_episode in range(MAX_EPISODE):
    s = env.reset()
    t = 0
    track_r = []    # 每回合的所有奖励
    while True:
        if RENDER: env.render()

        a = actor.choose_action(s)

        s_, r, done, info = env.step(a)

        if done: r = -20    # 回合结束的惩罚

        track_r.append(r)

        td_error = critic.learn(s, r, s_)  # Critic 学习
        actor.learn(s, a, td_error)     # Actor 学习

        s = s_
        t += 1

        if done or t &gt;= MAX_EP_STEPS:
            # 回合结束, 打印回合累积奖励
            ep_rs_sum = sum(track_r)
            if 'running_reward' not in globals():
                running_reward = ep_rs_sum
            else:
                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05
            if running_reward &gt; DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering
            print("episode:", i_episode, "  reward:", int(running_reward))
            break
</code></pre>
<p>建立神经网络的详细流程请直接看代码更直观，其他方面的代码也不是重点，所以直接看代码很好懂。</p>
<p>由于更新时的<strong>网络相关性</strong>、<strong>state 相关性</strong>，Actor-Critic 很难收敛。如果同学们对这份代码做过修改，并且达到了好的收敛性。</p>
<h1 id="27-什么是deep-deterministic-policy-gradient-ddpg">27. 什么是Deep Deterministic Policy Gradient(DDPG)</h1>
<p>今天我们会来说说强化学习中的一种 Actor-Critic 的提升方式 Deep Deterministic Policy Gradient (DDPG)，DDPG 最大的优势就是能够在连续动作上更有效地学习。</p>
<h2 id="27-1-拆分细讲">27.1 拆分细讲</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113192407568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>它吸收了 Actor-Critic 让 Policy gradient 单步更新的精华，而且还吸收让计算机学会玩游戏的 DQN 的精华，合并成了一种新算法，叫做 Deep Deterministic Policy Gradient。那 DDPG 到底是什么样的算法呢，我们就拆开来分析，我们将 DDPG 分成 <code>Deep</code> 和 <code>Deterministic Policy Gradient</code>，然后 <code>Deterministic Policy Gradient</code> 又能被细分为 <code>Deterministic</code> 和 <code>Policy Gradient</code>，接下来，我们就开始一个个分析。</p>
<h2 id="27-2-deep和dqn">27.2 Deep和DQN</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113192600882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Deep 顾名思义，就是走向更深层次，我们在 DQN 的影片当中提到过，使用一个记忆库和两套结构相同，但参数更新频率不同的神经网络能有效促进学习。那我们也把这种思想运用到 DDPG 当，使 DDPG 也具备这种优良形。但是 DDPG 的神经网络形式却比 DQN 的要复杂一点点。</p>
<h2 id="27-3-deterministic-policy-gradient">27.3 Deterministic Policy Gradient</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113192701133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Policy gradient 我们也在之前的短片中提到过，相比其他的强化学习方法，它能被用来在连续动作上进行动作的筛选。而且筛选的时候是根据所学习到的动作分布随机进行筛选，而 Deterministic 有点看不下去，Deterministic 说: 我说兄弟，你其实在做动作的时候没必要那么不确定，那么犹豫嘛，反正你最终都只是要输出一个动作值，干嘛要随机，铁定一点，有什么不。所以 Deterministic 就改变了输出动作的过程，斩钉截铁的只在连续动作上输出一个动作。</p>
<h2 id="27-4-ddpg神经网络">27.4 DDPG神经网络</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113192825892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>现在我们来说说 DDPG 中所用到的神经网络。它其实和我们之前提到的 Actor-Critic 形式差不多，也需要有基于 策略 Policy 的神经网络和基于价值 Value 的神经网络，但是为了体现 DQN 的思想，每种神经网络我们都需要再细分为两个，Policy Gradient 这边，我们有估计网络和现实网络，估计网络用来输出实时的动作，供 actor 在现实中。而现实网络则是用来更新价值网络系统的。所以我们再来看看价值系统这边，我们也有现实网络和估计网络，他们都在输出这个状态的价值，而输入端却有不同，状态现实网络这边会拿着从动作现实网络来的动作加上状态的观测值加以分析，而状态估计网络则是拿着当时 Actor 施加的动作当做输入。在实际运用中，DDPG 的这种做法的确带来了更有效的学习过程。</p>
<h1 id="28-deep-deterministic-policy-gradient-ddpg-tensorflow">28. Deep Deterministic Policy Gradient(DDPG)—TensorFlow</h1>
<h2 id="28-1-要点">28.1 要点</h2>
<p><strong>一句话概括 DDPG:</strong> Google DeepMind 提出的一种使用 <code>Actor-Critic</code> 结构，但是输出的不是行为的概率，而是具体的行为，用于连续动作 (continuous action) 的预测。<code>DDPG</code> 结合了之前获得成功的 <code>DQN</code> 结构，提高了 <code>Actor Critic</code> 的稳定性和收敛性。</p>
<p>因为 <code>DDPG</code> 和 <code>DQN</code> 还有 <code>Actor Critic</code> 很相关，所以最好这两者都了解下，对于学习 <code>DDPG</code> 很有帮助。</p>
<p>下面是这节内容的效果提前看:https://morvanzhou.github.io/static/results/reinforcement-learning/Pendulum%20DDPG.mp4</p>
<h2 id="28-2-算法">28.2 算法</h2>
<p><code>DDPG</code> 的算法实际上就是一种 <code>Actor-Critic</code>，我在上一篇中简短地介绍了 <code>Actor-Critic</code> 的算法。</p>
<p><img src="https://img-blog.csdnimg.cn/2020011320061733.png" alt="在这里插入图片描述"></p>
<p>关于 <code>Actor</code> 部分, 他的参数更新同样会涉及到 <code>Critic</code>，上面是关于 <code>Actor</code> 参数的更新，它的前半部分 <code>grad[Q]</code> 是从 <code>Critic</code> 来的，这是在说: <strong>这次 <code>Actor</code> 的动作要怎么移动, 才能获得更大的 <code>Q</code></strong>，而后半部分 <code>grad[u]</code> 是从 <code>Actor</code> 来的，这是在说: <strong><code>Actor</code> 要怎么样修改自身参数，使得 <code>Actor</code> 更有可能做这个动作</strong>。所以两者合起来就是在说: <strong><code>Actor</code> 要朝着更有可能获取大 <code>Q</code> 的方向修改动作参数了</strong>。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113200721884.png" alt="在这里插入图片描述"></p>
<p>上面这个是关于 <code>Critic</code> 的更新，它借鉴了 <code>DQN</code> 和 <code>Double Q learning</code> 的方式，有两个计算 <code>Q</code> 的神经网络，<code>Q_target</code> 中依据下一状态，用 <code>Actor</code> 来选择动作，而这时的 <code>Actor</code> 也是一个 <code>Actor_target</code> (有着 Actor 很久之前的参数)。使用这种方法获得的 <code>Q_target</code> 能像 <code>DQN</code> 那样切断相关性，提高收敛性。</p>
<h2 id="28-3-主结构">28.3 主结构</h2>
<p><strong>注意，录视频的时候代码有个地方有小错误，以下部分和视频中有些地方不同，特别是计算 <code>Actor</code> 更新的时候。所以请以文字描述中的为准。</strong></p>
<p>我们用 Tensorflow 搭建神经网络，主结构可以见这个 tensorboard 的出来的图。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113200857267.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>看起来很复杂吧，没关系，我们一步步，拆开来看就容易。首先看看 <code>Actor</code> 和 <code>Critic</code> 中各有什么结构。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113200929777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>其搭建的代码部分在这:</p>
<pre><code class="language-python">class Actor(object):
    def __init__(self):
        ...
        with tf.variable_scope('Actor'):
            # 这个网络用于及时更新参数
            self.a = self._build_net(S, scope='eval_net', trainable=True)
            # 这个网络不及时更新参数, 用于预测 Critic 的 Q_target 中的 action
            self.a_ = self._build_net(S_, scope='target_net', trainable=False)
        ...

class Critic(object):
    def __init__(self):
        with tf.variable_scope('Critic'):
            # 这个网络是用于及时更新参数
            # 这个 a 是来自 Actor 的, 
            # 但是 self.a 在更新 Critic 的时候是之前选择的 a 而不是来自 Actor 的 a
            self.a = a  
            self.q = self._build_net(S, self.a, 'eval_net', trainable=True)
            # 这个网络不及时更新参数, 用于给出 Actor 更新参数时的 Gradient ascent 强度
            self.q_ = self._build_net(S_, a_, 'target_net', trainable=False)
</code></pre>
<h2 id="28-5-actor-critic">28.5 Actor Critic</h2>
<p>有了对 <code>Actor Critic</code> 每个里面各两个神经网络结构的了解，我们再来具体看看他们是如何进行交，传递信息的。我们从 <code>Actor</code> 的学习更新方式开始说起。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113201114160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这张图我们就能一眼看穿 <code>Actor</code> 的更新到底基于了哪些东西. 可以看出，它使用了两个 <code>eval_net</code>，所以 <code>Actor</code> class 中用于 train 的代码我们这样写:</p>
<pre><code class="language-python">with tf.variable_scope('policy_grads'):
    # 这是在计算 (dQ/da) * (da/dparams)
    self.policy_grads = tf.gradients(
        ys=self.a, xs=self.e_params, # 计算 ys 对于 xs 的梯度
        grad_ys=a_grads # 这是从 Critic 来的 dQ/da
    )
with tf.variable_scope('A_train'):
    # 负的学习率为了使我们计算的梯度往上升, 和 Policy Gradient 中的方式一个性质
    opt = tf.train.AdamOptimizer(-self.lr)
    # 对 eval_net 的参数更新
    self.train_op = opt.apply_gradients(zip(self.policy_grads, self.e_params))
</code></pre>
<p>同时下面也提到的传送给 <code>Actor</code> 的 <code>a_grad</code> 应该用 Tensorflow 怎么计算。这个 <code>a_grad</code> 是 <code>Critic</code> class 里面的，这个 <code>a</code> 是来自 <code>Actor</code> 根据 <code>S</code> 计算而来的:</p>
<pre><code class="language-python">with tf.variable_scope('a_grad'):
    self.a_grads = tf.gradients(self.q, a)[0]  # dQ/da
</code></pre>
<p>而在 <code>Critic</code> 中，我们用的东西简单一点。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113201254790.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>下面就是 <code>Critic</code> 更新时的代码了。</p>
<pre><code class="language-python"># 计算 target Q
with tf.variable_scope('target_q'):
    # self.q_ 根据 Actor 的 target_net 来的
    self.target_q = R + self.gamma * self.q_

# 计算误差并反向传递误差
with tf.variable_scope('TD_error'):
    # self.q 又基于 Actor 的 target_net
    self.loss = tf.reduce_mean(tf.squared_difference(self.target_q, self.q))

with tf.variable_scope('C_train'):
    self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)
</code></pre>
<h2 id="28-6-记忆库mmeory">28.6 记忆库Mmeory</h2>
<p>以下是关于类似于 <code>DQN</code> 中的记忆库代码，我们用一个 <code>class</code> 来建立。关于 <code>Memory</code> 的详细算法，请直接去莫烦的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG.py" target="_blank" rel="noopener">Github</a> 中看，这样更简单。</p>
<pre><code class="language-python">class Memory(object):
    def __init__(self, capacity, dims):
        """用 numpy 初始化记忆库"""

    def store_transition(self, s, a, r, s_):
        """保存每次记忆在 numpy array 里"""

    def sample(self, n):
        """随即从记忆库中抽取 n 个记忆进行学习"""
</code></pre>
<h2 id="28-7-每回合算法">28.7 每回合算法</h2>
<p>这里的回合算法只提到了最重要的部分，省掉了一些没必要的，有助理解。</p>
<pre><code class="language-python">var = 3  # 这里初始化一个方差用于增强 actor 的探索性

for i in range(MAX_EPISODES):
    # ...
    for j in range(MAX_EP_STEPS):
        ...

        a = actor.choose_action(s)
        a = np.clip(np.random.normal(a, var), -2, 2) # 增强探索性
        s_, r, done, info = env.step(a)

        M.store_transition(s, a, r / 10, s_)   # 记忆库

        if M.pointer &gt; MEMORY_CAPACITY: # 记忆库头一次满了以后
            var *= .9998    # 逐渐降低探索性
            b_M = M.sample(BATCH_SIZE)
            # ...   # 将 b_M 拆分成下面的输入信息
            critic.learn(b_s, b_a, b_r, b_s_)
            actor.learn(b_s)

        s = s_

        if j == MAX_EP_STEPS-1:
            break

</code></pre>
<p>我也用这套 DDPG 测试过自己写的机器手臂的环境，发现效果也还行。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/experiment_arm.mp4</p>
<h2 id="28-8-简化版代码">28.8 简化版代码</h2>
<p>后来我在回过头来看代码，结果发现计算 <code>Actor</code> 更新时有点小问题，所以就修改了之前的代码。但是修改后我觉得，代码变得累赘了，所以我觉得再重写一个，简化所有流程。能看到这一个板块的朋友们有没有感到绝望(看了那么久上面的代码，结果有个更简单的)。</p>
<h1 id="29-什么是asynchronous-advantage-actor-critic-a3c">29. 什么是Asynchronous Advantage Actor-Critic (A3C)</h1>
<p>今天我们会来说说强化学习中的一种有效利用计算资，并且能提升训练效用的算法，Asynchronous Advantage Actor-Critic，简称 A3C。</p>
<h2 id="29-1-平行宇宙">29.1 平行宇宙</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113202009737.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这样是不是感觉特别有效。让你看看更有效率的，那就想想3个你同时在写作业，一共3题，每人做一题，只用了1/3 的时间就把作业做</p>
<h2 id="29-2-平行训练">29.2 平行训练</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113203002229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这就是传说中的 A3C。A3C 其实只是这种平行方式的一种而已，它采用的是我们之前提到的 Actor-Critic 的形式。为了训练一对 Actor 和 Critic，我们将它复制多份红色的，然后同时放在不同的平行宇宙当中，让他们各自玩各。然后每个红色副本都悄悄告诉黑色的 Actor-Critic 自己在那边的世界玩得怎么，有哪些经验值得分享。然后还能从黑色的 Actor-Critic 这边再次获取综合考量所有副本经验后的通关秘籍。这样一来一，形成了一种有效率的强化学习方式。</p>
<h2 id="29-3-多核训练">29.3 多核训练</h2>
<p><img src="https://img-blog.csdnimg.cn/20200113203203864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们知道目前的计算机多半是有双核、4核，甚至 6、8核。一般的学习方法，我们只能让机器人在一个核上面玩耍。但是如果使用 A3C 的方，我们可以给他们安排去不同的核，并行运算。实验结果，这样的计算方式往往比传统的方式快上好多倍。 那我们也多用用这样的红利吧。</p>
<h1 id="30-asynchronous-advantage-actor-critic-a3c-tensorflow">30. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow</h1>
<h2 id="30-1-要点">30.1 要点</h2>
<p><strong>一句话概括 A3C:</strong> Google DeepMind 提出的一种解决 <code>Actor-Critic</code> 不收敛问题的算法。它会创建多个并行的环境，让多个拥有副结构的 agent 同时在这些并行环境上更新主结构中的参数。并行中的 agent 们互不干扰，而主结构的参数更新受到副结构提交更新的不连续性干扰, 所以更新的相关性被降，收敛性提高。</p>
<p>下面是这节内容的效果提前看:https://morvanzhou.github.io/static/results/reinforcement-learning/Pendulum%20A3C.mp4</p>
<h2 id="30-2-算法">30.2 算法</h2>
<p><code>A3C</code> 的算法实际上就是将 <code>Actor-Critic</code> 放在了多个线程中进行同步训练。可以想象成几个人同时在玩一样的游戏，而他们玩游戏的经验都会同步上传到一个中央大脑。然后他们又从中央大脑中获取最新的玩游戏方法。</p>
<p><strong>这样, 对于这几个人，他们的好处是:</strong> 中央大脑汇集了所有人的经验，是最会玩游戏的一个，他们能时不时获取到中央大脑的必杀招，用在自己的场景中。</p>
<p><strong>对于中央大脑的好处是:</strong> 中央大脑最怕一个人的连续性更新，不只基于一个人推送更新这种方式能打消这种连续性。使中央大脑不必有用像 <code>DQN</code>、<code>DDPG</code> 那样的记忆库也能很好的更新。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113203551650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>为了达到这个目的, 我们要有两套体系，可以看作中央大脑拥有 <code>global_net</code> 和他的参数，每位玩家有一个 <code>global_net</code> 的副本 <code>local_net</code>，可以定时向 <code>global_net</code> 推送更新，然后定时从 <code>global_net</code> 那获取综合版的更新。</p>
<p>如果在 tensorboard 中查看我们今天要建立的体系，这就是你会看到的。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113203644113.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><code>W_0</code> 就是第0个 worke，每个 worker 都可以分享 <code>global_net</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113203719155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>如果我们调用 <code>sync</code> 中的 <code>pull</code>，这个 worker 就会从 <code>global_net</code> 中获取到最新的参数。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113203808243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>如果我们调用 <code>sync</code> 中的 <code>push</code>，这个 worker 就会将自己的个人更新推送去 <code>global_net</code>。</p>
<p>这次我们使用一个连续动作的环境 Pendulum 举例. 如果直接看所有代码, <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/10_A3C/A3C_continuous_action.py" target="_blank" rel="noopener">请看我的 Github</a>，如果你处理的是一个离散动作环境，可以参考这个Github 中的[这个文件](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/10_A3C/A3C_discrete_action.py。</p>
<p>接下来我们就开始定义连续动作的 A3C 。</p>
<h2 id="30-3-主结构">30.3 主结构</h2>
<p>我们用 Tensorflow 搭建神经网络，对于我们的 Actor，tensorboard 中可以看清晰的看到我们是如果搭建的:</p>
<p><img src="https://img-blog.csdnimg.cn/20200113203933273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们使用了 Normal distribution 来选择动作，所以在搭建神经网络的时候，<code>actor</code> 这边要输出动作的均值和方差。然后放入 Normal distribution 去选择动。计算 <code>actor</code> loss 的时候我们还需要使用到 <code>critic</code> 提供的 <code>TD error</code> 作为 gradient ascent 的导。</p>
<p><img src="https://morvanzhou.github.io/static/results/reinforcement-learning/6-3-6.png" alt="Asynchronous Advantage Actor-Critic (A3C) (Tensorflow)"></p>
<p><code>critic</code> 很简单啦，只需要得到他对于 state 的价值就好了. 用于计算 <code>TD error</code>。</p>
<h2 id="30-4-actor-critic网络">30.4 Actor Critic网络</h2>
<p>其搭建的代码部分在这，因为写下来全部代码比较眼，所以会有点伪代码 (如果想一次性看全部，请去我的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/10_A3C/A3C_continuous_action.py" target="_blank" rel="noopener">Github</a>):</p>
<p>我们将 <code>Actor</code> 和 <code>Critic</code> 合并成一整套系统，这样方便运行。</p>
<pre><code class="language-python"># 这个 class 可以被调用生成一个 global net.
# 也能被调用生成一个 worker 的 net, 因为他们的结构是一样的,
# 所以这个 class 可以被重复利用.
class ACNet(object):
    def __init__(self, globalAC=None):
        # 当创建 worker 网络的时候, 我们传入之前创建的 globalAC 给这个 worker
        if 这是 global:   # 判断当下建立的网络是 local 还是 global
            with tf.variable_scope('Global_Net'):
                self._build_net()
        else:
            with tf.variable_scope('worker'):
                self._build_net()

            # 接着计算 critic loss 和 actor loss
            # 用这两个 loss 计算要推送的 gradients

            with tf.name_scope('sync'):  # 同步
                with tf.name_scope('pull'):
                    # 更新去 global
                with tf.name_scope('push'):
                    # 获取 global 参数

    def _build_net(self):
        # 在这里搭建 Actor 和 Critic 的网络
        return 均值, 方差, state_value

    def update_global(self, feed_dict):
        # 进行 push 操作

    def pull_global(self):
        # 进行 pull 操作

    def choose_action(self, s):
        # 根据 s 选动作
</code></pre>
<p>这些只是在创建网络而已，<code>worker</code> 还有属于自己的 class，用来执行在每个线程里的工作。</p>
<h2 id="30-5-worker">30.5 Worker</h2>
<p>每个 <code>worker</code> 有自己的 class，class 里面有他的工作内容 <code>work</code>，看全部请来我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/10_A3C/A3C_continuous_action.py" target="_blank" rel="noopener">Github</a>。</p>
<pre><code class="language-python">class Worker(object):
    def __init__(self, name, globalAC):
        self.env = gym.make(GAME).unwrapped # 创建自己的环境
        self.name = name    # 自己的名字
        self.AC = ACNet(name, globalAC) # 自己的 local net, 并绑定上 globalAC

    def work(self):
        # s, a, r 的缓存, 用于 n_steps 更新
        buffer_s, buffer_a, buffer_r = [], [], []
        while not COORD.should_stop() and GLOBAL_EP &lt; MAX_GLOBAL_EP:
            s = self.env.reset()

            for ep_t in range(MAX_EP_STEP):
                a = self.AC.choose_action(s)
                s_, r, done, info = self.env.step(a)

                buffer_s.append(s)  # 添加各种缓存
                buffer_a.append(a)
                buffer_r.append(r)

                # 每 UPDATE_GLOBAL_ITER 步 或者回合完了, 进行 sync 操作
                if total_step % UPDATE_GLOBAL_ITER == 0 or done:
                    # 获得用于计算 TD error 的 下一 state 的 value
                    if done:
                        v_s_ = 0   # terminal
                    else:
                        v_s_ = SESS.run(self.AC.v, {self.AC.s: s_[np.newaxis, :]})[0, 0]

                    buffer_v_target = []  # 下 state value 的缓存, 用于算 TD
                    for r in buffer_r[::-1]:  # 进行 n_steps forward view
                        v_s_ = r + GAMMA * v_s_
                        buffer_v_target.append(v_s_)
                    buffer_v_target.reverse()

                    buffer_s, buffer_a, buffer_v_target = np.vstack(buffer_s), np.vstack(buffer_a), np.vstack(buffer_v_target)

                    feed_dict = {
                        self.AC.s: buffer_s,
                        self.AC.a_his: buffer_a,
                        self.AC.v_target: buffer_v_target,
                    }

                    self.AC.update_global(feed_dict)  # 推送更新去 globalAC
                    buffer_s, buffer_a, buffer_r = [], [], []  # 清空缓存
                    self.AC.pull_global()  # 获取 globalAC 的最新参数

                s = s_
                if done:
                    GLOBAL_EP += 1  # 加一回合
                    break   # 结束这回合
</code></pre>
<h2 id="30-6-worker并行工作">30.6 Worker并行工作</h2>
<p>这里才是真正的重点！Worker 的并行计算。</p>
<pre><code class="language-python">with tf.device("/cpu:0"):
    GLOBAL_AC = ACNet(GLOBAL_NET_SCOPE)  # 建立 Global AC
    workers = []
    for i in range(N_WORKERS):  # 创建 worker, 之后在并行
        workers.append(Worker(GLOBAL_AC))  # 每个 worker 都有共享这个 global AC

COORD = tf.train.Coordinator()  # Tensorflow 用于并行的工具

worker_threads = []
for worker in workers:
    job = lambda: worker.work()
    t = threading.Thread(target=job)  # 添加一个工作线程
    t.start()
    worker_threads.append(t)
COORD.join(worker_threads)  # tf 的线程调度
</code></pre>
<p>我的电脑里可以建立 4个 worke，也就可以把它们放在4个线程中并行探索更新。最后的学习结果可以用这个获取 moving average 的 reward 的图来概括。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113204409164.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>上面讲到的是一个 continuous action 的例子，<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/10_A3C/A3C_continuous_action.py" target="_blank" rel="noopener">全部代码在这里</a>清晰可见。还有一个是 discrete action 的例子. 使用的是 Cartpole 的实验, <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/10_A3C/A3C_discrete_action.py" target="_blank" rel="noopener">代码在这</a>。同时，我还做了一个 A3C 加上 RNN 的例子，同样是用 Pendulum 的例子，<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/10_A3C/A3C_RNN.py" target="_blank" rel="noopener">代码在这</a>。</p>
<h2 id="30-7-机械手臂">30.7 机械手臂</h2>
<p>我也用这套 A3C 测试过自己写的机器手臂的环境，发现效果也还行。有兴趣的朋友可以<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/experiments/Robot_arm" target="_blank" rel="noopener">看到这里</a>。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/experiment_arm.mp4</p>
<p>有很多人留言说想要我做一个关于这个机器手臂的教程，不负众望，你可以在<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1/" target="_blank" rel="noopener">这里</a> 看到我怎么从零开始、手写环境、debug 测试，来制作一个强化学习的机器手臂。</p>
<h2 id="30-8-multiprocessing-a3c">30.8 multiprocessing+A3C</h2>
<p>除此之外, 我心里一直有一个疙瘩，因为这个 A3C 中，我用的是 python 的 <code>threading</code>，懂 python 的朋友知道，<code>threading</code> 有 GIL，运算速度是问题，我的 CPU 都不是满格的。我一直想把这个 A3C 代码移植去 <code>multiprocessing</code>， 提高效率。但是 Tensorflow 的 <code>session</code> 就是和 <code>multiprocessing</code> 不兼容，Global Net 做不好。 怎么办？</p>
<p><a href="https://www.tensorflow.org/deploy/distributed" target="_blank" rel="noopener">Distributed Tensorflow</a> 是一个备选方案。但是这个要求你是在计算机集群上做，不然速度上还不如这个 <code>threading</code> 的 A3C。</p>
<p>和 Tensorflow 一样，我做过一些 Pytorch 的教程，pytorch 也是做神经网络的。但是它是支持 <code>multiprocessing</code> 的。我专门开了一个 repo, 把 Pytorch + multiprocessing 的代码<a href="https://github.com/MorvanZhou/pytorch-A3C" target="_blank" rel="noopener">分享了出来</a>。这会儿，CPU 满格。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113204954121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="31-distributed-proximal-policy-optimization-dppo-tensorflow">31. Distributed Proximal Policy Optimization (DPPO) —Tensorflow</h1>
<h2 id="31-1-要点">31.1 要点</h2>
<p>根据 OpenAI 的<a href="https://blog.openai.com/openai-baselines-ppo/" target="_blank" rel="noopener">官方博客</a>，DPPO 已经成为他们在强化学习上的默认算法。<strong>如果一句话概括 PPO: OpenAI 提出的一种解决 Policy Gradient 不好确定 Learning rate (或者 Step size) 的问题。因为如果 step size 过大，学出来的 Policy 会一直乱动，不会收敛，但如果 Step Size 太小，对于完成训练，我们会等到绝望。DPPO 利用 New Policy 和 Old Policy 的比例，限制了 New Policy 的更新幅度，让 Policy Gradient 对稍微大点的 Step size 不那么敏感。</strong></p>
<p>因为 PPO 是基于 <code>Actor-Critic</code> 算。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/Pendulum%20A3C.mp4</p>
<h2 id="31-2-openai-和-deepmind-的-demo">31.2 OpenAI 和 DeepMind 的 Demo</h2>
<p>OpenAI 的 <a href="https://blog.openai.com/openai-baselines-ppo/#ppo" target="_blank" rel="noopener">Demo</a>，视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/6-4-demo_openai.mp4</p>
<p>DeepMind 的 <a href="https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/" target="_blank" rel="noopener">Demo</a>:</p>
<p><img src="https://morvanzhou.github.io/static/results/reinforcement-learning/6-4-demo_google2.gif" alt=""></p>
<p><img src="https://morvanzhou.github.io/static/results/reinforcement-learning/6-4-demo_google1.gif" alt=""></p>
<p>看 Demo 他们都说 PPO 在复杂环境中有更好的表现。那我也就提起性子，把 papers 看了一遍。</p>
<h2 id="31-3-算法">31.3 算法</h2>
<p>PPO 的前生是 OpenAI 发表的 <a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="noopener">Trust Region Policy Optimization</a>，但是 Google DeepMind 看过 OpenAI 关于 Trust Region Policy Optimization 的 conference 后，却抢在 OpenAI 前面 (2017年7月7号) 把 <a href="https://arxiv.org/abs/1707.02286" target="_blank" rel="noopener">Distributed PPO</a>给先发布了。我觉得 DeepMind 有点抢，这让 OpenAI 有点难堪。不过 OpenAI 还是在 2017年7月20号 发表了一份拿得出手的 <a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">PPO 论文</a> (估计是突然看到了 Google 抢了自己的东西，所以赶紧把自己的也发了)。</p>
<p>所以我也总结了一下 DeepMind 和 OpenAI 的两篇 papers，基于他们两者的优势，写下了这份教程，还有练习代码。</p>
<p>OpenAI PPO 论文里给出的算法… 写得也太简单了(注意他们这个 PPO 算法应该算是单线程的):</p>
<p><img src="https://img-blog.csdnimg.cn/20200113205655412.png" alt="在这里插入图片描述"></p>
<p>看了上面的主结构，觉得少了很多东西。这让我直接跑去 DeepMind 的 Paper 看他们总结 OpenAI conference 上的 PPO 的代码:</p>
<p><img src="https://img-blog.csdnimg.cn/20200113205731452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>总的来说 PPO 是一套 Actor-Critic 结构，Actor 想<strong>最大化</strong> <code>J_PPO</code>，Critic 想<strong>最小化</strong> <code>L_B</code>。Critic 的 loss 好说，就是减小 TD error。而 Actor 的就是在 old Policy 上根据 Advantage (TD error) 修改 new Policy， advantage 大的时候，修改幅度大，让 new Policy 更可能发生。而且他们附加了一个 KL Penalty (惩罚，不懂的同学搜一下 KL divergence)，简单来说，如果 new Policy 和 old Policy 差太多，那 KL divergence 也越大，我们不希望 new Policy 比 old Policy 差太多，如果会差太多，就相当于用了一个大的 Learning rate，这样是不，难收。</p>
<p>关于 DeepMind 在这篇 paper 中提出的 DPPO 算法，和怎么样把 OpenAI 的 PPO 多线程。我们之后在下面细说，我们先把简单的 PPO 算法给实现。</p>
<h2 id="31-4-简单的-ppo-主结构">31.4 简单的 PPO 主结构</h2>
<p>我们用 Tensorflow 搭建神经网络，tensorboard 中可以看清晰的看到我们是如果搭建的:</p>
<p><img src="https://img-blog.csdnimg.cn/20200113205954943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>图中的 <code>pi</code> 就是我们的 Actor 了。每次要进行 PPO 更新 Actor 和 Critic 的时候，我们有需要将 <code>pi</code> 的参数复制给 <code>oldpi</code>。这就是 <code>update_oldpi</code> 这个 operation 在做的事。Critic 和 Actor 的内部结构，我们不会打开细说了。因为就是一堆的神经网络而已。这里的 Actor 使用了 normal distribution 正态分布输出动。</p>
<pre><code class="language-python">class PPO(object):
    def __init__(self):
        # 建 Actor Critic 网络
        # 搭计算图纸 graph
    def update(self, s, a, r):
        # 更新 PPO
    def choose_action(self, s):
        # 选动作
    def get_v(self, s):
        # 算 state value
</code></pre>
<p>而这个 <code>PPO</code> 和 <code>env</code> 环境的互动可以简化成这样。</p>
<pre><code class="language-python">ppo = PPO()
for ep in range(EP_MAX):
    s = env.reset()
    buffer_s, buffer_a, buffer_r = [], [], []
    for t in range(EP_LEN):
        env.render()
        a = ppo.choose_action(s)
        s_, r, done, _ = env.step(a)
        buffer_s.append(s)
        buffer_a.append(a)
        buffer_r.append((r+8)/8)    # normalize reward, 发现有帮助
        s = s_

        # 如果 buffer 收集一个 batch 了或者 episode 完了
        if (t+1) % BATCH == 0 or t == EP_LEN-1:
            # 计算 discounted reward
            v_s_ = ppo.get_v(s_)
            discounted_r = []
            for r in buffer_r[::-1]:
                v_s_ = r + GAMMA * v_s_
                discounted_r.append(v_s_)
            discounted_r.reverse()

            bs, ba, br = batch(buffer_s, buffer_a, discounted_r)
            # 清空 buffer
            buffer_s, buffer_a, buffer_r = [], [], []
            ppo.update(bs, ba, br)  # 更新 PPO
</code></pre>
<p>了解了这些更新步，我们就来看看如何更新我们的 <code>PPO</code>。我们更新 Critic 的时候是根据刚刚计算的 <code>discounted_r</code> 和自己分析出来的 <code>state value</code> 这两者的差 (advantage)。然后最小化这个差值:</p>
<pre><code class="language-python">class PPO:
    def __init__(self):
         # discounted reward - Critic 出来的 state value
        self.advantage = self.tfdc_r - self.v
        self.closs = tf.reduce_mean(tf.square(self.advantage))
        self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)
</code></pre>
<p>在更新 Actor 的时候，其实有两种方法，一种是用之前提到的 KL penalty 来更新。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113210624503.png" alt="在这里插入图片描述"></p>
<p>我在代码中也写上的这种方式的计算图纸要怎么搭, 不过还有一种是 OpenAI 在 <a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">PPO 这篇 paper</a> 中提到的 <code>clipped surrogate objective</code>, <code>surrogate objective</code> 就是这个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>E</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy="false">[</mo><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mrow><msub><msub><mi>π</mi><mi>θ</mi></msub><mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\hat E_t[\frac{\pi_\theta (a_t \vert s_t)}{ {\pi_\theta}_{old} (a_t \vert s_t)} \hat A_t]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.27337142857142854em;"><span style="top:-2.277342857142857em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.22265714285714283em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight">∣</span><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight">∣</span><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">A</span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.11110999999999999em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>。他们实验中得出的结论说: <code>clipped surrogate objective</code> 要比 <code>KL penalty</code> 形式好。那 <code>clipped surrogate objective</code> 到底是什么呢？其实就是限制了 surrogate 的变化幅，和 <code>KL</code> 的规则差不多。</p>
<p><img src="https://img-blog.csdnimg.cn/20200113210906645.png" alt="在这里插入图片描述"></p>
<p>这里的 <code>r(theta)</code> 是 (New Policy/Old Policy) 的比例，和前面的公式一样。我在代码中把这两种都写上了，如果觉得我这些代码省略的很严重，请直接前往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/simply_PPO.py" target="_blank" rel="noopener">Github 看全套代码</a>。</p>
<pre><code class="language-python">class PPO:
    def __init__(self):
        self.tfa = tf.placeholder(tf.float32, [None, A_DIM], 'action')
        self.tfadv = tf.placeholder(tf.float32, [None, 1], 'advantage')
        with tf.variable_scope('loss'):
            with tf.variable_scope('surrogate'):
                ratio = pi.prob(self.tfa) / oldpi.prob(self.tfa)
                surr = ratio * self.tfadv  # surrogate objective
            if METHOD['name'] == 'kl_pen':  # 如果用 KL penatily
                self.tflam = tf.placeholder(tf.float32, None, 'lambda')
                kl = kl_divergence(oldpi, pi)
                self.kl_mean = tf.reduce_mean(kl)
                self.aloss = -(tf.reduce_mean(surr - self.tflam * kl))
            else:  # 如果用 clipping 的方式
                self.aloss = -tf.reduce_mean(tf.minimum(
                    surr,
                    tf.clip_by_value(ratio, 1.-METHOD['epsilon'], 1.+METHOD['epsilon'])*self.tfadv))

        with tf.variable_scope('atrain'):
            self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)
</code></pre>
<p>好了, 接下来就是最重要的更新 PPO 时间了，同样，如果觉得我这些代码省略的很严重，请直接前往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/simply_PPO.py" target="_blank" rel="noopener">Github 看全套代码</a>。注意的是，这个 <code>update</code> 的步骤里, 我们用 <code>for loop</code> 更新了很多遍 Actor 和 Critic，在 loop 之前，<code>pi</code> 和 <code>old_pi</code> 是一样的, 每次 loop 的之，<code>pi</code> 会变动，而 <code>old_ pi</code> 不变，这样这个 surrogate  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>E</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy="false">[</mo><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mrow><msub><msub><mi>π</mi><mi>θ</mi></msub><mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\hat E_t[\frac{\pi_\theta (a_t \vert s_t)}{ {\pi_\theta}_{old} (a_t \vert s_t)} \hat A_t]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.27337142857142854em;"><span style="top:-2.277342857142857em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.22265714285714283em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight">∣</span><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight">∣</span><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">A</span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.11110999999999999em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span> 就会开始变动了。这就是 PPO 的精辟。</p>
<pre><code class="language-python">class PPO:
    def update(self, s, a, r):
        # 先要将 oldpi 里的参数更新 pi 中的
        self.sess.run(self.update_oldpi_op)

        # 更新 Actor 时, kl penalty 和 clipping 方式是不同的
        if METHOD['name'] == 'kl_pen':  # 如果用 KL penalty
            # 之后根据 kl 的值, 调整 METHOD['lam'] 这个参数
            for _ in range(A_UPDATE_STEPS):
                _, kl = self.sess.run(
                        [self.atrain_op, self.kl_mean],
                        {self.tfs: s, self.tfa: a, self.tfadv: adv, self.tflam: METHOD['lam']})
        else:  # 如果用 clipping 的方法
            [self.sess.run(self.atrain_op, {self.tfs: s, self.tfa: a, self.tfadv: adv}) for _ in range(A_UPDATE_STEPS)]

        # 更新 Critic 的时候, 他们是一样的
        [self.sess.run(self.ctrain_op, {self.tfs: s, self.tfdc_r: r}) for _ in range(C_UPDATE_STEPS)]
</code></pre>
<p>最后我们看一张学习的效果图:</p>
<p><img src="https://img-blog.csdnimg.cn/20200113211320674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>好了这就是整个 PPO 的主要流程了，其他的步骤都没那么重要了，可以直接在我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/simply_PPO.py" target="_blank" rel="noopener">Github 看全套代码</a> 中轻松弄懂。接下来我们看看怎么样把这个单线程的 PPO 变到多线程去 (Distributed PPO)。</p>
<h2 id="31-5-distributed-ppo">31.5 Distributed PPO</h2>
<p>Google DeepMind 提出来了一套和 A3C (<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/6-3-A3C/" target="_blank" rel="noopener">A3C 教程见这里</a>) 类似的并行 PPO 算。看了他们 <a href="https://arxiv.org/abs/1707.02286" target="_blank" rel="noopener">paper</a> 中的这个 DPPO 算法后，我觉得….不好编！取而代之，我觉得如果采用 OpenAI 的思路，用他那个 “简陋” 伪代码，但是弄成并行计算倒是好弄点。所以我就结合了 DeepMind 和 OpenAI，取他们的精华，写下了<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/DPPO.py" target="_blank" rel="noopener">这份 DPPO 的代码</a>。</p>
<p>总结一下我是怎么写的:</p>
<ul>
<li>用 OpenAI 提出的 <code>Clipped Surrogate Objective</code></li>
<li>使用多个线程 (workers) 平行在不同的环境中收集数据</li>
<li>workers 共享一个 Global PPO</li>
<li>workers 不会自己算 PPO 的 gradients，不会像 A3C 那样推送 Gradients 给 Global net</li>
<li>workers 只推送自己采集的数据给 Global PPO</li>
<li>Global PPO 拿到多个 workers 一定批量的数据后进行更新 (更新时 worker 停止采集)</li>
<li>更新完后, workers 用最新的 Policy 采集数据</li>
</ul>
<p>我使用的代码框架和<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/6-3-A3C/" target="_blank" rel="noopener">自己的 A3C</a> 框架有点像。这个 DPPO 具体的代码我不会在这边描述了，请直接看到<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/DPPO.py" target="_blank" rel="noopener">我写的代码</a>吧。</p>
<p>不过有些细节我想提前提一下，方便你们看代码:</p>
<ul>
<li>我用到了 python <code>threading</code> 当中的 Event 功能，用来控制 PPO 的更新时间和 <code>worker</code> 停止工作的时间</li>
<li>使用了 <code>threading</code> 中的 <code>Queue</code> 来存放 worker 收集的数据，发现用 python 的列表也可以达到一样效果，计算时间上没太多差别。</li>
<li>更新 PPO 的时候，我采用的是 DeepMind 的 <code>for loop</code> 形式。</li>
</ul>
<p>我也用这套 DPPO 测试过自己写的机器手臂的环境，发现效果也还行。</p>
<p>视频传送门：https://morvanzhou.github.io/static/results/reinforcement-learning/experiment_arm.mp4</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 yu_mingm623@163.com </span>
    </div>
</article>


<p>
    <a  class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>【莫烦Python】强化学习</p>
    <p><span class="copy-title">文章字数:</span><span class="post-count">35.6k</span></p>
    <p><span class="copy-title">本文作者:</span><a  title="郁明敏">郁明敏</a></p>
    <p><span class="copy-title">发布时间:</span>2019-12-08, 12:49:03</p>
    <p><span class="copy-title">最后更新:</span>2020-09-13, 16:21:22</p>
    <!-- <span class="copy-title">原始链接:</span><a class="post-url" href="/data_mining/ml_courses/ml_course_reinforcement_leanring_mofan/" title="【莫烦Python】强化学习">http://www.monsteryu.top/data_mining/ml_courses/ml_course_reinforcement_leanring_mofan/</a> -->
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>





    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js"
        value="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</input>
    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2019.11 路痴大魔王</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class="" id="rocket" ></a>

    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close"  onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div>


<script src="/js/clipboard.min.js?v=1.0.1" ></script>
<script src="/js/clipboard-use.js?v=1.0.1" ></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":400},"mobile":{"show":false},"log":false});</script></body>
<script src="/js/jquery.pjax.js?v=1.0.1" ></script>


<script src="/js/script.js?v=1.0.1" ></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['@郁明敏','#常用网址','#文档学习资源','#统计基础','#hive','#impala','#蓝鲸','#Mac','#chrome','#xshell','#AirFlow','#算法','#数据库','#Java后端学习路线',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: ;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 622px;
    }
    .nav.fullscreen {
        margin-left: -622px;
    }
    .nav-left {
        width: 200px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 552px;
        }
        .nav.fullscreen {
            margin-left: -552px;
        }
        .nav-left {
            width: 160px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 552px;
            margin-left: -552px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 552px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.3;
        background: url("");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
