<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>莫烦-强化学习课程 | 路痴大魔王</title>
  <meta name="keywords" content=" 强化学习 ">
  <meta name="description" content="莫烦-强化学习课程 | 路痴大魔王">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="页面未找到！">
<meta property="og:type" content="website">
<meta property="og:title" content="404">
<meta property="og:url" content="http:&#x2F;&#x2F;www.monsteryu.top&#x2F;404.html">
<meta property="og:site_name" content="路痴大魔王">
<meta property="og:description" content="页面未找到！">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-12-01T14:55:52.181Z">
<meta name="twitter:card" content="summary">


<link rel="icon" href="/img/avatar4.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/github.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1" ></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.0.1" ></script>

</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="false">
  <input class="theme_blog_path" value="">
</div>



<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar.jpg" />
</a>
<div class="author">
    <span>郁明敏</span>
</div>

<div class="icon">
    
        
        <a title="github" href="https://github.com/luch1monster" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-github"></use>
                </svg>
            
        </a>
        
    
        
        <a title="csdn" href="https://blog.csdn.net/LuCh1Monster" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-csdn"></use>
                </svg>
            
        </a>
        
    
        
        <a title="email" href="mailto:yu_mingm623@163.com" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-email"></use>
                </svg>
            
        </a>
        
    
        
        <a title="qq" href="http://wpa.qq.com/msgrd?v=3&uin=442523981&site=qq&menu=yes" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-qq"></use>
                </svg>
            
        </a>
        
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(43)</small></div></li>
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="前端"><i class="fold iconfont icon-right"></i>前端<small>(1)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="hexo">hexo<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
            <li><div data-rel="后端"><i class="fold iconfont icon-right"></i>后端<small>(16)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Python">Python<small>(15)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Hadoop">Hadoop<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
            <li><div data-rel="学习资源">学习资源<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="工具"><i class="fold iconfont icon-right"></i>工具<small>(9)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Chrome">Chrome<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Markdown">Markdown<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Notepad">Notepad<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="JetBrains">JetBrains<small>(3)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Sublime">Sublime<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="XShell">XShell<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="GitHub">GitHub<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="数据库"><i class="fold iconfont icon-right"></i>数据库<small>(5)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="蓝鲸">蓝鲸<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="MySQL">MySQL<small>(2)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Hive">Hive<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Impala">Impala<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
            <li><div data-rel="数据挖掘"><i class="fold iconfont icon-right"></i>数据挖掘<small>(4)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="LR">LR<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="数理统计">数理统计<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="强化学习">强化学习<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
            <li><div data-rel="文献阅读"><i class="fold iconfont icon-right"></i>文献阅读<small>(2)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="强化学习">强化学习<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="随机过程">随机过程<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
            <li><div data-rel="系统"><i class="fold iconfont icon-right"></i>系统<small>(5)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Linux">Linux<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Mac">Mac<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Windows">Windows<small>(3)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    </div>
    <div><a class="about  site_url"  href="/about">关于</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="43">
<input type="hidden" id="yelog_site_word_count" value="78.5k">
<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="#">叶落阁</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode==13){return false;}">
        <input class="search" type="text" placeholder="以 in: 开头进行全文搜索" autocomplete="off"id="local-search-input" >
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none" />
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a class="color3">AirFlow</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">蓝鲸</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Logistic Regression</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">Centos</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">MySQL</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">chrome</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">Requests</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">impala</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">datagrip</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">Flask</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">hadoop</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">GitHub</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">hive</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">Jupyter</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">Python学习资源</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">Linux</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">LGBM</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">GLIBC</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">系统</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">Mac</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">markdown</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">notepad</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">pandas</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">pycharm</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Python常用库</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">impyla</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">强化学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">随机过程</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">sublime</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">windows软件</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">windows</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">xshell</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">3-hexo</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">hexo</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Python小技巧</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Python关键字</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">数据挖掘</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">统计基础</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">Django用户登录</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        <a id="top" class="前端 hexo "
           href="/2019/11/27/3-hexo-usage/"
           data-tag="3-hexo,hexo"
           data-author="郁明敏" >
            <span class="post-title" title="3-hexo主题使用补充">3-hexo主题使用补充</span>
            <span class="post-date" title="2019-11-27 16:53:35">2019/11/27</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/20/use-flask-develop-data-api/"
           data-tag="Flask"
           data-author="郁明敏" >
            <span class="post-title" title="使用Flask RESTful开发数据API接口">使用Flask RESTful开发数据API接口</span>
            <span class="post-date" title="2019-12-20 14:42:54">2019/12/20</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/16/django-user-login/"
           data-tag="Django用户登录"
           data-author="郁明敏" >
            <span class="post-title" title="Django用户登录与注册系统">Django用户登录与注册系统</span>
            <span class="post-date" title="2019-12-16 21:22:02">2019/12/16</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/12/pandas-common-usages/"
           data-tag="pandas"
           data-author="郁明敏" >
            <span class="post-title" title="Pandas用法总结">Pandas用法总结</span>
            <span class="post-date" title="2019-12-12 19:30:49">2019/12/12</span>
        </a>
        
        <a  class="系统 Windows "
           href="/2019/12/12/windows-softwares-download/"
           data-tag="windows软件"
           data-author="郁明敏" >
            <span class="post-title" title="Windows软件下载">Windows软件下载</span>
            <span class="post-date" title="2019-12-12 13:53:23">2019/12/12</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/09/crawl-weekday-info/"
           data-tag="Requests"
           data-author="郁明敏" >
            <span class="post-title" title="Requests获取工作日信息">Requests获取工作日信息</span>
            <span class="post-date" title="2019-12-09 16:03:30">2019/12/09</span>
        </a>
        
        <a  class="工具 Notepad "
           href="/2019/12/08/notepad-common-usages/"
           data-tag="notepad"
           data-author="郁明敏" >
            <span class="post-title" title="Notepad++用法总结">Notepad++用法总结</span>
            <span class="post-date" title="2019-12-08 20:51:47">2019/12/08</span>
        </a>
        
        <a  class="工具 JetBrains "
           href="/2019/12/08/pycharm-common-usages/"
           data-tag="pycharm"
           data-author="郁明敏" >
            <span class="post-title" title="PyCharm用法总结">PyCharm用法总结</span>
            <span class="post-date" title="2019-12-08 20:46:04">2019/12/08</span>
        </a>
        
        <a  class="工具 Sublime "
           href="/2019/12/08/sublime-common-usages/"
           data-tag="sublime"
           data-author="郁明敏" >
            <span class="post-title" title="Sublime Text总结">Sublime Text总结</span>
            <span class="post-date" title="2019-12-08 20:42:31">2019/12/08</span>
        </a>
        
        <a  class="系统 Windows "
           href="/2019/12/08/windows-common-usages/"
           data-tag="windows"
           data-author="郁明敏" >
            <span class="post-title" title="Windows总结">Windows总结</span>
            <span class="post-date" title="2019-12-08 20:31:41">2019/12/08</span>
        </a>
        
        <a  class="工具 Chrome "
           href="/2019/12/08/chrome-common-plugins/"
           data-tag="chrome"
           data-author="郁明敏" >
            <span class="post-title" title="Chrome常用插件">Chrome常用插件</span>
            <span class="post-date" title="2019-12-08 20:29:18">2019/12/08</span>
        </a>
        
        <a  class="系统 Windows "
           href="/2019/12/08/windows-common-softwares/"
           data-tag="windows软件"
           data-author="郁明敏" >
            <span class="post-title" title="Window常用软件">Window常用软件</span>
            <span class="post-date" title="2019-12-08 20:22:58">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/python-common-packages/"
           data-tag="Python常用库"
           data-author="郁明敏" >
            <span class="post-title" title="Python常用库">Python常用库</span>
            <span class="post-date" title="2019-12-08 20:04:22">2019/12/08</span>
        </a>
        
        <a  class="系统 Mac "
           href="/2019/12/08/mac-common-usages/"
           data-tag="系统,Mac"
           data-author="郁明敏" >
            <span class="post-title" title="MAC用法总结">MAC用法总结</span>
            <span class="post-date" title="2019-12-08 19:49:04">2019/12/08</span>
        </a>
        
        <a  class="数据库 MySQL "
           href="/2019/12/08/centos-migrate-mysql/"
           data-tag="Centos,MySQL"
           data-author="郁明敏" >
            <span class="post-title" title="Centos下迁移MySQL">Centos下迁移MySQL</span>
            <span class="post-date" title="2019-12-08 19:37:05">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/python-install-packages/"
           data-tag="impala,impyla"
           data-author="郁明敏" >
            <span class="post-title" title="Python安装各种库的教程">Python安装各种库的教程</span>
            <span class="post-date" title="2019-12-08 19:29:16">2019/12/08</span>
        </a>
        
        <a  class="数据库 MySQL "
           href="/2019/12/08/centos-install-mysql/"
           data-tag="Centos,MySQL"
           data-author="郁明敏" >
            <span class="post-title" title="Centos安装MySQL服务">Centos安装MySQL服务</span>
            <span class="post-date" title="2019-12-08 19:01:46">2019/12/08</span>
        </a>
        
        <a  class="系统 Linux "
           href="/2019/12/08/linux-common-usages/"
           data-tag="Linux,LGBM,GLIBC"
           data-author="郁明敏" >
            <span class="post-title" title="Linux总结">Linux总结</span>
            <span class="post-date" title="2019-12-08 18:02:08">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/jupyter-usages/"
           data-tag="Jupyter"
           data-author="郁明敏" >
            <span class="post-title" title="jupyter用法总结">jupyter用法总结</span>
            <span class="post-date" title="2019-12-08 17:39:50">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/airflow-references/"
           data-tag="AirFlow"
           data-author="郁明敏" >
            <span class="post-title" title="AirFlow参考文档">AirFlow参考文档</span>
            <span class="post-date" title="2019-12-08 15:59:10">2019/12/08</span>
        </a>
        
        <a  class="数据库 蓝鲸 "
           href="/2019/12/08/blue-whale-common_usages/"
           data-tag="蓝鲸"
           data-author="郁明敏" >
            <span class="post-title" title="蓝鲸使用">蓝鲸使用</span>
            <span class="post-date" title="2019-12-08 15:40:18">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/python-connect-impala-timeout/"
           data-tag="impala,impyla"
           data-author="郁明敏" >
            <span class="post-title" title="Python连接Impala超时">Python连接Impala超时</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/create-impala-table-using-python/"
           data-tag="impala"
           data-author="郁明敏" >
            <span class="post-title" title="使用Python在Impala中建表">使用Python在Impala中建表</span>
            <span class="post-date" title="2019-12-08 15:13:02">2019/12/08</span>
        </a>
        
        <a  class="工具 JetBrains "
           href="/2019/12/08/datagrip-add-customized-connections/"
           data-tag="datagrip"
           data-author="郁明敏" >
            <span class="post-title" title="DataGrip自定义连接Hive和Impala">DataGrip自定义连接Hive和Impala</span>
            <span class="post-date" title="2019-12-08 15:08:00">2019/12/08</span>
        </a>
        
        <a  class="数据库 Impala "
           href="/2019/12/08/impala-common-usages/"
           data-tag="impala"
           data-author="郁明敏" >
            <span class="post-title" title="Impala常规使用">Impala常规使用</span>
            <span class="post-date" title="2019-12-08 14:56:27">2019/12/08</span>
        </a>
        
        <a  class="数据库 Hive "
           href="/2019/12/08/hive-qas/"
           data-tag="hive"
           data-author="郁明敏" >
            <span class="post-title" title="Hive总结">Hive总结</span>
            <span class="post-date" title="2019-12-08 14:48:43">2019/12/08</span>
        </a>
        
        <a  class="后端 Hadoop "
           href="/2019/12/08/hadoop-qas/"
           data-tag="hadoop"
           data-author="郁明敏" >
            <span class="post-title" title="Hadoop总结">Hadoop总结</span>
            <span class="post-date" title="2019-12-08 14:27:24">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/airflow-common-usages/"
           data-tag="AirFlow"
           data-author="郁明敏" >
            <span class="post-title" title="AirFlow用法总结">AirFlow用法总结</span>
            <span class="post-date" title="2019-12-08 14:24:18">2019/12/08</span>
        </a>
        
        <a  class="工具 Markdown "
           href="/2019/12/08/markdown-common-usages/"
           data-tag="markdown"
           data-author="郁明敏" >
            <span class="post-title" title="Markdown用法总结">Markdown用法总结</span>
            <span class="post-date" title="2019-12-08 13:40:22">2019/12/08</span>
        </a>
        
        <a  class="数据挖掘 强化学习 "
           href="/2019/12/08/morvan-courses-rl/"
           data-tag="强化学习"
           data-author="郁明敏" >
            <span class="post-title" title="莫烦-强化学习课程">莫烦-强化学习课程</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a  class="工具 GitHub "
           href="/2019/12/08/github-search-project-skills/"
           data-tag="GitHub"
           data-author="郁明敏" >
            <span class="post-title" title="GitHub搜索开源项目技巧">GitHub搜索开源项目技巧</span>
            <span class="post-date" title="2019-12-08 11:10:15">2019/12/08</span>
        </a>
        
        <a  class="工具 JetBrains "
           href="/2019/12/08/pycharm-latest-activate-method/"
           data-tag="pycharm"
           data-author="郁明敏" >
            <span class="post-title" title="PyCharm2019.2最新激活方式">PyCharm2019.2最新激活方式</span>
            <span class="post-date" title="2019-12-08 10:27:59">2019/12/08</span>
        </a>
        
        <a  class="数据挖掘 数理统计 "
           href="/2019/12/08/statistics-hypothesis-testing/"
           data-tag="统计基础"
           data-author="郁明敏" >
            <span class="post-title" title="统计假设检验">统计假设检验</span>
            <span class="post-date" title="2019-12-08 10:26:23">2019/12/08</span>
        </a>
        
        <a  class="工具 XShell "
           href="/2019/12/07/xshell-connect-docker-server/"
           data-tag="xshell"
           data-author="郁明敏" >
            <span class="post-title" title="XShell连接Docker服务器下Centos终端">XShell连接Docker服务器下Centos终端</span>
            <span class="post-date" title="2019-12-07 23:40:55">2019/12/07</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/07/install-airflow/"
           data-tag="AirFlow"
           data-author="郁明敏" >
            <span class="post-title" title="AirFlow安装">AirFlow安装</span>
            <span class="post-date" title="2019-12-07 13:52:21">2019/12/07</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/07/flask-QA/"
           data-tag="Flask"
           data-author="郁明敏" >
            <span class="post-title" title="Flask总结">Flask总结</span>
            <span class="post-date" title="2019-12-07 11:58:54">2019/12/07</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/07/unfamiliar-key-words-in-python/"
           data-tag="Python关键字"
           data-author="郁明敏" >
            <span class="post-title" title="冷僻的Python内置关键字">冷僻的Python内置关键字</span>
            <span class="post-date" title="2019-12-07 02:15:12">2019/12/07</span>
        </a>
        
        <a  class="学习资源 "
           href="/2019/12/06/learning-resources-python/"
           data-tag="Python学习资源"
           data-author="郁明敏" >
            <span class="post-title" title="Python学习资源">Python学习资源</span>
            <span class="post-date" title="2019-12-06 22:52:18">2019/12/06</span>
        </a>
        
        <a  class="文献阅读 强化学习 "
           href="/2019/12/06/reinforcement-learning/"
           data-tag="强化学习"
           data-author="郁明敏" >
            <span class="post-title" title="文献-强化学习">文献-强化学习</span>
            <span class="post-date" title="2019-12-06 17:23:52">2019/12/06</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/04/python-common-tips/"
           data-tag="Python小技巧"
           data-author="郁明敏" >
            <span class="post-title" title="Python常用小技巧">Python常用小技巧</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a  class="数据挖掘 "
           href="/2019/12/03/data-mining-common-steps-routines/"
           data-tag="数据挖掘"
           data-author="郁明敏" >
            <span class="post-title" title="数据挖掘下的一般步骤与套路">数据挖掘下的一般步骤与套路</span>
            <span class="post-date" title="2019-12-03 14:59:37">2019/12/03</span>
        </a>
        
        <a  class="文献阅读 随机过程 "
           href="/2019/12/01/stochastic-process/"
           data-tag="随机过程"
           data-author="郁明敏" >
            <span class="post-title" title="文献-随机过程">文献-随机过程</span>
            <span class="post-date" title="2019-12-01 23:45:46">2019/12/01</span>
        </a>
        
        <a  class="数据挖掘 LR "
           href="/2019/12/01/build-LR-model-steps/"
           data-tag="Logistic Regression"
           data-author="郁明敏" >
            <span class="post-title" title="LR建模总结">LR建模总结</span>
            <span class="post-date" title="2019-12-01 22:59:25">2019/12/01</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first">
                <i class="fa fa-arrow-circle-left" aria-hidden="true"></i>
            </div>
            <div class="brackets">
                <i class="fa fa-arrow-circle-right" aria-hidden="true"></i>
            </div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-morvan-courses-rl" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">莫烦-强化学习课程</h1>
    
    <div class="article-meta">
        
        
        <span class="author"><a>郁明敏</a></span>
        
        
        <span class="book">
            
                <a  data-rel="数据挖掘">数据挖掘</a>/
            
                <a  data-rel="强化学习">强化学习</a>
            
        </span>
        
        
        <span class="tag">
            
            <a class="color5">强化学习</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title='更新时间: 2019-12-11 22:45:05'>2019-12-08 12:49</time>
        
    </div>
    <div class="article-meta">
        
        <span>字数:21.1k</span>
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-什么是强化学习-Reinforcement-Learning"><span class="toc-text">1. 什么是强化学习(Reinforcement Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-从无到有"><span class="toc-text">1.1 从无到有</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-虚拟老师"><span class="toc-text">1.2 虚拟老师</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-对比监督学习"><span class="toc-text">1.3 对比监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-RL算法"><span class="toc-text">1.4 RL算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-强化学习汇总"><span class="toc-text">2. 强化学习汇总</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Model-free和Model-based"><span class="toc-text">2.1 Model-free和Model-based</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-基于概率和基于价值"><span class="toc-text">2.2 基于概率和基于价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-回合更新和单步更新"><span class="toc-text">2.3 回合更新和单步更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-在线学习和离线学习"><span class="toc-text">2.4 在线学习和离线学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-为什么用强化学习"><span class="toc-text">3. 为什么用强化学习?</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-强化学习介绍"><span class="toc-text">3.1 强化学习介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-模拟程序提前看"><span class="toc-text">3.2 模拟程序提前看</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-教程必备模块"><span class="toc-text">4.1 教程必备模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-快速了解强化学习"><span class="toc-text">4.2 快速了解强化学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-什么是Q-Learning"><span class="toc-text">5. 什么是Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-行为准则"><span class="toc-text">5.1 行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-Q-Learning决策"><span class="toc-text">5.2 Q-Learning决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-Q-Learning更新"><span class="toc-text">5.3 Q-Learning更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-Q-Learning整体算法"><span class="toc-text">5.4 Q-Learning整体算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-Q-Learning中的Gamma"><span class="toc-text">5.5 Q-Learning中的Gamma</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-强化学习小例子"><span class="toc-text">6. 强化学习小例子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-要点"><span class="toc-text">6.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-预设值"><span class="toc-text">6.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-Q表"><span class="toc-text">6.3 Q表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-定义动作"><span class="toc-text">6.4 定义动作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-环境反馈-S-，R"><span class="toc-text">6.5 环境反馈 S_，R</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-环境更新"><span class="toc-text">6.6 环境更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-7-强化学习主循环"><span class="toc-text">6.7 强化学习主循环</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-8-Q-Table的演变"><span class="toc-text">6.8 Q-Table的演变</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Q-Learning算法更新"><span class="toc-text">7. Q-Learning算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-要点"><span class="toc-text">7.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-算法"><span class="toc-text">7.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-算法的代码形式"><span class="toc-text">7.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-Q-Learning思维决策"><span class="toc-text">8. Q-Learning思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-代码主结构"><span class="toc-text">8.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-预设值"><span class="toc-text">8.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-决定行为"><span class="toc-text">8.3 决定行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-学习"><span class="toc-text">8.4 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-检测-state-是否存在"><span class="toc-text">8.5 检测 state 是否存在</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-什么是Sarsa"><span class="toc-text">9. 什么是Sarsa</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-Sarsa决策"><span class="toc-text">9.1 Sarsa决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-Sarsa更新行为准则"><span class="toc-text">9.2 Sarsa更新行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-对比Sarsa和Q-Learning算法"><span class="toc-text">9.3 对比Sarsa和Q-Learning算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-Sarsa算法更新"><span class="toc-text">10. Sarsa算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-要点"><span class="toc-text">10.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-算法"><span class="toc-text">10.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-算法的代码形式"><span class="toc-text">10.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-Sarsa思维决策"><span class="toc-text">11. Sarsa思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-1-代码主结构"><span class="toc-text">11.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-2-学习"><span class="toc-text">11.2 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-什么是Sarsa-lambda"><span class="toc-text">12. 什么是Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-Sarsa-n"><span class="toc-text">12.1 Sarsa(n)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-2-单步更新和回合更新"><span class="toc-text">12.2 单步更新和回合更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-有时迷茫"><span class="toc-text">12.3 有时迷茫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-4-Lambda含义"><span class="toc-text">12.4 Lambda含义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-5-Lambda取值"><span class="toc-text">12.5 Lambda取值</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-Sarsa-lambda"><span class="toc-text">13. Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#13-1-要点"><span class="toc-text">13.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-2-代码主结构"><span class="toc-text">13.2 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-3-预设值"><span class="toc-text">13.3 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-4-检测state是否存在"><span class="toc-text">13.4 检测state是否存在</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-5-学习"><span class="toc-text">13.5 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-4-DQN两大利器"><span class="toc-text">14.4 DQN两大利器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-DQN算法更新—TensorFlow"><span class="toc-text">15. DQN算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#15-1-要点"><span class="toc-text">15.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-2-算法"><span class="toc-text">15.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-3-算法的代码形式"><span class="toc-text">15.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#16-DQN神经网络—TensorFlow"><span class="toc-text">16. DQN神经网络—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#16-1-要点"><span class="toc-text">16.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-2-两个神经网络"><span class="toc-text">16.2 两个神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-3-神经网络结构"><span class="toc-text">16.3 神经网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-4-常见两个网络"><span class="toc-text">16.4 常见两个网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#17-DQN思维决策—TensorFlow"><span class="toc-text">17. DQN思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#17-1-代码主结构"><span class="toc-text">17.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-2-初始值"><span class="toc-text">17.2 初始值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-3-存储记忆"><span class="toc-text">17.3 存储记忆</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-4-选行为"><span class="toc-text">17.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-5-学习"><span class="toc-text">17.5 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-6-学习效果"><span class="toc-text">17.6 学习效果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-7-修改版的-DQN"><span class="toc-text">17.7 修改版的 DQN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#18-OpenAI-gym环境库"><span class="toc-text">18. OpenAI gym环境库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#18-1-要点"><span class="toc-text">18.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-2-安装gym"><span class="toc-text">18.2 安装gym</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-3-CartPole例子"><span class="toc-text">18.3 CartPole例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-4-MountainCar例子"><span class="toc-text">18.4 MountainCar例子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#19-Double-DQN—TensorFlow"><span class="toc-text">19. Double DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#19-1-要点"><span class="toc-text">19.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-2-Double-DQN算法"><span class="toc-text">19.2 Double DQN算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-3-更新方法"><span class="toc-text">19.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-4-记录-Q-值"><span class="toc-text">19.4 记录 Q 值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-5-对比结果"><span class="toc-text">19.5 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#20-Prioritized-Experience-Replay-DQN-—TensorFlow"><span class="toc-text">20. Prioritized Experience Replay(DQN)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#20-1-要点"><span class="toc-text">20.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-2-Prioritized-Replay算法"><span class="toc-text">20.2 Prioritized Replay算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-3-SumTree有效抽样"><span class="toc-text">20.3 SumTree有效抽样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-4-Memory类"><span class="toc-text">20.4 Memory类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-5-更新方法"><span class="toc-text">20.5 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-6-对比结果"><span class="toc-text">20.6 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#22-Dueling-DQN—TensorFlow"><span class="toc-text">22. Dueling DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#22-1-要点"><span class="toc-text">22.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-2-Dueling算法"><span class="toc-text">22.2 Dueling算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-3-更新方法"><span class="toc-text">22.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-4-对比结果"><span class="toc-text">22.4 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#23-什么是Policy-Gradients"><span class="toc-text">23. 什么是Policy Gradients</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#23-1-和以往的强化学习方法不同"><span class="toc-text">23.1 和以往的强化学习方法不同</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-2-更新不同之处"><span class="toc-text">23.2 更新不同之处</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-3-具体更新步骤"><span class="toc-text">23.3 具体更新步骤</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#24-Policy-Gradients算法更新—TensorFlow"><span class="toc-text">24. Policy Gradients算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#24-1-要点"><span class="toc-text">24.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-2-算法"><span class="toc-text">24.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-3-算法代码形式"><span class="toc-text">24.3 算法代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#25-Policy-Gradients思维决策—TensorFlow"><span class="toc-text">25. Policy Gradients思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#25-1-主要代码结构"><span class="toc-text">25.1 主要代码结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-2-初始化"><span class="toc-text">25.2 初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-3-建立Policy神经网络"><span class="toc-text">25.3 建立Policy神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-4-选行为"><span class="toc-text">25.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-5-存储回合"><span class="toc-text">25.5 存储回合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-6-学习"><span class="toc-text">25.6 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#26-什么是Actor-Critic"><span class="toc-text">26. 什么是Actor Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#26-1-为什么要有Actor和Critic"><span class="toc-text">26.1 为什么要有Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-2-Actor和Critic"><span class="toc-text">26.2 Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-3-增加单步更新属性"><span class="toc-text">26.3 增加单步更新属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-4-改进版Deep-Deterministic-Policy-Gradient-DDPG"><span class="toc-text">26.4 改进版Deep Deterministic Policy Gradient(DDPG)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#27-Actot-Critic—TensorFlow"><span class="toc-text">27. Actot Critic—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#27-1-要点"><span class="toc-text">27.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-2-算法"><span class="toc-text">27.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-3-代码主结构"><span class="toc-text">27.3 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-4-两者学习方式"><span class="toc-text">27.4 两者学习方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-5-每回合算法"><span class="toc-text">27.5 每回合算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#28-什么是Deep-Deterministic-Policy-Gradient-DDPG"><span class="toc-text">28. 什么是Deep Deterministic Policy Gradient(DDPG)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#28-1-拆分细讲"><span class="toc-text">28.1 拆分细讲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-2-Deep和DQN"><span class="toc-text">28.2 Deep和DQN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-3-Deterministic-Policy-Gradient"><span class="toc-text">28.3 Deterministic Policy Gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-4-DDPG神经网络"><span class="toc-text">28.4 DDPG神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#29-Deep-Deterministic-Policy-Gradient-DDPG-—TensorFlow"><span class="toc-text">29. Deep Deterministic Policy Gradient(DDPG)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#29-1-要点"><span class="toc-text">29.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-2-算法"><span class="toc-text">29.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-3-主结构"><span class="toc-text">29.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-4-主结构"><span class="toc-text">29.4 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-5-Actor-Critic"><span class="toc-text">29.5 Actor Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-6-记忆库Mmeory"><span class="toc-text">29.6 记忆库Mmeory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-7-每回合算法"><span class="toc-text">29.7 每回合算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-8-简化版代码"><span class="toc-text">29.8 简化版代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#30-什么是Asynchronous-Advantage-Actor-Critic-A3C"><span class="toc-text">30. 什么是Asynchronous Advantage Actor-Critic (A3C)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#30-1-平行宇宙"><span class="toc-text">30.1 平行宇宙</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-2-平行训练"><span class="toc-text">30.2 平行训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-3-多核训练"><span class="toc-text">30.3 多核训练</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#31-Asynchronous-Advantage-Actor-Critic-A3C-—TensorFlow"><span class="toc-text">31. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#31-1-要点"><span class="toc-text">31.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-2-算法"><span class="toc-text">31.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-3-主结构"><span class="toc-text">31.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-4-Actor-Critic网络"><span class="toc-text">31.4 Actor Critic网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-5-Worker"><span class="toc-text">31.5 Worker</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-6-Worker并行工作"><span class="toc-text">31.6 Worker并行工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-7-机械手臂"><span class="toc-text">31.7 机械手臂</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-8-multiprocessing-A3C"><span class="toc-text">31.8 multiprocessing+A3C</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#32-Distributed-Proximal-Policy-Optimization-DPPO-—Tensorflow"><span class="toc-text">32. Distributed Proximal Policy Optimization (DPPO) —Tensorflow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#32-1-要点"><span class="toc-text">32.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-2-OpenAI-和-DeepMind-的-Demo"><span class="toc-text">32.2 OpenAI 和 DeepMind 的 Demo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-3-算法"><span class="toc-text">32.3 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-4-简单的-PPO-主结构"><span class="toc-text">32.4 简单的 PPO 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-5-Distributed-PPO"><span class="toc-text">32.5 Distributed PPO</span></a></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><div class='inner-toc'><h2>目录</h2><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-什么是强化学习-Reinforcement-Learning"><span class="toc-text">1. 什么是强化学习(Reinforcement Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-从无到有"><span class="toc-text">1.1 从无到有</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-虚拟老师"><span class="toc-text">1.2 虚拟老师</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-对比监督学习"><span class="toc-text">1.3 对比监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-RL算法"><span class="toc-text">1.4 RL算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-强化学习汇总"><span class="toc-text">2. 强化学习汇总</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Model-free和Model-based"><span class="toc-text">2.1 Model-free和Model-based</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-基于概率和基于价值"><span class="toc-text">2.2 基于概率和基于价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-回合更新和单步更新"><span class="toc-text">2.3 回合更新和单步更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-在线学习和离线学习"><span class="toc-text">2.4 在线学习和离线学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-为什么用强化学习"><span class="toc-text">3. 为什么用强化学习?</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-强化学习介绍"><span class="toc-text">3.1 强化学习介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-模拟程序提前看"><span class="toc-text">3.2 模拟程序提前看</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-教程必备模块"><span class="toc-text">4.1 教程必备模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-快速了解强化学习"><span class="toc-text">4.2 快速了解强化学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-什么是Q-Learning"><span class="toc-text">5. 什么是Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-行为准则"><span class="toc-text">5.1 行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-Q-Learning决策"><span class="toc-text">5.2 Q-Learning决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-Q-Learning更新"><span class="toc-text">5.3 Q-Learning更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-Q-Learning整体算法"><span class="toc-text">5.4 Q-Learning整体算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-Q-Learning中的Gamma"><span class="toc-text">5.5 Q-Learning中的Gamma</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-强化学习小例子"><span class="toc-text">6. 强化学习小例子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-要点"><span class="toc-text">6.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-预设值"><span class="toc-text">6.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-Q表"><span class="toc-text">6.3 Q表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-定义动作"><span class="toc-text">6.4 定义动作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-环境反馈-S-，R"><span class="toc-text">6.5 环境反馈 S_，R</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-环境更新"><span class="toc-text">6.6 环境更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-7-强化学习主循环"><span class="toc-text">6.7 强化学习主循环</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-8-Q-Table的演变"><span class="toc-text">6.8 Q-Table的演变</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Q-Learning算法更新"><span class="toc-text">7. Q-Learning算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-要点"><span class="toc-text">7.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-算法"><span class="toc-text">7.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-算法的代码形式"><span class="toc-text">7.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-Q-Learning思维决策"><span class="toc-text">8. Q-Learning思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-代码主结构"><span class="toc-text">8.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-预设值"><span class="toc-text">8.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-决定行为"><span class="toc-text">8.3 决定行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-学习"><span class="toc-text">8.4 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-检测-state-是否存在"><span class="toc-text">8.5 检测 state 是否存在</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-什么是Sarsa"><span class="toc-text">9. 什么是Sarsa</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-Sarsa决策"><span class="toc-text">9.1 Sarsa决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-Sarsa更新行为准则"><span class="toc-text">9.2 Sarsa更新行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-对比Sarsa和Q-Learning算法"><span class="toc-text">9.3 对比Sarsa和Q-Learning算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-Sarsa算法更新"><span class="toc-text">10. Sarsa算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-要点"><span class="toc-text">10.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-算法"><span class="toc-text">10.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-算法的代码形式"><span class="toc-text">10.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-Sarsa思维决策"><span class="toc-text">11. Sarsa思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-1-代码主结构"><span class="toc-text">11.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-2-学习"><span class="toc-text">11.2 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-什么是Sarsa-lambda"><span class="toc-text">12. 什么是Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-Sarsa-n"><span class="toc-text">12.1 Sarsa(n)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-2-单步更新和回合更新"><span class="toc-text">12.2 单步更新和回合更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-有时迷茫"><span class="toc-text">12.3 有时迷茫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-4-Lambda含义"><span class="toc-text">12.4 Lambda含义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-5-Lambda取值"><span class="toc-text">12.5 Lambda取值</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-Sarsa-lambda"><span class="toc-text">13. Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#13-1-要点"><span class="toc-text">13.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-2-代码主结构"><span class="toc-text">13.2 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-3-预设值"><span class="toc-text">13.3 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-4-检测state是否存在"><span class="toc-text">13.4 检测state是否存在</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-5-学习"><span class="toc-text">13.5 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-4-DQN两大利器"><span class="toc-text">14.4 DQN两大利器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-DQN算法更新—TensorFlow"><span class="toc-text">15. DQN算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#15-1-要点"><span class="toc-text">15.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-2-算法"><span class="toc-text">15.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-3-算法的代码形式"><span class="toc-text">15.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#16-DQN神经网络—TensorFlow"><span class="toc-text">16. DQN神经网络—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#16-1-要点"><span class="toc-text">16.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-2-两个神经网络"><span class="toc-text">16.2 两个神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-3-神经网络结构"><span class="toc-text">16.3 神经网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-4-常见两个网络"><span class="toc-text">16.4 常见两个网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#17-DQN思维决策—TensorFlow"><span class="toc-text">17. DQN思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#17-1-代码主结构"><span class="toc-text">17.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-2-初始值"><span class="toc-text">17.2 初始值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-3-存储记忆"><span class="toc-text">17.3 存储记忆</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-4-选行为"><span class="toc-text">17.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-5-学习"><span class="toc-text">17.5 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-6-学习效果"><span class="toc-text">17.6 学习效果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-7-修改版的-DQN"><span class="toc-text">17.7 修改版的 DQN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#18-OpenAI-gym环境库"><span class="toc-text">18. OpenAI gym环境库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#18-1-要点"><span class="toc-text">18.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-2-安装gym"><span class="toc-text">18.2 安装gym</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-3-CartPole例子"><span class="toc-text">18.3 CartPole例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-4-MountainCar例子"><span class="toc-text">18.4 MountainCar例子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#19-Double-DQN—TensorFlow"><span class="toc-text">19. Double DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#19-1-要点"><span class="toc-text">19.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-2-Double-DQN算法"><span class="toc-text">19.2 Double DQN算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-3-更新方法"><span class="toc-text">19.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-4-记录-Q-值"><span class="toc-text">19.4 记录 Q 值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-5-对比结果"><span class="toc-text">19.5 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#20-Prioritized-Experience-Replay-DQN-—TensorFlow"><span class="toc-text">20. Prioritized Experience Replay(DQN)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#20-1-要点"><span class="toc-text">20.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-2-Prioritized-Replay算法"><span class="toc-text">20.2 Prioritized Replay算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-3-SumTree有效抽样"><span class="toc-text">20.3 SumTree有效抽样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-4-Memory类"><span class="toc-text">20.4 Memory类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-5-更新方法"><span class="toc-text">20.5 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-6-对比结果"><span class="toc-text">20.6 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#22-Dueling-DQN—TensorFlow"><span class="toc-text">22. Dueling DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#22-1-要点"><span class="toc-text">22.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-2-Dueling算法"><span class="toc-text">22.2 Dueling算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-3-更新方法"><span class="toc-text">22.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-4-对比结果"><span class="toc-text">22.4 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#23-什么是Policy-Gradients"><span class="toc-text">23. 什么是Policy Gradients</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#23-1-和以往的强化学习方法不同"><span class="toc-text">23.1 和以往的强化学习方法不同</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-2-更新不同之处"><span class="toc-text">23.2 更新不同之处</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-3-具体更新步骤"><span class="toc-text">23.3 具体更新步骤</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#24-Policy-Gradients算法更新—TensorFlow"><span class="toc-text">24. Policy Gradients算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#24-1-要点"><span class="toc-text">24.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-2-算法"><span class="toc-text">24.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-3-算法代码形式"><span class="toc-text">24.3 算法代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#25-Policy-Gradients思维决策—TensorFlow"><span class="toc-text">25. Policy Gradients思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#25-1-主要代码结构"><span class="toc-text">25.1 主要代码结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-2-初始化"><span class="toc-text">25.2 初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-3-建立Policy神经网络"><span class="toc-text">25.3 建立Policy神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-4-选行为"><span class="toc-text">25.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-5-存储回合"><span class="toc-text">25.5 存储回合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-6-学习"><span class="toc-text">25.6 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#26-什么是Actor-Critic"><span class="toc-text">26. 什么是Actor Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#26-1-为什么要有Actor和Critic"><span class="toc-text">26.1 为什么要有Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-2-Actor和Critic"><span class="toc-text">26.2 Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-3-增加单步更新属性"><span class="toc-text">26.3 增加单步更新属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-4-改进版Deep-Deterministic-Policy-Gradient-DDPG"><span class="toc-text">26.4 改进版Deep Deterministic Policy Gradient(DDPG)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#27-Actot-Critic—TensorFlow"><span class="toc-text">27. Actot Critic—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#27-1-要点"><span class="toc-text">27.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-2-算法"><span class="toc-text">27.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-3-代码主结构"><span class="toc-text">27.3 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-4-两者学习方式"><span class="toc-text">27.4 两者学习方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-5-每回合算法"><span class="toc-text">27.5 每回合算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#28-什么是Deep-Deterministic-Policy-Gradient-DDPG"><span class="toc-text">28. 什么是Deep Deterministic Policy Gradient(DDPG)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#28-1-拆分细讲"><span class="toc-text">28.1 拆分细讲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-2-Deep和DQN"><span class="toc-text">28.2 Deep和DQN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-3-Deterministic-Policy-Gradient"><span class="toc-text">28.3 Deterministic Policy Gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-4-DDPG神经网络"><span class="toc-text">28.4 DDPG神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#29-Deep-Deterministic-Policy-Gradient-DDPG-—TensorFlow"><span class="toc-text">29. Deep Deterministic Policy Gradient(DDPG)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#29-1-要点"><span class="toc-text">29.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-2-算法"><span class="toc-text">29.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-3-主结构"><span class="toc-text">29.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-4-主结构"><span class="toc-text">29.4 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-5-Actor-Critic"><span class="toc-text">29.5 Actor Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-6-记忆库Mmeory"><span class="toc-text">29.6 记忆库Mmeory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-7-每回合算法"><span class="toc-text">29.7 每回合算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-8-简化版代码"><span class="toc-text">29.8 简化版代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#30-什么是Asynchronous-Advantage-Actor-Critic-A3C"><span class="toc-text">30. 什么是Asynchronous Advantage Actor-Critic (A3C)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#30-1-平行宇宙"><span class="toc-text">30.1 平行宇宙</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-2-平行训练"><span class="toc-text">30.2 平行训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-3-多核训练"><span class="toc-text">30.3 多核训练</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#31-Asynchronous-Advantage-Actor-Critic-A3C-—TensorFlow"><span class="toc-text">31. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#31-1-要点"><span class="toc-text">31.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-2-算法"><span class="toc-text">31.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-3-主结构"><span class="toc-text">31.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-4-Actor-Critic网络"><span class="toc-text">31.4 Actor Critic网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-5-Worker"><span class="toc-text">31.5 Worker</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-6-Worker并行工作"><span class="toc-text">31.6 Worker并行工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-7-机械手臂"><span class="toc-text">31.7 机械手臂</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-8-multiprocessing-A3C"><span class="toc-text">31.8 multiprocessing+A3C</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#32-Distributed-Proximal-Policy-Optimization-DPPO-—Tensorflow"><span class="toc-text">32. Distributed Proximal Policy Optimization (DPPO) —Tensorflow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#32-1-要点"><span class="toc-text">32.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-2-OpenAI-和-DeepMind-的-Demo"><span class="toc-text">32.2 OpenAI 和 DeepMind 的 Demo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-3-算法"><span class="toc-text">32.3 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-4-简单的-PPO-主结构"><span class="toc-text">32.4 简单的 PPO 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-5-Distributed-PPO"><span class="toc-text">32.5 Distributed PPO</span></a></li></ol></li></ol></div></p>
<p>本博客是学习了莫烦强化学习课程的总结，部分内容转载自莫烦的个人博客，在他的个人主页上 <a href="https://morvanzhou.github.io" target="_blank" rel="noopener">https://morvanzhou.github.io</a> 上有很多关于机器学习的相关课程，且配有视频和文字。莫烦是一位我非常敬佩的博主，能够使用最浅显易懂语言让你了解各种模型的原理，有兴趣的作者可以自己看学习一下其他的课程。</p>
<p>所有代码在莫烦的 GitHub 中可以找到，这里放一下地址: <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents" target="_blank" rel="noopener">https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents</a> </p>
<h1 id="1-什么是强化学习-Reinforcement-Learning"><a href="#1-什么是强化学习-Reinforcement-Learning" class="headerlink" title="1. 什么是强化学习(Reinforcement Learning)"></a>1. 什么是强化学习(Reinforcement Learning)</h1><p>强化学习是机器学习大家族中的一大类，使用强化学习能够让机器学着如何在环境中拿到高分，表现出优秀的成绩。而这些成绩背后却是他所付出的辛苦劳动，不断的试错，不断地尝试，累积经验，学习经验。</p>
<h2 id="1-1-从无到有"><a href="#1-1-从无到有" class="headerlink" title="1.1 从无到有"></a>1.1 从无到有</h2><p><img src="https://img-blog.csdnimg.cn/20191210204551928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>强化学习是一类算法，是让计算机实现从一开始什么都不懂，脑袋里没有一点想法，通过不断地尝试，从错误中学习，最后找到规律，学会了达到目的的方法。这就是一个完整的强化学习过程，实际中的强化学习例子有很多。比如近期最有名的 AlphaGo，机器头一次在围棋场上战胜人类高手，让计算机自己学着玩经典游戏 Atari，这些都是让计算机在不断的尝试中更新自己的行为准则，从而一步步学会如何下好围棋，如何操控游戏得到高分。既然要让计算机自己学，那计算机通过什么来学习呢？</p>
<h2 id="1-2-虚拟老师"><a href="#1-2-虚拟老师" class="headerlink" title="1.2 虚拟老师"></a>1.2 虚拟老师</h2><p><img src="https://img-blog.csdnimg.cn/20191210204737184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>原来计算机也需要一位虚拟的老师，这个老师比较吝啬，他不会告诉你如何移动，如何做决定，他为你做的事只有给你的行为打分，那我们应该以什么形式学习这些现有的资源，或者说怎么样只从分数中学习到我应该怎样做决定呢？很简单，我只需要记住那些高分，低分对应的行为，下次用同样的行为拿高分，并避免低分的行为。</p>
<p>比如老师会根据我的开心程度来打分，我开心时，可以得到高分，我不开心时得到低分。有了这些被打分的经验，我就能判断为了拿到高分，我应该选择一张开心的脸，避免选到伤心的脸，这也是强化学习的核心思想。可以看出在强化学习中，一种行为的分数是十分重要的。所以强化学习具有分数导向性。我们换一个角度来思考，这种分数导向性好比我们在监督学习中的正确标签。</p>
<h2 id="1-3-对比监督学习"><a href="#1-3-对比监督学习" class="headerlink" title="1.3 对比监督学习"></a>1.3 对比监督学习</h2><p><img src="https://img-blog.csdnimg.cn/20191210204924115.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们知道监督学习，是已经有了数据和数据对应的正确标签，比如这样。监督学习就能学习出那些脸对应哪种标签。不过强化学习还要更进一步，一开始它并没有数据和标签。</p>
<p>他要通过一次次在环境中的尝试，获取这些数据和标签，然后再学习通过哪些数据能够对应哪些标签，通过学习到的这些规律，竟可能地选择带来高分的行为 (比如这里的开心脸)。这也就证明了在强化学习中，分数标签就是他的老师，他和监督学习中的老师也差不多。</p>
<h2 id="1-4-RL算法"><a href="#1-4-RL算法" class="headerlink" title="1.4 RL算法"></a>1.4 RL算法</h2><p><img src="https://img-blog.csdnimg.cn/20191210205048606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>强化学习是一个大家族，它包含了很多种算法，我们也会——提到之中一些比较有名的算法，比如有通过行为的价值来选取特定行为的方法，包括使用表格学习的 Q-Learnin、Sarsa，使用神经网络学习的 Deep Q Network，还有直接输出行为的 Policy Gradients，又或者了解所处的环境，想象出一个虚拟的环境并从虚拟的环境中学习等等。</p>
<h1 id="2-强化学习汇总"><a href="#2-强化学习汇总" class="headerlink" title="2. 强化学习汇总"></a>2. 强化学习汇总</h1><p>了解强化学习中常用到的几种方法，以及他们的区别，对我们根据特定问题选择方法时很有帮助。强化学习是一个大家族，发展历史也不短，具有很多种不同方法。比如说比较知名的控制方法 Q-Learning、Policy Gradients，还有基于对环境的理解的 Model-based RL 等等。接下来我们通过分类的方式来了解他们的区别。</p>
<h2 id="2-1-Model-free和Model-based"><a href="#2-1-Model-free和Model-based" class="headerlink" title="2.1 Model-free和Model-based"></a>2.1 Model-free和Model-based</h2><p><img src="https://img-blog.csdnimg.cn/20191210205404948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们可以将所有强化学习的方法分为理不理解所处环境，如果我们不尝试去理解环境，环境给了我们什么就是什么。我们就把这种方法叫做 Model-free，这里的 Model 就是用模型来表示环境，那理解了环境也就是学会了用一个模型来代表环境，所以这种就是 Model-based 方法。我们想象，现在环境就是我们的世界，我们的机器人正在这个世界里玩耍，他不理解这个世界是怎样构成的，也不理解世界对于他的行为会怎么样反馈。举个例子，他决定丢颗原子弹去真实的世界，结果把自己给炸死了，所有结果都是那么现实。不过如果采取的是 Model-based RL，机器人会通过过往的经验，先理解真实世界是怎样的，并建立一个模型来模拟现实世界的反馈，最后他不仅可以在现实世界中玩耍，也能在模拟的世界中玩耍，这样就没必要去炸真实世界，连自己也炸死了，他可以像玩游戏一样炸炸游戏里的世界，也保住了自己的小命。那我们就来说说这两种方式的强化学习各用那些方法吧。</p>
<p>Model-free 的方法有很多, 像 Q-Learning, Sarsa、Policy Gradients 都是从环境中得到反馈然后从中学习。而 Model-based RL 只是多了一道程序，为真实世界建模，也可以说他们都是 Model-free 的强化学习，只是 Model-based 多出了一个虚拟环境，我们不仅可以像 Model-free 那样在现实中玩耍，还能在游戏中玩耍，而玩耍的方式也都是 Model-free 中那些玩耍方式，最终 Model-based 还有一个杀手锏是 Model-free 超级羡慕的，那就是想象力。</p>
<p>Model-free 中，机器人只能按部就班，一步一步等待真实世界的反馈，再根据反馈采取下一步行动。而 Model-based 能通过想象来预判断接下来将要发生的所有情，然后选择这些想象情况中最好的那种，并依据这种情况来采取下一步的策略，这也就是围棋场上 AlphaGo 能够超越人类的原因。接下来，我们再来用另外一种分类方法将强化学习分为基于概率和基于价值。</p>
<h2 id="2-2-基于概率和基于价值"><a href="#2-2-基于概率和基于价值" class="headerlink" title="2.2 基于概率和基于价值"></a>2.2 基于概率和基于价值</h2><p><img src="https://img-blog.csdnimg.cn/2019121020593729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>基于概率是强化学习中最直接的一种，他能通过感官分析所处的环境，直接输出下一步要采取的各种动作的概率，然后根据概率采取行动，所以每种动作都有可能被选中，只是可能性不同。而基于价值的方法输出则是所有动作的价值，我们会根据最高价值来选着动作，相比基于概率的方法，基于价值的决策部分更为铁定，毫不留情，就选价值最高的；而基于概率的，即使某个动作的概率最高，但是还是不一定会选到他。</p>
<p>我们现在说的动作都是一个一个不连续的动作，而对于选取连续的动作，基于价值的方法是无能为力的。我们却能用一个<strong>概率分布</strong>在连续动作中选取特定动作，这也是基于概率的方法的优点之一。那么这两类使用的方法又有哪些呢？</p>
<p>比如在基于概率这边，有 Policy Gradients，在基于价值这边有 Q-Learnin、Sarsa 等。而且我们还能结合这两类方法的优势之处，创造更牛逼的一种方法，叫做 Actor-Critic。<code>actor</code> 会基于概率做出动作，而 <code>critic</code> 会对做出的动作给出动作的价值，这样就在原有的 Policy Gradients 上加速了学习过程。</p>
<h2 id="2-3-回合更新和单步更新"><a href="#2-3-回合更新和单步更新" class="headerlink" title="2.3 回合更新和单步更新"></a>2.3 回合更新和单步更新</h2><p><img src="https://img-blog.csdnimg.cn/20191210210243643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>强化学习还能用另外一种方式分类，<strong>回合更新</strong>和<strong>单步更新</strong>，想象强化学习就是在玩游戏，游戏回合有开始和结束。回合更新指的是游戏开始后，我们要等待游戏结束，然后再总结这一回合中的所有转折点，再更新我们的行为准则。而单步更新则是在游戏进行中每一步都在更新，不用等待游戏的结束， 这样我们就能边玩边学习了。</p>
<p>再来说说方法，Monte-carlo Learning 和基础版的 Policy Gradients 等都是回合更新制，Q-Learning、 Sarsa、升级版的 Policy Gradients 等都是单步更新制。因为单步更新更有效率，所以现在大多方法都是基于单步更新。比如有的强化学习问题并不属于回合问题。</p>
<h2 id="2-4-在线学习和离线学习"><a href="#2-4-在线学习和离线学习" class="headerlink" title="2.4 在线学习和离线学习"></a>2.4 在线学习和离线学习</h2><p><img src="https://img-blog.csdnimg.cn/2019121021045949.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>最后一种分类方式是<strong>在线学习</strong>和<strong>离线学习</strong>，所谓在线学习，就是指我必须本人在场，并且一定是本人边玩边学习，而离线学习是你可以选择自己玩，也可以选择看着别人玩，通过看别人玩来学习别人的行为准则，离线学习 同样是从过往的经验中学习，但是这些过往的经历没必要是自己的经历，任何人的经历都能被学习。或者我也不必要边玩边学习，我可以白天先存储下来玩耍时的记忆，然后晚上通过离线学习来学习白天的记忆。那么每种学习的方法又有哪些呢？</p>
<p>最典型的在线学习就是 Sarsa 了，还有一种优化 Sarsa 的算法，叫做 Sarsa-Lambda，最典型的离线学习就是 Q-Learning，后来人也根据离线学习的属性，开发了更强大的算法，比如让计算机学会玩电动的 Deep-Q-Network。</p>
<p>这就是我们从各种不同的角度来对比了强化学习中的多种算法。</p>
<h1 id="3-为什么用强化学习"><a href="#3-为什么用强化学习" class="headerlink" title="3. 为什么用强化学习?"></a>3. 为什么用强化学习?</h1><h2 id="3-1-强化学习介绍"><a href="#3-1-强化学习介绍" class="headerlink" title="3.1 强化学习介绍"></a>3.1 强化学习介绍</h2><p><strong>强化学习</strong>(Reinforcement Learning) 是一个机器学习大家族中的分支，由于近些年来的技术突破，和<strong>深度学习</strong> (Deep Learning) 的整合使得强化学习有了进一步的运用。比如让计算机学着玩游戏，AlphaGo 挑战世界围棋高手，都是强化学习在行的事。强化学习也是让你的程序从对当前环境完全陌生，成长为一个在环境中游刃有余的高手。</p>
<p>这些教程的教学，不依赖于任何强化学习的 Python 模块。因为强化学习的复杂性、多样，到现在还没有比较好的统一化模块。不过我们还是能用最基础的方法编出优秀的强化学习程序!</p>
<h2 id="3-2-模拟程序提前看"><a href="#3-2-模拟程序提前看" class="headerlink" title="3.2 模拟程序提前看"></a>3.2 模拟程序提前看</h2><p>下面是其中莫烦强化学习教程中一些模拟视频:</p>
<ul>
<li>Maze</li>
</ul>
<iframe width="500" height="550" src="https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20sarsa_lambda.mp4" frameborder="0" allowfullscreen> </iframe>
-   Cartpole

<iframe width="500" height="550" src="https://morvanzhou.github.io/static/results/reinforcement-learning/cartpole%20dqn.mp4" frameborder="0" allowfullscreen> </iframe>
-   Mountain car

<iframe width="500" height="550" src="https://morvanzhou.github.io/static/results/reinforcement-learning/mountaincar%20dqn.mp4" frameborder="0" allowfullscreen> </iframe>
# 4. 课程要求

<h2 id="4-1-教程必备模块"><a href="#4-1-教程必备模块" class="headerlink" title="4.1 教程必备模块"></a>4.1 教程必备模块</h2><p>强化学习有一些现成的模块可以使，但是那些模块并不全面，而且强化学习很依赖与你给予的学习环境。对于不同学习环境的强化学，可能 RL 的代码就不同。所以我们要抱着以不变应万变的心态，用基础的模块，从基础学起。懂了原理，再复杂的环境也不在话下。</p>
<p>所以用到的模块和对应的教程:</p>
<ul>
<li>Numpy, Pandas (必学), 用于学习的数据处理</li>
<li>Matplotlib (可学), 偶尔会用来呈现误差曲线什么的</li>
<li>Tkinter (可学), 你可以自己用它来编写模拟环境</li>
<li>Tensorflow (可学), 后面实现神经网络与强化学习结合的时候用到</li>
<li>OpenAI gym (可学), 提供了很多现成的模拟环境</li>
</ul>
<h2 id="4-2-快速了解强化学习"><a href="#4-2-快速了解强化学习" class="headerlink" title="4.2 快速了解强化学习"></a>4.2 快速了解强化学习</h2><p>莫烦制作了每种强化学习对应的简介视频(有趣的机器学习)，大家可以只花很少的时间来观看了解这些学习方法的不同之处. 有了一定概念和基础，我们在这套教材里实现起来就容易多了。而且不懂的时候也能只花很少的时间回顾就。</p>
<h1 id="5-什么是Q-Learning"><a href="#5-什么是Q-Learning" class="headerlink" title="5. 什么是Q-Learning"></a>5. 什么是Q-Learning</h1><p>今天我们会来说说强化学习中一个很有名的算法— Q-learning。</p>
<h2 id="5-1-行为准则"><a href="#5-1-行为准则" class="headerlink" title="5.1 行为准则"></a>5.1 行为准则</h2><p><img src="https://img-blog.csdnimg.cn/20191209231339444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="写作业OR看电视"></p>
<p>我们做事情都会有一个自己的行为准则，比如小时候爸妈常说“不写完作业就不准看电视”。所以我们在 写作业的这种状态下，好的行为就是继续写作业，直到写完它，我们还可以得到奖励。不好的行为就是没写完就跑去看电视了，被爸妈发现，后果很严重。小时候这种事情做多了，也就变成我们不可磨灭的记忆。这和我们要提到的 Q-Learning 有什么关系呢？原来 Q-Learning 也是一个决策过程，和小时候的这种情况差不多。我们举例说明。</p>
<p>假设现在我们处于写作业的状态而且我们以前并没有尝试过写作业时看电视，所以现在我们有两种选择: 1) 继续写作业；2) 跑去看电视。因为以前没有被罚过，所以我选看电视，然后现在的状态变成了看电视, 我又选了继续看电视，接着我还是看电视，最后爸妈回家，发现我没写完作业就去看电视了，狠狠地惩罚了我一次。我也深刻地记下了这一次经历，并在我的脑海中将 “没写完作业就看电视” 这种行为更改为负面行为，我们在看看 Q-Learning 根据很多这样的经历是如何来决策的吧。</p>
<h2 id="5-2-Q-Learning决策"><a href="#5-2-Q-Learning决策" class="headerlink" title="5.2 Q-Learning决策"></a>5.2 Q-Learning决策</h2><p><img src="https://img-blog.csdnimg.cn/20191209235013646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>假设我们的行为准则已经学习好了，现在我们处于状态 <code>s1</code> ，我在写作业，我有两个行为 <code>a1</code> 和 <code>a2</code>，分别是看电视和写作业。根据我的经验，在这种 <code>s1</code> 状态下，<code>a2</code> 写作业带来的潜在奖励要比 <code>a1</code> 看电视高，这里的潜在奖励我们可以用一个有关于 <code>s</code> 和 <code>a</code> 的 <code>Q 表格</code>代替。在我的记忆 <code>Q表格</code> 中，<code>Q(s1, a1)=-2</code> 要小于 <code>Q(s1, a2)=1</code>，所以我们判断要选择 <code>a2</code> 作为下一个行为。现在我们的状态更新成 <code>s2</code> ，我们还是有两个同样的选择，重复上面的过程，在行为准则 <code>Q 表</code>中寻找 <code>Q(s2, a1)</code>和 <code>Q(s2, a2</code>) 的值，并比较他们的大小，选取较大的一个。接着根据 <code>a2</code> 我们到达 <code>s3</code> 并在此重复上面的决策过程。Q-Learning 的方法也就是这样决策的。看完决策，我看在来研究一下这张行为准则 Q 表是通过什么样的方式更改和提升的。</p>
<h2 id="5-3-Q-Learning更新"><a href="#5-3-Q-Learning更新" class="headerlink" title="5.3 Q-Learning更新"></a>5.3 Q-Learning更新</h2><p><img src="https://img-blog.csdnimg.cn/20191209233020278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>所以我们回到之前的流程，根据 Q 表的估计，因为在 <code>s1</code> 中，<code>a2</code> 的值比较大，通过之前的决策方法，我们在 <code>s1</code> 采取了 <code>a2</code>，并到达 <code>s2</code>，这时我们开始更新用于决策的 Q 表。接着我们并没有在实际中采取任何行为，而是再想象自己在 <code>s2</code> 上采取了每种行为，分别看看两种行为哪一个的 Q 值大，比如说 <code>Q(s2, a2)</code> 的值比 <code>Q(s2, a1)</code> 的大，所以我们把大的 <code>Q(s2, a2)</code> 乘上一个衰减值 <code>gamma</code> (比如是 0.9) 并加上到达 <code>s2</code> 时所获取的奖励 <code>R</code> (这里还没有获取到我们的棒棒糖，所以奖励为 0)，因为会获取实实在在的奖励 <code>R</code>， 我们将这个作为我现实中 <code>Q(s1, a2)</code> 的值，但是我们之前是根据 Q 表估计 <code>Q(s1, a2)</code> 的值。所以有了<strong>现实</strong>和<strong>估计值</strong>, 我们就能更新 <code>Q(s1, a2)</code> ，根据<strong>估计与现实的差距</strong>，将这个差距乘以一个学习效率 <code>alpha</code> 累加上老的 <code>Q(s1, a2)</code> 的值变成新的值。但时刻记住, 我们虽然用 <code>maxQ(s2)</code> 估算了一下 <code>s2</code> 状态，但还没有在 <code>s2</code> 做出任何的行为，<code>s2</code> 的行为决策要等到更新完了以后再重新另外做。这就是 <code>off-policy</code> 的 Q-Learning 是如何决策和学习优化决策的过程。</p>
<h2 id="5-4-Q-Learning整体算法"><a href="#5-4-Q-Learning整体算法" class="headerlink" title="5.4 Q-Learning整体算法"></a>5.4 Q-Learning整体算法</h2><p><img src="https://img-blog.csdnimg.cn/20191209233154685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这一张图概括了我们之前所有的内容。这也是 Q-Learning 的算法，每次更新我们都用到了 <code>Q 现实</code>和 <code>Q 估计</code>，而且 Q-Learning 的迷人之处就是 在 <code>Q(s1, a2) 现实</code>中, 也包含了一个 <code>Q(s2)</code> 的最大估计值，将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实，很奇妙吧。最后我们来说说这套算法中一些参数的意义。<code>Epsilon greedy</code> 是用在决策上的一种策略，比如 <code>epsilon=0.9</code> 时, 就说明有 <strong>90%</strong> 的情况我会按照 Q 表的最优值选择行为，<strong>10%</strong> 的时间使用随机选行为。<code>alpha</code> 是学习率，来决定这次的误差有多少是要被学习的，<code>alpha</code> 是一个小于1 的数。<code>gamma</code> 是对未来 reward 的衰减值。我们可以这样想象。</p>
<h2 id="5-5-Q-Learning中的Gamma"><a href="#5-5-Q-Learning中的Gamma" class="headerlink" title="5.5 Q-Learning中的Gamma"></a>5.5 Q-Learning中的Gamma</h2><p><img src="https://img-blog.csdnimg.cn/20191209233227569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们重写一下 <code>Q(s1)</code> 的公式，将 <code>Q(s2)</code> 拆开，因为 <code>Q(s2)</code> 可以像 <code>Q(s1)</code> 一样，是关于 <code>Q(s3)</code> 的，所以可以写成这样，然后以此类推，不停地这样写下去，最后就能写成这样。可以看出 <code>Q(s1)</code> 是有关于之后所有的奖励，但这些奖励正在衰减，离 <code>s1</code> 越远的状态衰减越严重。不好理解？行，我们想象 Q-Learning 的机器人天生近视眼，<code>gamma=1</code> 时，机器人有了一副合适的眼镜，在 <code>s1</code> 看到的 Q 是未来没有任何衰变的奖励，也就是机器人能清清楚楚地看到之后所有步的全部价值，但是当 <code>gamma=0</code>，近视机器人没了眼镜，只能摸到眼前的 reward, 同样也就只在乎最近的大奖励，如果 <code>gamma</code> 从 0 变到 1，眼镜的度数由浅变深，对远处的价值看得越清楚，所以机器人渐渐变得有远见，不仅仅只看眼前的利益，也为自己的未来着想。</p>
<h1 id="6-强化学习小例子"><a href="#6-强化学习小例子" class="headerlink" title="6. 强化学习小例子"></a>6. 强化学习小例子</h1><h2 id="6-1-要点"><a href="#6-1-要点" class="headerlink" title="6.1 要点"></a>6.1 要点</h2><p>这一次我们会用 tabular Q-Learning 的方法实现一个小例子，例子的环境是一个一维世界，在世界的右边有宝藏，探索者只要得到宝藏尝到了甜头，然后以后就记住了得到宝藏的方法，这就是他用强化学习所学习到的行为。</p>
<h2 id="6-2-预设值"><a href="#6-2-预设值" class="headerlink" title="6.2 预设值"></a>6.2 预设值</h2><p>这一次需要的模块和参数设置:</p>
<pre><code class="python">import numpy as np
import pandas as pd
import time

N_STATES = 6  # 1维世界的宽度
ACTIONS = [&#39;left&#39;, &#39;right&#39;]  # 探索者的可用动作
EPSILON = 0.9  # 贪婪度 greedy
ALPHA = 0.1    # 学习率
GAMMA = 0.9    # 奖励递减值
MAX_EPISODES = 13  # 最大回合数
FRESH_TIME = 0.3   # 移动间隔时间
</code></pre>
<h2 id="6-3-Q表"><a href="#6-3-Q表" class="headerlink" title="6.3 Q表"></a>6.3 Q表</h2><p>对于 tabular Q-Learning，我们必须将所有的 <code>Q-values</code> (行为值) 放在 <code>q_table</code> 中，更新 <code>q_table</code> 也是在更新他的行为准则。 <code>q_table</code> 的 <code>index</code> 是所有对应的 <code>state</code> (探索者位置)，<code>columns</code> 是对应的 <code>action</code> (探索者行为)。</p>
<pre><code class="python">def build_q_table(n_states, actions):
    table = pd.DataFrame(
        np.zeros((n_states, len(actions))),     # q_table 全 0 初始
        columns=actions,    # columns 对应的是行为名称
    )
    return table

# q_table:
&quot;&quot;&quot;
   left  right
0   0.0    0.0
1   0.0    0.0
2   0.0    0.0
3   0.0    0.0
4   0.0    0.0
5   0.0    0.0
&quot;&quot;&quot;</code></pre>
<h2 id="6-4-定义动作"><a href="#6-4-定义动作" class="headerlink" title="6.4 定义动作"></a>6.4 定义动作</h2><p>接着定义探索者是如何挑选行为的，这时我们引入 <code>epsilon greedy</code> 的概念。因为在初始阶段，随机的探索环境往往比固定的行为模式要好，所以这也是累积经验的阶段，我们希望探索者不会那么贪婪(greedy)。所以 <code>EPSILON</code> 就是用来控制贪婪程度的值。<code>EPSILON</code> 可以随着探索时间不断提升(越来越贪婪)，不过在这个例子中，我们就固定成 <code>EPSILON = 0.9</code>，90% 的时间是选择最优策略，10% 的时间来探索。</p>
<pre><code class="python"># 在某个 state 地点, 选择行为
def choose_action(state, q_table):
    state_actions = q_table.iloc[state, :]  # 选出这个 state 的所有 action 值
    if (np.random.uniform() &gt; EPSILON) or (state_actions.all() == 0):  # 非贪婪 or 或者这个 state 还没有探索过
        action_name = np.random.choice(ACTIONS)
    else:
        action_name = state_actions.argmax()    # 贪婪模式
    return action_name</code></pre>
<h2 id="6-5-环境反馈-S-，R"><a href="#6-5-环境反馈-S-，R" class="headerlink" title="6.5 环境反馈 S_，R"></a>6.5 环境反馈 S_，R</h2><p>做出行为后，环境也要给我们的行为一个反馈，反馈出下个 <code>state (S_)</code> 和 在上个 <code>state (S)</code> 做出 <code>action (A)</code> 所得到的 <code>reward (R)</code>。 这里定义的规则就是，只有当 <code>o</code> 移动到了 <code>T</code>，探索者才会得到唯一的一个奖励，奖励值 <code>R=1</code>，其他情况都没有奖励。</p>
<pre><code class="python">def get_env_feedback(S, A):
    # This is how agent will interact with the environment
    if A == &#39;right&#39;:    # move right
        if S == N_STATES - 2:   # terminate
            S_ = &#39;terminal&#39;
            R = 1
        else:
            S_ = S + 1
            R = 0
    else:   # move left
        R = 0
        if S == 0:
            S_ = S  # reach the wall
        else:
            S_ = S - 1
    return S_, R
</code></pre>
<h2 id="6-6-环境更新"><a href="#6-6-环境更新" class="headerlink" title="6.6 环境更新"></a>6.6 环境更新</h2><p>接下来就是环境的更新，不用细看。</p>
<pre><code class="python">def update_env(S, episode, step_counter):
    # This is how environment be updated
    env_list = [&#39;-&#39;]*(N_STATES-1) + [&#39;T&#39;]   # &#39;-----T&#39; our environment
    if S == &#39;terminal&#39;:
        interaction = &#39;Episode %s: total_steps = %s&#39; % (episode+1, step_counter)
        print(&#39;\r{}&#39;.format(interaction), end=&#39;&#39;)
        time.sleep(2)
        print(&#39;\r                                &#39;, end=&#39;&#39;)
    else:
        env_list[S] = &#39;o&#39;
        interaction = &#39;&#39;.join(env_list)
        print(&#39;\r{}&#39;.format(interaction), end=&#39;&#39;)
        time.sleep(FRESH_TIME)
</code></pre>
<h2 id="6-7-强化学习主循环"><a href="#6-7-强化学习主循环" class="headerlink" title="6.7 强化学习主循环"></a>6.7 强化学习主循环</h2><p>最重要的地方就在这里，你定义的 RL 方法都在这里体现。在之后的教程中，我们会更加详细得讲解 RL 中的各种方法，下面的内容大家大概看看就行，这节内容不用仔细研究。</p>
<p><img src="https://img-blog.csdnimg.cn/2019121011194063.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="RL主要算法"></p>
<pre><code class="python">def rl():
    q_table = build_q_table(N_STATES, ACTIONS)  # 初始 q table
    for episode in range(MAX_EPISODES):     # 回合
        step_counter = 0
        S = 0   # 回合初始位置
        is_terminated = False   # 是否回合结束
        update_env(S, episode, step_counter)    # 环境更新
        while not is_terminated:

            A = choose_action(S, q_table)   # 选行为
            S_, R = get_env_feedback(S, A)  # 实施行为并得到环境的反馈
            q_predict = q_table.loc[S, A]    # 估算的(状态-行为)值
            if S_ != &#39;terminal&#39;:
                # 实际的(状态-行为)值 (回合没结束)
                q_target = R + GAMMA * q_table.iloc[S_, :].max()   
            else:
                q_target = R  #  实际的(状态-行为)值 (回合结束)
                is_terminated = True  # terminate this episode

            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # q_table 更新
            S = S_  # 探索者移动到下一个 state

            update_env(S, episode, step_counter+1)  # 环境更新

            step_counter += 1
    return q_table</code></pre>
<p>写好所有的评估和更新准则后，我们就能开始训练了，把探索者丢到环境中，让它自己去玩吧。</p>
<pre><code class="python">if __name__ == &quot;__main__&quot;:
    q_table = rl()
    print(&#39;\r\nQ-table:\n&#39;)
    print(q_table)
</code></pre>
<h2 id="6-8-Q-Table的演变"><a href="#6-8-Q-Table的演变" class="headerlink" title="6.8 Q-Table的演变"></a>6.8 Q-Table的演变</h2><p>Q-Learning 学习过程其实就是不断去更新 Q 表的过程，实际训练时你会发现没带迭代到达终点的步数越来越少，其实就是下一次的迭代会基于上一次的 Q 表来做判断，这样模型就更能够判断准确。</p>
<p>这里训练 10 次，看一下到达终点的步数已经 Q 表的更新过程:</p>
<pre><code>Episode 1: total_steps = 38
   left  right
0   0.0    0.0
1   0.0    0.0
2   0.0    0.0
3   0.0    0.0
4   0.0    0.1
5   0.0    0.0
==================================================
Episode 2: total_steps = 22
   left  right
0   0.0  0.000
1   0.0  0.000
2   0.0  0.000
3   0.0  0.009
4   0.0  0.190
5   0.0  0.000
==================================================
Episode 3: total_steps = 9
   left  right
0   0.0  0.00000
1   0.0  0.00000
2   0.0  0.00081
3   0.0  0.02520
4   0.0  0.27100
5   0.0  0.00000

==================================================
Episode 4: total_steps = 5
   left  right
0   0.0  0.000000
1   0.0  0.000073
2   0.0  0.002997
3   0.0  0.047070
4   0.0  0.343900
5   0.0  0.000000

==================================================
Episode 5: total_steps = 7
      left  right
0  0.00000  0.000007
1  0.00000  0.000572
2  0.00003  0.006934
3  0.00000  0.073314
4  0.00000  0.409510
5  0.00000  0.000000

==================================================
Episode 6: total_steps = 5
      left  right
0  0.00000  0.000057
1  0.00000  0.001138
2  0.00003  0.012839
3  0.00000  0.102839
4  0.00000  0.468559
5  0.00000  0.000000

==================================================
Episode 7: total_steps = 5
      left  right
0  0.00000  0.000154
1  0.00000  0.002180
2  0.00003  0.020810
3  0.00000  0.134725
4  0.00000  0.521703
5  0.00000  0.000000

==================================================
Episode 8: total_steps = 5
      left  right
0  0.00000  0.000335
1  0.00000  0.003835
2  0.00003  0.030854
3  0.00000  0.168206
4  0.00000  0.569533
5  0.00000  0.000000

==================================================
Episode 9: total_steps = 5
      left  right
0  0.00000  0.000647
1  0.00000  0.006228
2  0.00003  0.042907
3  0.00000  0.202643
4  0.00000  0.612580
5  0.00000  0.000000

==================================================
Episode 10: total_steps = 5
      left  right
0  0.00000  0.001142
1  0.00000  0.009467
2  0.00003  0.056855
3  0.00000  0.237511
4  0.00000  0.651322
5  0.00000  0.000000

==================================================
Q-table:
      left     right
0  0.00000  0.001142
1  0.00000  0.009467
2  0.00003  0.056855
3  0.00000  0.237511
4  0.00000  0.651322
5  0.00000  0.000000
</code></pre><p>上述结果模型基本训练了5次左右，就已经学习到了相关的经验了，后面基本上每次只需要5步左右就可以到达终点了。</p>
<p>再来看看 Q 表的更新过程:</p>
<ul>
<li>第1步: 只更新了 <code>s4</code> 的 <code>right</code> 的 Q 值，其中未更新的 $Q(s4, \text{right})$ 就是 <code>q_predict</code> 为 0，这里 <code>(1 + 0.9*max(s5,:))</code> 表示 <code>q_target</code>。</li>
</ul>
<p>$$<br>Q(s4, \text{right}) \leftarrow Q(s4, \text{right}) + 0.1 \times [(1 + 0.9 \times max(s5,:)) - Q(s4, \text{right})]<br>\\ =0 + 0.1 \times [1+0.9 \times 0] = 0.1<br>$$</p>
<ul>
<li>第2步: 在第1步基础上更新了 Q表，这一步会用到上述已经更新好的 <code>Q(s4)</code> 的值，同时会更新 <code>Q(s4)</code> 和 <code>Q(s3)</code><ul>
<li>首先更新的是 <code>Q(s3)</code>，这一步是当前状态是 <code>s3</code>，且一步动作是 <code>right</code>。</li>
<li>然后更新的是 <code>Q(s4)</code>，这一步是当前状态是 <code>s4</code>，且一步动作是 <code>right</code>。</li>
</ul>
</li>
</ul>
<p>$$<br>=====更新Q(s_3)=====<br>\\ Q(s3, \text{right}) \leftarrow Q(s3, \text{right}) + 0.1 \times [(0 + 0. \times max(s4,:)) - Q(s3, \text{right})]<br>\\ =0 + 0.1 \times [0+0.9 \times 0.1 - 0] = 0.009<br>\\ =====更新Q(s_4)=====<br>\\ Q(s4, \text{right}) \leftarrow Q(s4, \text{right}) + 0.1 \times [(1 + 0.9 \times max(s5,:)) - Q(s4, \text{right})]<br>\\ =0.1 + 0.1 \times [1+0.9 \times 0 - 0.1] = 0.190<br>$$</p>
<p>这里可能有人会问，这个 <code>Q(s4)=0.190</code> 的值是基于 <code>s4</code> 状态下向=右走更新的值，为什么 <code>s4</code> 的状态不会选择采取 <code>left</code> 的动作跳回 <code>s3</code>么？如果选择往左走，<code>Q(s4)</code> 的值又会怎么变化？</p>
<p>显然在 <code>s4</code> 的状态下，是有可能选择往左走的，但是概率是 10%，90% 的情况会选择往右走，因为此时 <code>Q(s4,right)</code> 已经有相应的价值分了。下面演算一下如果 <code>s4</code> 状态下 10% 概率下选择往走时 <code>Q(s4, left)</code> 的值。<br>$$<br>Q(s4, \text{left}) \leftarrow Q(s4, \text{left}) + 0.1 \times [(0 + 0.9 \times max(s3,:)) - Q(s4, \text{left})]<br>\\ = 0 + 0.1\times[0+0.9 \times 0.1 - 0] = 0.009<br>$$<br>其他的步骤也都是基于这种演算过程，大家可以自己算一下。</p>
<h1 id="7-Q-Learning算法更新"><a href="#7-Q-Learning算法更新" class="headerlink" title="7. Q-Learning算法更新"></a>7. Q-Learning算法更新</h1><h2 id="7-1-要点"><a href="#7-1-要点" class="headerlink" title="7.1 要点"></a>7.1 要点</h2><p>上次我们知道了 RL 之中的 Q-Learning 方法是在做什么事，今天我们就来说说一个更具体的例子。让探索者学会走迷宫，黄色的是天堂 (<code>reward=1</code>)，黑色的地狱(<code>reward=-1</code>)。大多数 RL 是由 reward 导向的，所以定义 reward 是 RL 中比较重要的一点。</p>
<iframe 
    width="580"
    height="600" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20q.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<h2 id="7-2-算法"><a href="#7-2-算法" class="headerlink" title="7.2 算法"></a>7.2 算法</h2><p><img src="https://img-blog.csdnimg.cn/20191210133923830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="主要公式"></p>
<p>整个算法就是一直不断更新 Q table 里的值，然后再根据新的值来判断要在某个 state 采取怎样的 action。Q-Learning 是一个 <code>off-policy</code> 的算法，因为里面的 <code>max</code> action 让 Q-table 的更新可以不基于正在经历的经验(可以是现在学习着很久以前的经验，甚至是学习他人的经验)。不过这一次的例子, 我们没有运用到 <code>off-policy</code>，而是把 Q-Learning 用在了 <code>on-policy</code> 上，也就是现学现卖，将现在经历的直接当场学习并运用。<code>On-policy</code> 和 <code>off-policy</code> 的差别我们会在之后的 <a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-1-DQN1/" target="_blank" rel="noopener">Deep Q network (off-policy)</a> 学习中见识到。而之后的教程也会讲到一个 <code>on-policy</code> (Sarsa) 的形式，我们之后再对比。</p>
<h2 id="7-3-算法的代码形式"><a href="#7-3-算法的代码形式" class="headerlink" title="7.3 算法的代码形式"></a>7.3 算法的代码形式</h2><p>首先我们先 <code>import</code> 两个模块，<code>maze_env</code> 是我们的环境模块，已经编写好了。大家可以直接莫烦的GitHub中<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/2_Q_Learning_maze/maze_env.py" target="_blank" rel="noopener">下载</a>，<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境。 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<ul>
<li>导入自定义模块</li>
</ul>
<pre><code class="python">from maze_env import Maze
from RL_brain import QLearningTable
</code></pre>
<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Q-Learning 最重要的迭代更新部分啦。</p>
<pre><code class="python">def update():
    # 学习 100 回合
    for episode in range(100):
        # 初始化 state 的观测值
        observation = env.reset()

        while True:
            # 更新可视化环境
            env.render()

            # RL 大脑根据 state 的观测值挑选 action
            action = RL.choose_action(str(observation))

            # 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值,
            # reward 和 done (是否是掉下地狱或者升上天堂)
            observation_, reward, done = env.step(action)

            # RL 从这个序列 (state, action, reward, state_) 中学习
            RL.learn(str(observation), action, reward, str(observation_))

            # 将下一个 state 的值传到下一次循环
            observation = observation_

            # 如果掉下地狱或者升上天堂, 这回合就结束了
            if done:
                break

    # 结束游戏并关闭窗口
    print(&#39;game over&#39;)
    env.destroy()

if __name__ == &quot;__main__&quot;:
    # 定义环境 env 和 RL 方式
    env = Maze()
    RL = QLearningTable(actions=list(range(env.n_actions)))

    # 开始可视化环境 env
    env.after(100, update)
    env.mainloop()
</code></pre>
<h1 id="8-Q-Learning思维决策"><a href="#8-Q-Learning思维决策" class="headerlink" title="8. Q-Learning思维决策"></a>8. Q-Learning思维决策</h1><h2 id="8-1-代码主结构"><a href="#8-1-代码主结构" class="headerlink" title="8.1 代码主结构"></a>8.1 代码主结构</h2><p>与上回不一样的地方是，我们将要以一个 <code>class</code> 形式定义 Q-Learning，并把这种 tabular Q-Learning 方法叫做 <code>QLearningTable</code>。</p>
<pre><code class="python">class QLearningTable:
    # 初始化
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):

    # 选行为
    def choose_action(self, observation):

    # 学习更新参数
    def learn(self, s, a, r, s_):

    # 检测 state 是否存在
    def check_state_exist(self, state):
</code></pre>
<h2 id="8-2-预设值"><a href="#8-2-预设值" class="headerlink" title="8.2 预设值"></a>8.2 预设值</h2><p>初始的参数意义不会在这里提及了，请参考这个快速了解通道 <a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-3-tabular-q2/#" target="_blank" rel="noopener">机器学习系列-Q learning</a>。</p>
<pre><code class="python">import numpy as np
import pandas as pd


class QLearningTable:
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        self.actions = actions  # actions list
        self.lr = learning_rate # 学习率
        self.gamma = reward_decay   # 奖励衰减
        self.epsilon = e_greedy     # 贪婪度
        # 初始 q_table
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)   
</code></pre>
<h2 id="8-3-决定行为"><a href="#8-3-决定行为" class="headerlink" title="8.3 决定行为"></a>8.3 决定行为</h2><p>这里是定义如何根据所在的 <code>state</code>，或者是在这个 <code>state</code> 上的 观测值 (observation) 来决策。</p>
<pre><code class="python">def choose_action(self, observation):
    # 检测本 state 是否在 q_table 中存在(见后面标题内容)
    self.check_state_exist(observation) 

    # 选择 action
    if np.random.uniform() &lt; self.epsilon:  # 选择 Q value 最高的 action
        state_action = self.q_table.loc[observation, :]
        # 同一个 state, 可能会有多个相同的 Q action value, 所以我们乱序一下
        action = np.random.choice(state_action[state_action == np.max(state_action)].index)
    else:  # 随机选择 action
        action = np.random.choice(self.actions)

    return action</code></pre>
<h2 id="8-4-学习"><a href="#8-4-学习" class="headerlink" title="8.4 学习"></a>8.4 学习</h2><p>同上一个简单的 Q-Learning 例子一样，我们根据是否是 <code>terminal</code> state (回合终止符) 来判断应该如何更新 <code>q_table</code>。更新的方式是不是很熟悉呢:</p>
<pre><code class="python">update = self.lr * (q_target - q_predict)</code></pre>
<p>这可以理解成神经网络中的更新方式，<code>学习率 * (真实值 - 预测值)</code>。 将判断误差传递回去，有着和神经网络更新的异曲同工之处。</p>
<pre><code class="python">def learn(self, s, a, r, s_):
    self.check_state_exist(s_)  # 检测 q_table 中是否存在 s_ (见后面标题内容)
    q_predict = self.q_table.loc[s, a]
    if s_ != &#39;terminal&#39;:
        # 下个 state 不是 终止符
        q_target = r + self.gamma * self.q_table.loc[s_, :].max()  
    else:
        q_target = r  # 下个 state 是终止符
    # 更新对应的 state-action 值
    self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  </code></pre>
<h2 id="8-5-检测-state-是否存在"><a href="#8-5-检测-state-是否存在" class="headerlink" title="8.5 检测 state 是否存在"></a>8.5 检测 state 是否存在</h2><p>这个功能就是检测 <code>q_table</code> 中有没有当前 <code>state</code> 的步骤了，如果还没有当前 <code>state</code>，那我我们就插入一组全 0 数据，当做这个 <code>state</code> 的所有 <code>action</code> 初始 values。</p>
<pre><code class="python">def check_state_exist(self, state):
    if state not in self.q_table.index:
        # append new state to q table
        self.q_table = self.q_table.append(
            pd.Series(
                [0]*len(self.actions),
                index=self.q_table.columns,
                name=state,
            )
        )</code></pre>
<h1 id="9-什么是Sarsa"><a href="#9-什么是Sarsa" class="headerlink" title="9. 什么是Sarsa"></a>9. 什么是Sarsa</h1><p>今天我们会来说说强化学习中一个和 Q-Learning 类似的算法，叫做 Sarsa。</p>
<p><img src="https://img-blog.csdnimg.cn/20191210212458422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>在强化学习中 Sarsa 和 Q-Learning 及其类似，这节内容会基于之前我们所讲的 Q-Learning。所以还不熟悉 Q-Learning 的朋友们，请前往我制作的 Q-Learning 简介(知乎专栏)。我们会对比 Q-Learning，来看看 Sarsa 是特殊在哪些方面。和上次一样，我们还是使用写作业和看电视这个例子。没写完作业去看电视被打，写完了作业有糖吃。</p>
<h2 id="9-1-Sarsa决策"><a href="#9-1-Sarsa决策" class="headerlink" title="9.1 Sarsa决策"></a>9.1 Sarsa决策</h2><p><img src="https://img-blog.csdnimg.cn/20191210212638770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Sarsa 的决策部分和 Q-Learning 一模一样，因为我们使用的是 Q 表的形式决策，所以我们会在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩。但是不同的地方在于 Sarsa 的更新方式是不一样的。</p>
<h2 id="9-2-Sarsa更新行为准则"><a href="#9-2-Sarsa更新行为准则" class="headerlink" title="9.2 Sarsa更新行为准则"></a>9.2 Sarsa更新行为准则</h2><p><img src="https://img-blog.csdnimg.cn/20191210212726800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>同样，我们会经历正在写作业的状态 <code>s1</code>，然后再挑选一个带来最大潜在奖励的动作 <code>a2</code>，这样我们就到达了继续写作业状态 <code>s2</code>，而在这一步，如果你用的是 Q-Learning，你会观看一下在 <code>s2</code> 上选取哪一个动作会带来最大的奖励，但是在真正要做决定，却不一定会选取到那个带来最大奖励的动作，Q-Learning 在这一步只是估计了一下接下来的动作值。而 Sarsa 是实践派，他说到做到，在 <code>s2</code> 这一步估算的动作也是接下来要做的动作。所以 <code>Q(s1, a2)</code> 现实的计算值，我们也会稍稍改，去掉<code>maxQ</code> ，取而代之的是在 <code>s2</code> 上我们实实在在选取的 a2 的 Q 值。最后像 Q-Learning 一样，求出现实和估计的差距，并更新 Q 表里的 <code>Q(s1, a2)</code>。</p>
<h2 id="9-3-对比Sarsa和Q-Learning算法"><a href="#9-3-对比Sarsa和Q-Learning算法" class="headerlink" title="9.3 对比Sarsa和Q-Learning算法"></a>9.3 对比Sarsa和Q-Learning算法</h2><p><img src="https://img-blog.csdnimg.cn/2019121021312130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>从算法来看，这就是他们两最大的不同之处。因为 Sarsa 是说到做到型，所以我们也叫他 on-policy，在线学，学着自己在做的事情。而 Q-Learning 是说到但并不一定做到，所以它也叫作 Off-policy，离线学习。而因为有了 <code>maxQ</code>, Q-Learning 也是一个特别勇敢的算法。</p>
<p><img src="https://img-blog.csdnimg.cn/20191210213803956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>为什么说他勇敢呢，因为 Q-Learning 机器人永远都会选择最近的一条通往成功的道路，不管这条路会有多危险。而 Sarsa 则是相当保守，他会选择离危险远远的，拿到宝藏是次要的，保住自己的小命才是王道。这就是使用 Sarsa 方法的不同之处。</p>
<h1 id="10-Sarsa算法更新"><a href="#10-Sarsa算法更新" class="headerlink" title="10. Sarsa算法更新"></a>10. Sarsa算法更新</h1><h2 id="10-1-要点"><a href="#10-1-要点" class="headerlink" title="10.1 要点"></a>10.1 要点</h2><p>这次我们用同样的迷宫例子来实现 RL 中另一种和 Q-Learning 类似的算法，叫做 Sarsa (state-action-reward-state-action)。我们从这一个简称可以了解到，Sarsa 的整个循环都将是在一个路径上，也就是 On-Policy，下一个 <code>state_</code> 和下一个 <code>action_</code> 将会变成他真正采取的 <code>action_</code> 和 <code>state_</code>。和 <code>Q-Learning</code> 的不同之处就在这。 Q-Learning 的下个一个 <code>state_</code>、<code>action_</code> 在算法更新的时候都还是不确定的(off-policy)。而 Sarsa 的 <code>state_</code> 和 <code>action_</code> 在这次算法更新的时候已经确定好了 (on-policy)。</p>
<iframe 
    width="580"
    height="600" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20q.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<h2 id="10-2-算法"><a href="#10-2-算法" class="headerlink" title="10.2 算法"></a>10.2 算法</h2><p><img src="https://img-blog.csdnimg.cn/20191210214429797.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>整个算法还是一直不断更新 Q-table 里的值，然后再根据新的值来判断要在某个 <code>state</code> 采取怎样的 <code>action</code>。不过于 Q-Learning 不同之处:</p>
<ul>
<li>他在当前 <code>state</code> 已经想好了 <code>state</code> 对应的 <code>action</code>, 而且想好了 下一个 <code>state_</code> 和下一个 <code>action_</code> (Qlearning 还没有想好下一个 <code>action_</code>)</li>
<li>更新 <code>Q(s,a)</code> 的时候基于的是下一个 <code>Q(s_, a_)</code> (Qlearning 是基于 <code>maxQ(s_)</code>)</li>
</ul>
<p>这种不同之处使得 Sarsa 相对于 Q-Learning，更加的胆小。因为 Qlearning 永远都是想着 <code>maxQ</code> 最大化，因为这个 <code>maxQ</code> 而变得贪婪，不考虑其他非 <code>maxQ</code> 的结果。我们可以理解成 Q-Learning 是一种贪婪、大胆、勇敢的算法，对于错误、死亡并不在乎。而 Sarsa 是一种保守的算法，他在乎每一步决策，对于错误和死亡比较铭感。这一点我们会在可视化的部分看出他们的不同。两种算法都有他们的好处，比如在实际中，你比较在乎机器的损害，用一种保守的算法，在训练时就能减少损坏的次数。</p>
<h2 id="10-3-算法的代码形式"><a href="#10-3-算法的代码形式" class="headerlink" title="10.3 算法的代码形式"></a>10.3 算法的代码形式</h2><p>首先我们先 <code>import</code> 两个模块， <code>maze_env</code> 是我们的环境模块，已经编写好了，大家可以直接在这里下载，<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 tkinter 来编写虚拟环境。莫烦也有对应的教程 <code>maze_env</code> 就是用 tkinter 编写的。 而 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<pre><code class="python">from maze_env import Maze
from RL_brain import SarsaTable
</code></pre>
<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Sarsa 最重要的迭代更新部分啦。</p>
<pre><code class="python">def update():
    for episode in range(100):
        # 初始化环境
        observation = env.reset()

        # Sarsa 根据 state 观测选择行为
        action = RL.choose_action(str(observation))

        while True:
            # 刷新环境
            env.render()

            # 在环境中采取行为, 获得下一个 state_ (obervation_), reward, 和是否终止
            observation_, reward, done = env.step(action)

            # 根据下一个 state (obervation_) 选取下一个 action_
            action_ = RL.choose_action(str(observation_))

            # 从 (s, a, r, s, a) 中学习, 更新 Q_tabel 的参数 ==&gt; Sarsa
            RL.learn(str(observation), action, reward, str(observation_), action_)

            # 将下一个当成下一步的 state (observation) and action
            observation = observation_
            action = action_

            # 终止时跳出循环
            if done:
                break

    # 大循环完毕
    print(&#39;game over&#39;)
    env.destroy()

if __name__ == &quot;__main__&quot;:
    env = Maze()
    RL = SarsaTable(actions=list(range(env.n_actions)))

    env.after(100, update)
    env.mainloop()
</code></pre>
<p>下一节我们会来讲解 <code>SarsaTable</code> 这种算法具体要怎么编。</p>
<h1 id="11-Sarsa思维决策"><a href="#11-Sarsa思维决策" class="headerlink" title="11. Sarsa思维决策"></a>11. Sarsa思维决策</h1><p>接着上节内容，我们来实现 <code>RL_brain</code> 的 <code>SarsaTable</code> 部分，这也是 RL 的大脑部分，负责决策和思考。</p>
<h2 id="11-1-代码主结构"><a href="#11-1-代码主结构" class="headerlink" title="11.1 代码主结构"></a>11.1 代码主结构</h2><p>和之前定义 Q-Learning 中的 <code>QLearningTable</code> 一样，因为使用 tabular 方式的 <code>Sarsa</code> 和 <code>Q-Learning</code> 的相似度极高。</p>
<pre><code class="python">class SarsaTable:
    # 初始化 (与之前一样)
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):

    # 选行为 (与之前一样)
    def choose_action(self, observation):

    # 学习更新参数 (有改变)
    def learn(self, s, a, r, s_):

    # 检测 state 是否存在 (与之前一样)
    def check_state_exist(self, state):
</code></pre>
<p>我们甚至可以定义一个 主 <code>class RL</code> ，然后将 <code>QLearningTable</code> 和 <code>SarsaTable</code> 作为主 <code>class RL</code> 的衍生，这个主 <code>RL</code> 可以这样定义。所以我们将之前的 <code>__init__</code>、 <code>check_state_exist</code>、 <code>choose_action</code>、<code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<pre><code class="python">import numpy as np
import pandas as pd


class RL(object):
    def __init__(self, action_space, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        ... # 和 QLearningTable 中的代码一样

    def check_state_exist(self, state):
        ... # 和 QLearningTable 中的代码一样

    def choose_action(self, observation):
        ... # 和 QLearningTable 中的代码一样

    def learn(self, *args):
        pass # 每种的都有点不同, 所以用 pass
</code></pre>
<p>如果是这样定义父类的 <code>RL</code> class，通过继承关系，那之子类 <code>QLearningTable</code> class 就能简化成这样:</p>
<pre><code class="python">class QLearningTable(RL):   # 继承了父类 RL
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        super(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    # 表示继承关系

    def learn(self, s, a, r, s_):   # learn 的方法在每种类型中有不一样, 需重新定义
        self.check_state_exist(s_)
        q_predict = self.q_table.loc[s, a]
        if s_ != &#39;terminal&#39;:
            q_target = r + self.gamma * self.q_table.loc[s_, :].max()
        else:
            q_target = r
        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)
</code></pre>
<h2 id="11-2-学习"><a href="#11-2-学习" class="headerlink" title="11.2 学习"></a>11.2 学习</h2><p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。这就是我们所有的 <code>SarsaTable</code> 于父类 <code>RL</code> 不同之处的代码。是不是很简单。</p>
<pre><code class="python">class SarsaTable(RL):   # 继承 RL class

    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        super(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    # 表示继承关系

    def learn(self, s, a, r, s_, a_):
        self.check_state_exist(s_)
        q_predict = self.q_table.loc[s, a]
        if s_ != &#39;terminal&#39;:
            # q_target 基于选好的 a_ 而不是 Q(s_) 的最大值
            q_target = r + self.gamma * self.q_table.loc[s_, a_]  
        else:
            q_target = r  # 如果 s_ 是终止符
        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # 更新 q_table
</code></pre>
<h1 id="12-什么是Sarsa-lambda"><a href="#12-什么是Sarsa-lambda" class="headerlink" title="12. 什么是Sarsa-lambda"></a>12. 什么是Sarsa-lambda</h1><p>今天我们会来说说强化学习中基于 Sarsa 的一种提速方法，叫做 Sarsa-Lambda。</p>
<h2 id="12-1-Sarsa-n"><a href="#12-1-Sarsa-n" class="headerlink" title="12.1 Sarsa(n)"></a>12.1 Sarsa(n)</h2><p><img src="https://img-blog.csdnimg.cn/20191210232355243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>通过上面的介绍，我们知道这个 Sarsa 的算法是一种在线学习法，On-Policy。但是这个 Lambda 到底是什么。其实吧，Sarsa 是一种单步更新法，在环境中每走一步，更新一次自己的行为准则，我们可以在这样的 Sarsa 后面打一个括号，说他是 <code>Sarsa(0)</code>，因为他等走完这一步以后直接更新行为准则。如果延续这种想法，走完这步，再走一步，然后再更新，我们可以叫它 <code>Sarsa(1)</code>。同理，如果等待回合完毕我们一次性再更新呢，比如这回合我们走了 n 步，那我们就叫 <code>Sarsa(n)</code>。为了统一这样的流程，我们就有了一个 <code>lambda</code> 值来代替我们想要选择的步数，这也就是 <code>Sarsa(lambda)</code> 的由来。我们看看最极端的两个例子，对比单步更新和回合更新，看看回合更新的优势在哪里。</p>
<h2 id="12-2-单步更新和回合更新"><a href="#12-2-单步更新和回合更新" class="headerlink" title="12.2 单步更新和回合更新"></a>12.2 单步更新和回合更新</h2><p><img src="https://img-blog.csdnimg.cn/20191210232453544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>虽然我们每一步都在更新，但是在没有获取宝藏的时候，我们现在站着的这一步也没有得到任何更新，也就是直到获取宝藏时，我们才为获取到宝藏的上一步更新为: 这一步很好，和获取宝藏是有关联的，而之前为了获取宝藏所走的所有步都被认为和获取宝藏没关系。回合更新虽然我要等到这回合结束，才开始对本回合所经历的所有步都添加更新，但是这所有的步都是和宝藏有关系的，都是为了得到宝藏需要学习的步，所以每一个脚印在下回合被选则的几率又高了一些。在这种角度来看，回合更新似乎会有效率一些。</p>
<h2 id="12-3-有时迷茫"><a href="#12-3-有时迷茫" class="headerlink" title="12.3 有时迷茫"></a>12.3 有时迷茫</h2><p><img src="https://img-blog.csdnimg.cn/20191210232518798.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们看看这种情况，还是使用单步更新的方法在每一步都进行更新，但是同时记下之前的寻宝之路。你可以想像，每走一步，插上一个小旗子，这样我们就能清楚的知道除了最近的一步，找到宝物时还需要更新哪些步了。不过，有时候情况可能没有这么乐观。 开始的几次，因为完全没有头绪，我可能在原地打转了很久，然后才找到宝藏，那些重复的脚步真的对我拿到宝藏很有必要吗？答案我们都知道。所以 <code>Sarsa(lambda)</code> 就来拯救你啦。</p>
<h2 id="12-4-Lambda含义"><a href="#12-4-Lambda含义" class="headerlink" title="12.4 Lambda含义"></a>12.4 Lambda含义</h2><p><img src="https://img-blog.csdnimg.cn/20191210232548913.png" alt="在这里插入图片描述"></p>
<p>其实 lambda 就是一个衰变值，他可以让你知道离奖励越远的步可能并不是让你最快拿到奖励的步, 所以我们想象我们站在宝藏的位置，回头看看我们走过的寻宝之路，离宝藏越近的脚印越看得清，远处的脚印太渺小，我们都很难看清，那我们就索性记下离宝藏越近的脚印越重要，越需要被好好的更新。和之前我们提到过的<strong>奖励衰减值</strong> gamma 一样，lambda 是脚步衰减值，都是一个在 0 和 1 之间的数。</p>
<h2 id="12-5-Lambda取值"><a href="#12-5-Lambda取值" class="headerlink" title="12.5 Lambda取值"></a>12.5 Lambda取值</h2><p><img src="https://img-blog.csdnimg.cn/20191210232646125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>当 <code>lambda</code> 取 0，就变成了 Sarsa 的单步更新。当 <code>lambda=1</code>，就变成了回合更新，对所有步更新的力度都是一样。当 <code>lambda</code> 在 0 和 1 之间，取值越大，离宝藏越近的步更新力度越大。这样我们就不用受限于单步更新的每次只能更新最近的一步，我们可以更有效率的更新所有相关步了。</p>
<h1 id="13-Sarsa-lambda"><a href="#13-Sarsa-lambda" class="headerlink" title="13. Sarsa-lambda"></a>13. Sarsa-lambda</h1><h2 id="13-1-要点"><a href="#13-1-要点" class="headerlink" title="13.1 要点"></a>13.1 要点</h2><p>Sarsa-lambda 是基于 Sarsa 方法的升级版，它能更有效率地学习到怎么样获得好的 reward。如果说 Sarsa 和 Q-Learning 都是每次获取到 reward，只更新获取到 reward 的前一步。 那 Sarsa-lambda 就是更新获取到 reward 的前 lambda 步。<code>lambda</code> 是在 <code>[0, 1]</code> 之间取值:</p>
<ul>
<li><p>如果 <code>lambda=0</code>，Sarsa-lambda 就是 Sarsa，只更新获取到 reward 前经历的最后一步。</p>
</li>
<li><p>如果 <code>lambda=1</code>，Sarsa-lambda 更新的是获取到 reward 前所有经历的步。</p>
</li>
</ul>
<p>这样解释起来有点抽象，还是建议大家观看莫烦制作的什么是 Sarsa-lambda 短视频，用动画展示具体的区别。</p>
<iframe 
    width="580"
    height="600" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20sarsa_lambda.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<h2 id="13-2-代码主结构"><a href="#13-2-代码主结构" class="headerlink" title="13.2 代码主结构"></a>13.2 代码主结构</h2><p>使用 <code>SarsaLambdaTable</code> 在算法更新迭代的部分，是和之前的 <code>SarsaTable</code> 一样的，所以这一节，我们没有算法更新部分，直接变成 思维决策部分。</p>
<pre><code class="python">class SarsaLambdaTable:
    # 初始化 (有改变)
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):

    # 选行为 (与之前一样)
    def choose_action(self, observation):

    # 学习更新参数 (有改变)
    def learn(self, s, a, r, s_):

    # 检测 state 是否存在 (有改变)
    def check_state_exist(self, state):
</code></pre>
<p>同样，我们选择继承的方式，将 <code>SarsaLambdaTable</code> 继承到 <code>RL</code>，所以我们将之前的 <code>__init__</code>、<code>check_state_exist</code>、 <code>choose_action</code>、 <code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<p>算法的相应更改请参考这个:</p>
<p><img src="https://img-blog.csdnimg.cn/20191210234954271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="13-3-预设值"><a href="#13-3-预设值" class="headerlink" title="13.3 预设值"></a>13.3 预设值</h2><p>在预设值当中，我们添加了 <code>trace_decay=0.9</code> 这个就是 <code>lambda</code> 的值了。这个值将会使得拿到 reward 前的每一步都有价值。</p>
<pre><code class="python">class SarsaLambdaTable(RL): # 继承 RL class
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):
        super(SarsaLambdaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)

        # 后向观测算法, eligibility trace.
        self.lambda_ = trace_decay
        # 空的 eligibility trace 表
        self.eligibility_trace = self.q_table.copy()
</code></pre>
<h2 id="13-4-检测state是否存在"><a href="#13-4-检测state是否存在" class="headerlink" title="13.4 检测state是否存在"></a>13.4 检测state是否存在</h2><p><code>check_state_exist</code> 和之前的是高度相似的。唯一不同的地方是我们考虑了 <code>eligibility_trace</code>。</p>
<pre><code class="python">class SarsaLambdaTable(RL): # 继承 RL class
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):
        pass
    def check_state_exist(self, state):
        if state not in self.q_table.index:
            # append new state to q table
            to_be_append = pd.Series(
                    [0] * len(self.actions),
                    index=self.q_table.columns,
                    name=state,
                )
            self.q_table = self.q_table.append(to_be_append)

            # also update eligibility trace
            self.eligibility_trace = self.eligibility_trace.append(to_be_append)
</code></pre>
<h2 id="13-5-学习"><a href="#13-5-学习" class="headerlink" title="13.5 学习"></a>13.5 学习</h2><p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaLambdaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。就是我们所有的 <code>SarsaLambdaTable</code> 于父类 <code>RL</code> 不同之处的代码。是不是很简单。</p>
<pre><code class="python">class SarsaLambdaTable(RL): # 继承 RL class
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):
        ...
    def check_state_exist(self, state):
        ...
    def learn(self, s, a, r, s_, a_):
        # 这部分和 Sarsa 一样
        self.check_state_exist(s_)
        q_predict = self.q_table.ix[s, a]
        if s_ != &#39;terminal&#39;:
            q_target = r + self.gamma * self.q_table.ix[s_, a_]
        else:
            q_target = r
        error = q_target - q_predict

        # 这里开始不同:
        # 对于经历过的 state-action, 我们让他+1, 证明他是得到 reward 路途中不可或缺的一环
        self.eligibility_trace.ix[s, a] += 1

        # Q table 更新
        self.q_table += self.lr * error * self.eligibility_trace

        # 随着时间衰减 eligibility trace 的值, 离获取 reward 越远的步, 他的&quot;不可或缺性&quot;越小
        self.eligibility_trace *= self.gamma*self.lambda_
</code></pre>
<p>除了图中和上面代码这种更新方式，还有一种会更加有效率。我们可以将上面的这一步替换成下面这样:</p>
<pre><code class="python"># 上面代码中的方式:
self.eligibility_trace.ix[s, a] += 1

# 更有效的方式:
self.eligibility_trace.ix[s, :] *= 0
self.eligibility_trace.ix[s, a] = 1</code></pre>
<p>他们两的不同之处可以用这张图来概括:</p>
<p><img src="https://img-blog.csdnimg.cn/20191210235506805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这是针对于一个 state-action 值按经历次数的变化。最上面是经历 state-action 的时间点，第二张图是使用这种方式所带来的 “不可或缺性值”:</p>
<pre><code class="python">self.eligibility_trace.ix[s, a] += 1</code></pre>
<p>下面图是使用这种方法带来的 “不可或缺性值”:</p>
<pre><code class="python">self.eligibility_trace.ix[s, :] *= 0
self.eligibility_trace.ix[s, a] = 1</code></pre>
<p>实验证明选择下面这种方法会有更好的效果。大家也可以自己玩一玩, 试试两种方法的不同表现。</p>
<p>最后不要忘了，eligibility trace 只是记录每个回合的每一步，新回合开始的时候需要将 Trace 清零。</p>
<pre><code class="python">for episode in range(100):
    ...
    # 新回合, 清零
    RL.eligibility_trace *= 0

    while True: # 开始回合
        ...</code></pre>
<h2 id="14-4-DQN两大利器"><a href="#14-4-DQN两大利器" class="headerlink" title="14.4 DQN两大利器"></a>14.4 DQN两大利器</h2><p><img src="https://img-blog.csdnimg.cn/20191211100609210.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>简单来说，DQN 有一个记忆库用于学习之前的经历。在之前的简介影片中提到过，Q-Learning 是一种 <code>off-policy</code> 离线学习法，它能学习当前经历着的，也能学习过去经历过的，甚至是学习别人的经历。所以每次 DQN 更新的时候，我们都可以随机抽取一些之前的经历进行学习。随机抽取这种做法打乱了经历之间的相关，也使得神经网络更新更有效率。<code>Fixed Q-targets</code> 也是一种打乱相关性的机理，如果使用 <code>fixed Q-targets</code>，我们就会在 DQN 中使用到两个<strong>结构相同</strong>但<strong>参数不同</strong>的神经网络，预测 <strong>Q 估计</strong> 的神经网络具备最新的参数，而预测 <strong>Q 现实</strong> 的神经网络使用的参数则是很久以前的。有了这两种提升手段，DQN 才能在一些游戏中超越人类。</p>
<h1 id="15-DQN算法更新—TensorFlow"><a href="#15-DQN算法更新—TensorFlow" class="headerlink" title="15. DQN算法更新—TensorFlow"></a>15. DQN算法更新—TensorFlow</h1><h2 id="15-1-要点"><a href="#15-1-要点" class="headerlink" title="15.1 要点"></a>15.1 要点</h2><p>Deep Q Network 的简称叫 DQN，是将 Q-Learning 的优势和 Neural networks 结合了。如果我们使用 tabular Q-Learning，对于每一个 <code>state</code> 和 <code>action</code> 我们都需要存放在一张 <code>q_table</code> 的表中。如果像现实生活中，情况可就比那个迷宫的状况复杂多了，我们有千千万万个 <code>state</code>，如果将这千万个 <code>state</code> 的值都放在表中，受限于我们计算机硬件，这样从表中获取数据，更新数据是没有效率的，这就是 DQN 产生的原因了。我们可以使用神经网络来估算 这个 <code>state</code> 的值，这样就不需要一张表了。</p>
<p>这次的教程我们还是基于熟悉的迷宫环境，重点在实现 DQN 算法，之后我们再拿着做好的 DQN 算法去跑其他更有意思的环境。</p>
<iframe 
    width="580"
    height="600" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20dqn.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<h2 id="15-2-算法"><a href="#15-2-算法" class="headerlink" title="15.2 算法"></a>15.2 算法</h2><p><img src="https://img-blog.csdnimg.cn/20191211101344134.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>整个算法乍看起来很复杂，不过我们拆分一下，就变简单了。也就是个 Q-Learning 主框架上加了些装饰。</p>
<p>这些装饰包括:</p>
<ul>
<li>记忆库 (用于重复学习)</li>
<li>神经网络计算 Q 值</li>
<li>暂时冻结 <code>q_target</code> 参数 (切断相关性)</li>
</ul>
<h2 id="15-3-算法的代码形式"><a href="#15-3-算法的代码形式" class="headerlink" title="15.3 算法的代码形式"></a>15.3 算法的代码形式</h2><p>接下来我们对应上面的算法，来实现主循环。首先 <code>import</code> 所需模块。</p>
<pre><code class="python">from maze_env import Maze
from RL_brain import DeepQNetwork
</code></pre>
<p>下面的代码，就是 DQN 于环境交互最重要的部分。</p>
<pre><code class="python">def run_maze():
    step = 0    # 用来控制什么时候学习
    for episode in range(300):
        # 初始化环境
        observation = env.reset()

        while True:
            # 刷新环境
            env.render()

            # DQN 根据观测值选择行为
            action = RL.choose_action(observation)

            # 环境根据行为给出下一个 state, reward, 是否终止
            observation_, reward, done = env.step(action)

            # DQN 存储记忆
            RL.store_transition(observation, action, reward, observation_)

            # 控制学习起始时间和频率 (先累积一些记忆再开始学习)
            if (step &gt; 200) and (step % 5 == 0):
                RL.learn()

            # 将下一个 state_ 变为 下次循环的 state
            observation = observation_

            # 如果终止, 就跳出循环
            if done:
                break
            step += 1   # 总步数

    # end of game
    print(&#39;game over&#39;)
    env.destroy()


if __name__ == &quot;__main__&quot;:
    env = Maze()
    RL = DeepQNetwork(env.n_actions, env.n_features,
                      learning_rate=0.01,
                      reward_decay=0.9,
                      e_greedy=0.9,
                      # 每 200 步替换一次 target_net 的参数
                      replace_target_iter=200,
                      memory_size=2000, # 记忆上限
                      # output_graph=True  # 是否输出 tensorboard 文件
                      )
    env.after(100, run_maze)
    env.mainloop()
    RL.plot_cost()  # 观看神经网络的误差曲线
</code></pre>
<p>下一节我们会来讲解 Deep Q Network 这种算法具体要怎么编。</p>
<h1 id="16-DQN神经网络—TensorFlow"><a href="#16-DQN神经网络—TensorFlow" class="headerlink" title="16. DQN神经网络—TensorFlow"></a>16. DQN神经网络—TensorFlow</h1><h2 id="16-1-要点"><a href="#16-1-要点" class="headerlink" title="16.1 要点"></a>16.1 要点</h2><p>接着上节内容，这节我们使用 Tensorflow，来搭建 DQN 当中的神经网络部分 (用来预测 Q 值)。</p>
<h2 id="16-2-两个神经网络"><a href="#16-2-两个神经网络" class="headerlink" title="16.2 两个神经网络"></a>16.2 两个神经网络</h2><p>为了使用 Tensorflow 来实现 DQN，比较推荐的方式是搭建两个神经网，<code>target_net</code> 用于预测 <code>q_target</code> 值，它不会及时更新参数。<code>eval_net</code> 用于预测 <code>q_eval</code>，这个神经网络拥有最新的神经网络参数。不过这两个神经网络结构是完全一样的，只是里面的参数不一样。在这个短视频里，能找到我们为什么要建立两个不同参数的神经网络。</p>
<p><img src="https://img-blog.csdnimg.cn/20191211103056750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="16-3-神经网络结构"><a href="#16-3-神经网络结构" class="headerlink" title="16.3 神经网络结构"></a>16.3 神经网络结构</h2><p>因为 DQN 的结构相比之前所讲的内容都不一样，所以我们不使用继承来实现这次的功能。这次我们创建一个 <code>DeepQNetwork</code> 的 <code>class</code>，以及神经网络部分的功能。下次再说强化学习的更新部分。</p>
<pre><code class="python">class DeepQNetwork:
    # 建立神经网络
    def _build_net(self):
</code></pre>
<h2 id="16-4-常见两个网络"><a href="#16-4-常见两个网络" class="headerlink" title="16.4 常见两个网络"></a>16.4 常见两个网络</h2><p>两个神经网络是为了固定住一个神经网络 (<code>target_net</code>) 的参数，<code>target_net</code> 是 <code>eval_net</code> 的一个历史版本，拥有 <code>eval_net</code> 很久之前的一组参数，而且这组参数被固定一段时间，然后再被 <code>eval_net</code> 的新参数所替换。而 <code>eval_net</code> 是不断在被提升的，所以是一个可以被训练的网络 <code>trainable=True</code>，而 <code>target_net</code> 的 <code>trainable=False</code>。</p>
<pre><code class="python">class DeepQNetwork:
    def _build_net(self):
        # -------------- 创建 eval 神经网络, 及时提升参数 --------------
        self.s = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s&#39;)  # 用来接收 observation
        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name=&#39;Q_target&#39;) # 用来接收 q_target 的值, 这个之后会通过计算得到
        with tf.variable_scope(&#39;eval_net&#39;):
            # c_names(collections_names) 是在更新 target_net 参数时会用到
            c_names, n_l1, w_initializer, b_initializer = \
                [&#39;eval_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES], 10, \
                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers

            # eval_net 的第一层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope(&#39;l1&#39;):
                w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)
                b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names)
                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)

            # eval_net 的第二层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope(&#39;l2&#39;):
                w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)
                b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names)
                self.q_eval = tf.matmul(l1, w2) + b2

        with tf.variable_scope(&#39;loss&#39;): # 求误差
            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))
        with tf.variable_scope(&#39;train&#39;):    # 梯度下降
            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)

        # ---------------- 创建 target 神经网络, 提供 target Q ---------------------
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s_&#39;)    # 接收下个 observation
        with tf.variable_scope(&#39;target_net&#39;):
            # c_names(collections_names) 是在更新 target_net 参数时会用到
            c_names = [&#39;target_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES]

            # target_net 的第一层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope(&#39;l1&#39;):
                w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)
                b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names)
                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)

            # target_net 的第二层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope(&#39;l2&#39;):
                w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)
                b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names)
                self.q_next = tf.matmul(l1, w2) + b2
</code></pre>
<h1 id="17-DQN思维决策—TensorFlow"><a href="#17-DQN思维决策—TensorFlow" class="headerlink" title="17. DQN思维决策—TensorFlow"></a>17. DQN思维决策—TensorFlow</h1><p>接着上节内容，我们来定义 <code>DeepQNetwork</code> 的决策和思考部分。</p>
<h2 id="17-1-代码主结构"><a href="#17-1-代码主结构" class="headerlink" title="17.1 代码主结构"></a>17.1 代码主结构</h2><p>定义完上次的神经网络部分以后，这次我们来定义其他部分，包括:</p>
<pre><code class="python">class DeepQNetwork:
    # 上次的内容
    def _build_net(self):

    # 这次的内容:
    # 初始值
    def __init__(self):

    # 存储记忆
    def store_transition(self, s, a, r, s_):

    # 选行为
    def choose_action(self, observation):

    # 学习
    def learn(self):

    # 看看学习效果 (可选)
    def plot_cost(self):
</code></pre>
<h2 id="17-2-初始值"><a href="#17-2-初始值" class="headerlink" title="17.2 初始值"></a>17.2 初始值</h2><pre><code class="python">class DeepQNetwork:
    def __init__(
            self,
            n_actions,
            n_features,
            learning_rate=0.01,
            reward_decay=0.9,
            e_greedy=0.9,
            replace_target_iter=300,
            memory_size=500,
            batch_size=32,
            e_greedy_increment=None,
            output_graph=False,
    ):
        self.n_actions = n_actions
        self.n_features = n_features
        self.lr = learning_rate
        self.gamma = reward_decay
        self.epsilon_max = e_greedy     # epsilon 的最大值
        self.replace_target_iter = replace_target_iter  # 更换 target_net 的步数
        self.memory_size = memory_size  # 记忆上限
        self.batch_size = batch_size    # 每次更新时从 memory 里面取多少记忆出来
        self.epsilon_increment = e_greedy_increment # epsilon 的增量
        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max # 是否开启探索模式, 并逐步减少探索次数

        # 记录学习次数 (用于判断是否更换 target_net 参数)
        self.learn_step_counter = 0

        # 初始化全 0 记忆 [s, a, r, s_]
        self.memory = np.zeros((self.memory_size, n_features*2+2)) # 和视频中不同, 因为 pandas 运算比较慢, 这里改为直接用 numpy

        # 创建 [target_net, evaluate_net]
        self._build_net()

        # 替换 target net 的参数
        t_params = tf.get_collection(&#39;target_net_params&#39;)  # 提取 target_net 的参数
        e_params = tf.get_collection(&#39;eval_net_params&#39;)   # 提取  eval_net 的参数
        self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)] # 更新 target_net 参数

        self.sess = tf.Session()

        # 输出 tensorboard 文件
        if output_graph:
            # $ tensorboard --logdir=logs
            tf.summary.FileWriter(&quot;logs/&quot;, self.sess.graph)

        self.sess.run(tf.global_variables_initializer())
        self.cost_his = []  # 记录所有 cost 变化, 用于最后 plot 出来观看
</code></pre>
<h2 id="17-3-存储记忆"><a href="#17-3-存储记忆" class="headerlink" title="17.3 存储记忆"></a>17.3 存储记忆</h2><p>DQN 的精髓部分之一: 记录下所有经历过的步，这些步可以进行反复的学习，所以是一种 <code>off-policy</code> 方法，你甚至可以自己玩，然后记录下自己玩的经历，让这个 DQN 学习你是如何通关的。</p>
<pre><code class="python">class DeepQNetwork:
    def __init__(self):
        pass
    def store_transition(self, s, a, r, s_):
        if not hasattr(self, &#39;memory_counter&#39;):
            self.memory_counter = 0

        # 记录一条 [s, a, r, s_] 记录
        transition = np.hstack((s, [a, r], s_))

        # 总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换
        index = self.memory_counter % self.memory_size
        self.memory[index, :] = transition # 替换过程

        self.memory_counter += 1
</code></pre>
<h2 id="17-4-选行为"><a href="#17-4-选行为" class="headerlink" title="17.4 选行为"></a>17.4 选行为</h2><p>和之前的 <code>QLearningTable</code> 和 <code>SarsaTable</code> 等一样，都需要一个选行为的功能。</p>
<pre><code class="python">class DeepQNetwork:
    def __init__(self):
        pass
    def store_transition(self, s, a, r, s_):
        pass
    def choose_action(self, observation):
        # 统一 observation 的 shape (1, size_of_observation)
        observation = observation[np.newaxis, :]

        if np.random.uniform() &lt; self.epsilon:
            # 让 eval_net 神经网络生成所有 action 的值, 并选择值最大的 action
            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})
            action = np.argmax(actions_value)
        else:
            action = np.random.randint(0, self.n_actions)   # 随机选择
        return action
</code></pre>
<h2 id="17-5-学习"><a href="#17-5-学习" class="headerlink" title="17.5 学习"></a>17.5 学习</h2><p>最重要的一步来了，就是在 <code>DeepQNetwork</code> 中，是如何学习以及更新参数的。这里涉及了 <code>target_net</code> 和 <code>eval_net</code> 的交互使用。</p>
<pre><code class="python">class DeepQNetwork:
    def __init__(self):
        ...
    def store_transition(self, s, a, r, s_):
        ...
    def choose_action(self, observation):
        ...
    def _replace_target_params(self):
        ...
    def learn(self):
        # 检查是否替换 target_net 参数
        if self.learn_step_counter % self.replace_target_iter == 0:
            self.sess.run(self.replace_target_op)
            print(&#39;\ntarget_params_replaced\n&#39;)

        # 从 memory 中随机抽取 batch_size 这么多记忆
        if self.memory_counter &gt; self.memory_size:
            sample_index = np.random.choice(self.memory_size, size=self.batch_size)
        else:
            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)
        batch_memory = self.memory[sample_index, :]

        # 获取 q_next (target_net 产生了 q) 和 q_eval(eval_net 产生的 q)
        q_next, q_eval = self.sess.run(
            [self.q_next, self.q_eval],
            feed_dict={
                self.s_: batch_memory[:, -self.n_features:],
                self.s: batch_memory[:, :self.n_features]
            })

        # 下面这几步十分重要. q_next, q_eval 包含所有 action 的值,
        # 而我们需要的只是已经选择好的 action 的值, 其他的并不需要.
        # 所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新凭据.
        # 这是我们最终要达到的样子, 比如 q_target - q_eval = [1, 0, 0] - [-1, 0, 0] = [2, 0, 0]
        # q_eval = [-1, 0, 0] 表示这一个记忆中有我选用过 action 0, 而 action 0 带来的 Q(s, a0) = -1, 所以其他的 Q(s, a1) = Q(s, a2) = 0.
        # q_target = [1, 0, 0] 表示这个记忆中的 r+gamma*maxQ(s_) = 1, 而且不管在 s_ 上我们取了哪个 action,
        # 我们都需要对应上 q_eval 中的 action 位置, 所以就将 1 放在了 action 0 的位置.

        # 下面也是为了达到上面说的目的, 不过为了更方面让程序运算, 达到目的的过程有点不同.
        # 是将 q_eval 全部赋值给 q_target, 这时 q_target-q_eval 全为 0,
        # 不过 我们再根据 batch_memory 当中的 action 这个 column 来给 q_target 中的对应的 memory-action 位置来修改赋值.
        # 使新的赋值为 reward + gamma * maxQ(s_), 这样 q_target-q_eval 就可以变成我们所需的样子.
        # 具体在下面还有一个举例说明.

        q_target = q_eval.copy()
        batch_index = np.arange(self.batch_size, dtype=np.int32)
        eval_act_index = batch_memory[:, self.n_features].astype(int)
        reward = batch_memory[:, self.n_features + 1]

        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)

        &quot;&quot;&quot;
        假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:
        q_eval =
        [[1, 2, 3],
         [4, 5, 6]]

        q_target = q_eval =
        [[1, 2, 3],
         [4, 5, 6]]

        然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:
        比如在:
            记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;
            记忆 1 的 q_target 计算值是 -2, 而且我用了 action 2:
        q_target =
        [[-1, 2, 3],
         [4, 5, -2]]

        所以 (q_target - q_eval) 就变成了:
        [[(-1)-(1), 0, 0],
         [0, 0, (-2)-(6)]]

        最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络.
        所有为 0 的 action 值是当时没有选择的 action, 
        之前有选择的 action 才有不为0的值.
        我们只反向传递之前选择的 action 的值,
        &quot;&quot;&quot;

        # 训练 eval_net
        _, self.cost = self.sess.run([self._train_op, self.loss],
                feed_dict={self.s: batch_memory[:, :self.n_features],
                           self.q_target: q_target})
        self.cost_his.append(self.cost) # 记录 cost 误差

        # 逐渐增加 epsilon, 降低行为的随机性
        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max
        self.learn_step_counter += 1
</code></pre>
<h2 id="17-6-学习效果"><a href="#17-6-学习效果" class="headerlink" title="17.6 学习效果"></a>17.6 学习效果</h2><p>为了看看学习效果，我们在最后输出学习过程中的 <code>cost</code> 变化曲线。</p>
<pre><code class="python">class DeepQNetwork:
    def __init__(self):
        ...
    def store_transition(self, s, a, r, s_):
        ...
    def choose_action(self, observation):
        ...
    def _replace_target_params(self):
        ...
    def learn(self):
        ...
    def plot_cost(self):
        import matplotlib.pyplot as plt
        plt.plot(np.arange(len(self.cost_his)), self.cost_his)
        plt.ylabel(&#39;Cost&#39;)
        plt.xlabel(&#39;training steps&#39;)
        plt.show()
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20191211105754234.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>可以看出曲线并不是平滑下降的，这是因为 DQN 中的 <code>input</code> 数据是一步步改变的，而且会根据学习情况，获取到不同的数据。所以这并不像一般的监督学习，DQN 的 <code>cost</code> 曲线就有所不同了。</p>
<h2 id="17-7-修改版的-DQN"><a href="#17-7-修改版的-DQN" class="headerlink" title="17.7 修改版的 DQN"></a>17.7 修改版的 DQN</h2><p><img src="https://img-blog.csdnimg.cn/20191211105915178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>最后提供一种修改版的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/DQN_modified.py" target="_blank" rel="noopener">DQN 代码</a>，这是录制完视频以后做的，这是将 <code>q_target</code> 的计算也加在了 Tensorflow 的 <code>graph</code> 里面。这种结构还是有好处的, 作为学习样本的话，计算结构全部在 <code>tensorboard</code> 上，就更好理解，代码结构也更好理解。</p>
<p>我只在原本的 DQN 代码上改了一点点东西，大家应该可以很容易辨别。</p>
<h1 id="18-OpenAI-gym环境库"><a href="#18-OpenAI-gym环境库" class="headerlink" title="18. OpenAI gym环境库"></a>18. OpenAI gym环境库</h1><h2 id="18-1-要点"><a href="#18-1-要点" class="headerlink" title="18.1 要点"></a>18.1 要点</h2><p>手动编环境是一件很耗时间的事情，所以如果有能力使用别人已经编好的环境，可以节约我们很多时间。OpenAI gym 就是这样一个模块，它提供了我们很多优秀的模拟环境。我们的各种 RL 算法都能使用这些环境，不过 OpenAI gym 暂时只支持 MacOS 和 Linux 系统，Windows 已经支持，但是听说还没有全面支持，大家时不时查看下官网，可能。是在等不及更新了，也行用 tkinter 来手动编写一下环境。之前的 maze 环境是用 <code>tkinter</code> 编出来的，实在不行可以使用 <code>tkinter</code> 编写环境。或者还可以玩玩更厉害的，想 OpenAI 一样，使用 <code>pyglet</code> 模块来编写，莫烦做了一个从环境开始编写的<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1/" target="_blank" rel="noopener">强化学习实战</a>。</p>
<h2 id="18-2-安装gym"><a href="#18-2-安装gym" class="headerlink" title="18.2 安装gym"></a>18.2 安装gym</h2><p>在 MacOS 和 Linux 系统下，安装 <code>gym</code> 很方便，首先确定你是 python 2.7 或者 python 3.5 版本。然后在你的 terminal 中复制下面这些，但是 gym 暂时还不完全支持 Windows，不过有些虚拟环境已经的到了支持，想立杆子那个已经支持了。 所以接下来要说的安装方法只有 MacOS 和 Linux 的。Windows 用户的安装方式应该也差不多，如果 Windows 用户遇到了问题，欢迎在留言区分享解决的方法。</p>
<pre><code class="bash"># python 2.7, 复制下面
→ pip install gym

# python 3.5, 复制下面
→ pip3 install gym
</code></pre>
<p>如果没有报错，恭喜你，这样你就装好了 gym 的最基本款，可以开始玩以下游戏啦:</p>
<ul>
<li><a href="https://gym.openai.com/envs#algorithmic" target="_blank" rel="noopener">algorithmic</a></li>
<li><a href="https://gym.openai.com/envs#toy_text" target="_blank" rel="noopener">toy_text</a></li>
<li><a href="https://gym.openai.com/envs#classic_control" target="_blank" rel="noopener">classic_control</a> (这个需要 pyglet 模块)</li>
</ul>
<p>如果在安装中遇到问题。可能是缺少了一些必要模块，可以使用下面语句来安装这些模块(安装时间可能有点久):</p>
<pre><code class="bash"># MacOS:
$ brew install cmake boost boost-python sdl2 swig wget

# Ubuntu 14.04:
$ apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig
</code></pre>
<p>如果想要玩 gym 提供的全套游戏，下面这几句就能满足你:</p>
<pre><code class="bash"># python 2.7, 复制下面
→ pip install gym[all]

# python 3.5, 复制下面
→ pip3 install gym[all]
</code></pre>
<h2 id="18-3-CartPole例子"><a href="#18-3-CartPole例子" class="headerlink" title="18.3 CartPole例子"></a>18.3 CartPole例子</h2><iframe 
    width="580"
    height="500" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/cartpole%20dqn.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<p>之前我编写的 <code>maze_env</code> 基本上是按照 <code>gym</code> 环境格式写的，所以你换成 <code>gym</code> 格式会很简单。</p>
<p>接下来我们对应上面的算法，来实现主循环，首先 import 所需模。</p>
<pre><code class="python">import gym
from RL_brain import DeepQNetwork

env = gym.make(&#39;CartPole-v0&#39;)   # 定义使用 gym 库中的那一个环境
env = env.unwrapped # 不做这个会有很多限制

print(env.action_space) # 查看这个环境中可用的 action 有多少个
print(env.observation_space)  # 查看这个环境中可用的 state 的 observation 有多少个
print(env.observation_space.high)  # 查看 observation 最高取值
print(env.observation_space.low)   # 查看 observation 最低取值
</code></pre>
<p>于之前使用 tkinter 定义的环境有点不一样，我们可以不使用 <code>if __name__ == &quot;__main__&quot;</code> 的方式，下面是一种类似，却更简单的写法。之中我们会用到里面的 <code>reward</code>，但是 <code>env.step()</code> 说提供的 <code>reward</code> 不一定是最有效率的 <code>reward</code>，我们大可对这些进行修改，使 DQN 学得更有效率。你可以自己对比一下不修改 reward 和 按我这样修改，他们学习过程的不同。</p>
<pre><code class="python"># 定义使用 DQN 的算法
RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0],
                  learning_rate=0.01, e_greedy=0.9,
                  replace_target_iter=100, memory_size=2000,
                  e_greedy_increment=0.0008,)

total_steps = 0 # 记录步数

for i_episode in range(100):

    # 获取回合 i_episode 第一个 observation
    observation = env.reset()
    ep_r = 0
    while True:
        env.render()    # 刷新环境

        action = RL.choose_action(observation)  # 选行为

        observation_, reward, done, info = env.step(action) # 获取下一个 state

        x, x_dot, theta, theta_dot = observation_  # 细分开,为了修改原配的 reward

        # x 是车的水平位移, 所以 r1 是车越偏离中心, 分越少
        # theta 是棒子离垂直的角度, 角度越大, 越不垂直. 所以 r2 是棒越垂直, 分越高

        x, x_dot, theta, theta_dot = observation_
        r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8
        r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5
        # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度, 这样 DQN 学习更有效率
        reward = r1 + r2   

        # 保存这一组记忆
        RL.store_transition(observation, action, reward, observation_)

        if total_steps &gt; 1000:
            RL.learn()  # 学习

        ep_r += reward
        if done:
            print(&#39;episode: &#39;, i_episode,
                  &#39;ep_r: &#39;, round(ep_r, 2),
                  &#39; epsilon: &#39;, round(RL.epsilon, 2))
            break

        observation = observation_
        total_steps += 1
# 最后输出 cost 曲线
RL.plot_cost()
</code></pre>
<p>这是更为典型的 RL cost 曲线:</p>
<p><img src="https://img-blog.csdnimg.cn/20191211111927696.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="18-4-MountainCar例子"><a href="#18-4-MountainCar例子" class="headerlink" title="18.4 MountainCar例子"></a>18.4 MountainCar例子</h2><iframe 
    width="580"
    height="500" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/mountaincar%20dqn.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<p>代码基本和上述代码相同，就只是在 reward 上动了下手脚。</p>
<pre><code class="python">import gym
from RL_brain import DeepQNetwork

env = gym.make(&#39;MountainCar-v0&#39;)
env = env.unwrapped

print(env.action_space)
print(env.observation_space)
print(env.observation_space.high)
print(env.observation_space.low)

RL = DeepQNetwork(n_actions=3, n_features=2, learning_rate=0.001, e_greedy=0.9,
                  replace_target_iter=300, memory_size=3000,
                  e_greedy_increment=0.0001,)

total_steps = 0


for i_episode in range(10):

    observation = env.reset()
    ep_r = 0
    while True:
        env.render()

        action = RL.choose_action(observation)

        observation_, reward, done, info = env.step(action)

        position, velocity = observation_

        # 车开得越高 reward 越大
        reward = abs(position - (-0.5))

        RL.store_transition(observation, action, reward, observation_)

        if total_steps &gt; 1000:
            RL.learn()

        ep_r += reward
        if done:
            get = &#39;| Get&#39; if observation_[0] &gt;= env.unwrapped.goal_position else &#39;| ----&#39;
            print(&#39;Epi: &#39;, i_episode,
                  get,
                  &#39;| Ep_r: &#39;, round(ep_r, 4),
                  &#39;| Epsilon: &#39;, round(RL.epsilon, 2))
            break

        observation = observation_
        total_steps += 1

RL.plot_cost()
</code></pre>
<p>出来的 cost 曲线是这样:</p>
<p><img src="https://img-blog.csdnimg.cn/20191211112627768.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这两个都只是例子而已，具体的实施你也可以大动手脚，比如你的 <code>reward</code> 定义得更好，你的神经网络结构更好，使得他们学的更快，这些都是自己定义的。</p>
<h1 id="19-Double-DQN—TensorFlow"><a href="#19-Double-DQN—TensorFlow" class="headerlink" title="19. Double DQN—TensorFlow"></a>19. Double DQN—TensorFlow</h1><h2 id="19-1-要点"><a href="#19-1-要点" class="headerlink" title="19.1 要点"></a>19.1 要点</h2><p>接下来我们说说为什么会有 Double DQN 这种算法。所以我们从 Double DQN 相对于 Natural DQN (传统 DQN) 的优势说起。</p>
<p>一句话概括，DQN 基于 Q-Learning，Q-Learning 中有 <code>Qmax</code>, <code>Qmax</code> 会导致 <code>Q现实</code> 当中的<strong>过估计</strong> (overestimate)，而 Double DQN 就是用来解决过估计的。在实际问题中，如果你输出你的 DQN 的 Q 值，可能就会发现，Q 值都超级大，这就是出现了 overestimate。</p>
<p>这次的 Double DQN 的算法基于的是 OpenAI Gym 中的 <code>Pendulum</code> 环境。</p>
<iframe 
    width="580"
    height="500" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/Pendulum%20DQN.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<h2 id="19-2-Double-DQN算法"><a href="#19-2-Double-DQN算法" class="headerlink" title="19.2 Double DQN算法"></a>19.2 Double DQN算法</h2><p>我们知道 DQN 的神经网络部分可以看成一个 <code>最新的神经网络</code> + <code>老神经网络</code>，他们有相同的结构，但内部的参数更新却有时差，而它的 <code>Q现实</code> 部分是这样的:<br>$$<br>Y_t^{DQN} = R_{t+1} + \gamma ; max_a Q(S_{t+1}, a; \theta_t^-)<br>$$<br>因为我们的神经网络预测 <code>Qmax</code> 本来就有误差，每次也向着最大误差的 <code>Q现实</code> 改进神经网络，就是因为这个 <code>Qmax</code> 导致了 overestimate，所以 Double DQN 的想法就是引入另一个神经网络来打消一些最大误差的影响。而 DQN 中本来就有两个神经网络，我们何不利用一下这个地理优势呢。所以，我们用 <code>Q估计</code> 的神经网络估计 <code>Q现实</code> 中 <code>Qmax(s&#39;, a&#39;)</code> 的最大动作值。然后用这个被 <code>Q估计</code> 估计出来的动作来选择 <code>Q现实</code> 中的 <code>Q(s&#39;)</code>，总结一下:</p>
<p>有两个神经网络: <code>Q_eval</code> (Q估计中的) 和<code>Q_next</code> (Q现实中的)。原本的 <code>Q_next = max(Q_next(s&#39;, a_all))</code>。Double DQN 中的 <code>Q_next = Q_next(s&#39;, argmax(Q_eval(s&#39;, a_all)))</code>，也可以表达成下面那样:<br>$$<br>Y_t^{DoubleDQN} = R_{t+1} + \gamma ; Q(argmax_a Q(S_{t+1}, a; \theta_t), \theta_t^-)<br>$$</p>
<h2 id="19-3-更新方法"><a href="#19-3-更新方法" class="headerlink" title="19.3 更新方法"></a>19.3 更新方法</h2><p>这里的代码都是基于之前 DQN 教程中的代码 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/RL_brain.py" target="_blank" rel="noopener">(github)</a>，在 <code>RL_brain</code> 中我们将 <code>class</code> 的名字改成 <code>DoubleDQN</code>，为了对比 Natural DQN，我们也保留原来大部分的 DQN 的代码。我们在 <code>__init__</code> 中加一个 <code>double_q</code> 参数来表示使用的是 Natural DQN 还是 Double DQN。为了对比的需要，我们的 <code>tf.Session()</code> 也单独传入，并移除原本在 DQN 代码中的这一句 <code>self.sess.run(tf.global_variables_initializer())</code>。</p>
<pre><code class="python">class DoubleDQN:
    def __init__(..., double_q=True, sess=None):
        ...
        self.double_q = double_q
        if sess is None:
            self.sess = tf.Session()
            self.sess.run(tf.global_variables_initializer())
        else:
            self.sess = sess
        ...
</code></pre>
<p>接着我们来修改 <code>learn()</code> 中的代码，我们对比 Double DQN 和 Natural DQN 在 tensorboard 中的图，发现他们的结构并没有不同，但是在计算 <code>q_target</code> 的时候，方法是不同的。</p>
<p><img src="https://img-blog.csdnimg.cn/20191211122057506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<pre><code class="python">class DoubleDQN:
    def learn(self):
        # 这一段和 DQN 一样:
        if self.learn_step_counter % self.replace_target_iter == 0:
            self.sess.run(self.replace_target_op)
            print(&#39;\ntarget_params_replaced\n&#39;)

        if self.memory_counter &gt; self.memory_size:
            sample_index = np.random.choice(self.memory_size, size=self.batch_size)
        else:
            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)
        batch_memory = self.memory[sample_index, :]

        # 这一段和 DQN 不一样
        q_next, q_eval4next = self.sess.run(
            [self.q_next, self.q_eval],
            feed_dict={self.s_: batch_memory[:, -self.n_features:],    # next observation
                       self.s: batch_memory[:, -self.n_features:]})    # next observation
        q_eval = self.sess.run(self.q_eval, {self.s: batch_memory[:, :self.n_features]})
        q_target = q_eval.copy()
        batch_index = np.arange(self.batch_size, dtype=np.int32)
        eval_act_index = batch_memory[:, self.n_features].astype(int)
        reward = batch_memory[:, self.n_features + 1]

        if self.double_q:   # 如果是 Double DQN
            # q_eval 得出的最高奖励动作
            max_act4next = np.argmax(q_eval4next, axis=1)
            # Double DQN 选择 q_next 依据 q_eval 选出的动作
            selected_q_next = q_next[batch_index, max_act4next]
        else:       # 如果是 Natural DQN
            selected_q_next = np.max(q_next, axis=1)  # natural DQN

        q_target[batch_index, eval_act_index] = reward + self.gamma * selected_q_next

        # 这下面和 DQN 一样:
        _, self.cost = self.sess.run([self._train_op, self.loss],
                feed_dict={self.s: batch_memory[:, :self.n_features],
                           self.q_target: q_target})
        self.cost_his.append(self.cost)
        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max
        self.learn_step_counter += 1
</code></pre>
<h2 id="19-4-记录-Q-值"><a href="#19-4-记录-Q-值" class="headerlink" title="19.4 记录 Q 值"></a>19.4 记录 Q 值</h2><p>为了记录下我们选择动作时的 Q 值，接下来我们就修改 <code>choose_action()</code> 功能，让它记录下每次选择的 Q 值。</p>
<pre><code class="python">class DoubleDQN:
    def choose_action(self, observation):
        observation = observation[np.newaxis, :]
        actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})
        action = np.argmax(actions_value)

        if not hasattr(self, &#39;q&#39;):  # 记录选的 Qmax 值
            self.q = []
            self.running_q = 0
        self.running_q = self.running_q*0.99 + 0.01 * np.max(actions_value)
        self.q.append(self.running_q)

        if np.random.uniform() &gt; self.epsilon:  # 随机选动作
            action = np.random.randint(0, self.n_actions)
        return action
</code></pre>
<h2 id="19-5-对比结果"><a href="#19-5-对比结果" class="headerlink" title="19.5 对比结果"></a>19.5 对比结果</h2><p>接着我们就来对比 Natural DQN 和 Double DQN 带来的不同结果啦，<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/run_Pendulum.py" target="_blank" rel="noopener">代码在这</a></p>
<pre><code class="python">import gym
from RL_brain import DoubleDQN
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf


env = gym.make(&#39;Pendulum-v0&#39;)
env.seed(1) # 可重复实验
MEMORY_SIZE = 3000
ACTION_SPACE = 11    # 将原本的连续动作分离成 11 个动作

sess = tf.Session()
with tf.variable_scope(&#39;Natural_DQN&#39;):
    natural_DQN = DoubleDQN(
        n_actions=ACTION_SPACE, n_features=3, memory_size=MEMORY_SIZE,
        e_greedy_increment=0.001, double_q=False, sess=sess
    )

with tf.variable_scope(&#39;Double_DQN&#39;):
    double_DQN = DoubleDQN(
        n_actions=ACTION_SPACE, n_features=3, memory_size=MEMORY_SIZE,
        e_greedy_increment=0.001, double_q=True, sess=sess, output_graph=True)

sess.run(tf.global_variables_initializer())


def train(RL):
    total_steps = 0
    observation = env.reset()
    while True:
        # if total_steps - MEMORY_SIZE &gt; 8000: env.render()

        action = RL.choose_action(observation)

        f_action = (action-(ACTION_SPACE-1)/2)/((ACTION_SPACE-1)/4)   # 在 [-2 ~ 2] 内离散化动作

        observation_, reward, done, info = env.step(np.array([f_action]))

        reward /= 10     # normalize 到这个区间 (-1, 0). 立起来的时候 reward = 0.
        # 立起来以后的 Q target 会变成 0, 因为 Q_target = r + gamma * Qmax(s&#39;, a&#39;) = 0 + gamma * 0
        # 所以这个状态时的 Q 值大于 0 时, 就出现了 overestimate.

        RL.store_transition(observation, action, reward, observation_)

        if total_steps &gt; MEMORY_SIZE:   # learning
            RL.learn()

        if total_steps - MEMORY_SIZE &gt; 20000:   # stop game
            break

        observation = observation_
        total_steps += 1
    return RL.q # 返回所有动作 Q 值

# train 两个不同的 DQN
q_natural = train(natural_DQN)
q_double = train(double_DQN)

# 出对比图
plt.plot(np.array(q_natural), c=&#39;r&#39;, label=&#39;natural&#39;)
plt.plot(np.array(q_double), c=&#39;b&#39;, label=&#39;double&#39;)
plt.legend(loc=&#39;best&#39;)
plt.ylabel(&#39;Q eval&#39;)
plt.xlabel(&#39;training steps&#39;)
plt.grid()
plt.show()
</code></pre>
<p>所以这个出来的图是这样:</p>
<p><img src="https://img-blog.csdnimg.cn/20191211122328325.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>可以看出, Natural DQN 学得差不多后，在立起来时，大部分时间都是 估计的 <strong>Q值</strong> 要大于0， 这时就出现了 overestimate，而 Double DQN 的 <strong>Q值</strong> 就消除了一些 overestimate， 将估计值保持在 0 左右。</p>
<h1 id="20-Prioritized-Experience-Replay-DQN-—TensorFlow"><a href="#20-Prioritized-Experience-Replay-DQN-—TensorFlow" class="headerlink" title="20. Prioritized Experience Replay(DQN)—TensorFlow"></a>20. Prioritized Experience Replay(DQN)—TensorFlow</h1><h2 id="20-1-要点"><a href="#20-1-要点" class="headerlink" title="20.1 要点"></a>20.1 要点</h2><p>这一次还是使用 MountainCar 来进行实验，因为这次我们不需要重度改变它的 reward 了。所以只要是没有拿到小旗子，<code>reward=-1</code>，拿到小旗子时，我们定义它获得了 <code>+10</code> 的 reward。比起之前 DQN 中，这个 reward 定义更加准。如果使用这种 reward 定义方式，可以想象 Natural DQN 会花很久的时间学习，因为记忆库中只有很少很少的 <code>+10</code> reward 可以学习。正负样本不一样，而使用 Prioritized replay，就会重视这种少量的，但值得学习的样本。</p>
<iframe 
    width="580"
    height="500" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/mountaincar%20dqn.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<h2 id="20-2-Prioritized-Replay算法"><a href="#20-2-Prioritized-Replay算法" class="headerlink" title="20.2 Prioritized Replay算法"></a>20.2 Prioritized Replay算法</h2><p><img src="https://img-blog.csdnimg.cn/20191211122754539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这一套算法重点就在我们 <code>batch</code> 抽样的时候并不是随机抽样，而是按照 Memory 中的样本优先级来抽，所以这能更有效地找到我们需要学习的样本。</p>
<p>那么样本的优先级是怎么定的呢？原来我们可以用到 <code>TD-error</code>，也就是 <code>Q现实 - Q估计</code> 来规定优先学习的程度。如果 <code>TD-error</code> 越大，就代表我们的预测精度还有很多上升空间，那么这个样本就越需要被学习，也就是优先级 <code>p</code> 越高。</p>
<p>有了 <code>TD-error</code> 就有了优先级 <code>p</code>，那我们如何有效地根据 <code>p</code> 来抽样呢？如果每次抽样都需要针对 <code>p</code> 对所有样本排序，这将会是一件非常消耗计算能力的事。好在我们还有其他方法，这种方法不会对得到的样本进行排序，这就是这篇 <a href="https://arxiv.org/abs/1511.05952" target="_blank" rel="noopener">paper</a> 中提到的 <code>SumTree</code>.</p>
<p>SumTree 是一种树形结构，每片树叶存储每个样本的优先级 <code>p</code>，每个树枝节点只有两个分叉，节点的值是两个分叉的合，所以 SumTree 的顶端就是所有 <code>p</code> 的和。正如下面<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" target="_blank" rel="noopener">图片(来自Jaromír Janisch)</a>，最下面一层树叶存储样本的 <code>p</code>，叶子上一层最左边的 13 = 3 + 10，按这个规律相加，顶层的 root 就是全部 <code>p</code> 的合了。</p>
<p><img src="https://img-blog.csdnimg.cn/20191211123132523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>抽样时，我们会将 <code>p</code> 的总合除以 <code>batch size</code>，分成 batch size 那么多区间，<code>(n=sum(p)/batch_size)</code>。如果将所有 node 的 <code>priority</code> 加起来是42的，我们如果抽6个样本，这时的区间拥有的 priority 可能是这样。</p>
<pre><code>[0-7], [7-14], [14-21], [21-28], [28-35], [35-42]</code></pre><p>然后在每个区间里随机选取一个数。比如在第区间 <code>[21-28]</code> 里选到了24，就按照这个 24 从最顶上的 42 开始向下搜索。首先看到最顶上 <code>42</code> 下面有两个 child nodes，拿着手中的 24 对比左边的 child <code>29</code>，如果 左边的 child 比自己手中的值大，那我们就走左边这条路，接着再对比 <code>29</code> 下面的左边那个点 <code>13</code>，这时手中的 24 比 <code>13</code> 大，那我们就走右边的路，并且将手中的值根据 <code>13</code> 修改一下，变成 <code>24-13=11</code>。 接着拿着 11 和 <code>13</code> 左下角的 <code>12</code> 比，结果 <code>12</code> 比 11 大，那我们就选 12 当做这次选到的 priority，并且也选择 12 对应的数据。</p>
<h2 id="20-3-SumTree有效抽样"><a href="#20-3-SumTree有效抽样" class="headerlink" title="20.3 SumTree有效抽样"></a>20.3 SumTree有效抽样</h2><p>首先要提的是, 这个 SumTree 的算法是对于 <a href="https://github.com/jaara/AI-blog/blob/master/SumTree.py" target="_blank" rel="noopener">Jaromír Janisch 写的 Sumtree</a> 的修改版。Jaromír Janisch 的代码在更新 sumtree 的时候和抽样的时候多次用到了 recursive 递归结构，我使用的是 while 循环，测试要比递归结构运行快。在 <code>class</code> 中的功能也比它的代码少几个，我优化了一下。</p>
<pre><code class="python">class SumTree(object):
    # 建立 tree 和 data,
    # 因为 SumTree 有特殊的数据结构,
    # 所以两者都能用一个一维 np.array 来存储
    def __init__(self, capacity):

    # 当有新 sample 时, 添加进 tree 和 data
    def add(self, p, data):

    # 当 sample 被 train, 有了新的 TD-error, 就在 tree 中更新
    def update(self, tree_idx, p):

    # 根据选取的 v 点抽取样本
    def get_leaf(self, v):

    # 获取 sum(priorities)
    @property
    def totoal_p(self):
</code></pre>
<p>具体的抽要和更新值的规则和上面说的类似. 具体的代码在这里呈现的话比较累赘, 详细代码请去往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py#L18-L86" target="_blank" rel="noopener">Github对应的位置</a></p>
<h2 id="20-4-Memory类"><a href="#20-4-Memory类" class="headerlink" title="20.4 Memory类"></a>20.4 Memory类</h2><p>这个 Memory 类也是基于 <a href="https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py" target="_blank" rel="noopener">Jaromír Janisch 所写的 Memory</a> 进行了修改和优化。</p>
<pre><code class="python">class Memory(object):
    # 建立 SumTree 和各种参数
    def __init__(self, capacity):

    # 存储数据, 更新 SumTree
    def store(self, transition):

    # 抽取 sample
    def sample(self, n):

    # train 完被抽取的 samples 后更新在 tree 中的 sample 的 priority
    def batch_update(self, tree_idx, abs_errors):
</code></pre>
<p>具体的代码在这里呈现的话比较累赘，详细代码请去往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py#L89-L129" target="_blank" rel="noopener">Github对应的位置</a> 下面有很多朋友经常问的一个问题，这个 <code>ISweight</code> 到底怎么算。需要提到的一点是，代码中的计算方法是经过了简化的，将 paper 中的步骤合并了一些。比如 <code>prob = p / self.tree.total_p; ISWeights = np.power(prob/min_prob, -self.beta)</code><br>$$<br>\text {Compute importance-sampling weight } w_j=(N\cdot P(j))^{-\beta} / max_iw_i<br>$$<br>下面是我的推导，如果有不正确还请指出。在paper 中，<code>ISWeight = (N*Pj)^(-beta) / maxi_wi</code> 里面的 <code>maxi_wi</code> 是为了 <code>normalize ISWeight</code>，所以我们先把他放在一边。所以单纯的 importance sampling 就是 <code>(N*Pj)^(-beta)</code>，那 <code>maxi_wi = maxi[(N*Pi)^(-beta)]</code>。</p>
<p>如果将这两个式子合并:</p>
<pre><code>ISWeight = (N*Pj)^(-beta) / maxi[ (N*Pi)^(-beta) ]</code></pre><p>而且如果将 <code>maxi[ (N*Pi)^(-beta) ]</code> 中的 (-beta) 提出来, 这就变成了 <code>mini[ (N*Pi) ] ^ (-beta)</code>。看出来了吧，有的东西可以抵消掉的。最后</p>
<pre><code>ISWeight = (Pj / mini[Pi])^(-beta)</code></pre><p>这样我们就有了代码中的样子。还有代码中的 <code>alpha</code> 是一个决定我们要使用多少 ISweight 的影响，如果 <code>alpha=0</code>，我们就没使用到任何 Importance Sampling。</p>
<h2 id="20-5-更新方法"><a href="#20-5-更新方法" class="headerlink" title="20.5 更新方法"></a>20.5 更新方法</h2><p>基于之前的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/RL_brain.py" target="_blank" rel="noopener">DQN 代码</a>，我们做出以下修改。我们将 class 的名字改成 <code>DQNPrioritiedReplay</code>，为了对比 Natural DQN，我们也保留原来大部分的 DQN 的代码。我们在 <code>__init__</code> 中加一个 <code>prioritized</code> 参数来表示 DQN 是否具备 <code>prioritized</code> 能力。为了对比的需要，我们的 <code>tf.Session()</code>也单独传入，并移除原本在 DQN 代码中的这一句: <code>self.sess.run(tf.global_variables_initializer())</code></p>
<pre><code class="python">class DQNPrioritiedReplay:
    def __init__(..., prioritized=True, sess=None)
        self.prioritized = prioritized
        # ...
        if self.prioritized:
            self.memory = Memory(capacity=memory_size)
        else:
            self.memory = np.zeros((self.memory_size, n_features*2+2))

        if sess is None:
            self.sess = tf.Session()
            self.sess.run(tf.global_variables_initializer())
        else:
            self.sess = sess
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20191211124238498.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>搭建神经网络时，我们发现 DQN with Prioritized replay 只多了一个 <code>ISWeights</code>，这个正是<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-6-prioritized-replay/#algorithm" target="_blank" rel="noopener">刚刚算法中</a>提到的 <code>Importance-Sampling Weights</code>，用来恢复被 Prioritized replay 打乱的抽样概率分布。</p>
<pre><code class="python">class DQNPrioritizedReplay:
    def _build_net(self)
        ...
        # self.prioritized 时 eval net 的 input 多加了一个 ISWeights
        self.s = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s&#39;)  # input
        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name=&#39;Q_target&#39;)  # for calculating loss
        if self.prioritized:
            self.ISWeights = tf.placeholder(tf.float32, [None, 1], name=&#39;IS_weights&#39;)

        ...
        # 为了得到 abs 的 TD error 并用于修改这些 sample 的 priority, 我们修改如下
        with tf.variable_scope(&#39;loss&#39;):
            if self.prioritized:
                self.abs_errors = tf.reduce_sum(tf.abs(self.q_target - self.q_eval), axis=1)    # for updating Sumtree
                self.loss = tf.reduce_mean(self.ISWeights * tf.squared_difference(self.q_target, self.q_eval))
            else:
                self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))
</code></pre>
<p>因为和 Natural DQN 使用的 Memory 不一样，所以在存储 transition 的时候方式也略不相同。</p>
<pre><code class="python">class DQNPrioritizedReplay:
    def store_transition(self, s, a, r, s_):
        if self.prioritized:    # prioritized replay
            transition = np.hstack((s, [a, r], s_))
            self.memory.store(transition)
        else:       # random replay
            if not hasattr(self, &#39;memory_counter&#39;):
                self.memory_counter = 0
            transition = np.hstack((s, [a, r], s_))
            index = self.memory_counter % self.memory_size
            self.memory[index, :] = transition
            self.memory_counter += 1
</code></pre>
<p>接下来是相对于 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py" target="_blank" rel="noopener">Natural DQN 代码</a>，我们在 <code>learn()</code> 改变的部分也在如下展示。</p>
<pre><code class="python">class DQNPrioritizedReplay:
    def learn(self):
        ...
        # 相对于 DQN 代码, 改变的部分
        if self.prioritized:
            tree_idx, batch_memory, ISWeights = self.memory.sample(self.batch_size)
        else:
            sample_index = np.random.choice(self.memory_size, size=self.batch_size)
            batch_memory = self.memory[sample_index, :]

        ...

        if self.prioritized:
            _, abs_errors, self.cost = self.sess.run([self._train_op, self.abs_errors, self.loss],
                    feed_dict={self.s: batch_memory[:, :self.n_features],
                               self.q_target: q_target,
                               self.ISWeights: ISWeights})
            self.memory.batch_update(tree_idx, abs_errors)   # update priority
        else:
            _, self.cost = self.sess.run([self._train_op, self.loss],
                    feed_dict={self.s: batch_memory[:, :self.n_features],
                               self.q_target: q_target})

        ...</code></pre>
<h2 id="20-6-对比结果"><a href="#20-6-对比结果" class="headerlink" title="20.6 对比结果"></a>20.6 对比结果</h2><p><img src="https://img-blog.csdnimg.cn/20191211124009684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>运行我 Github 中的这个 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/run_MountainCar.py" target="_blank" rel="noopener">MountainCar 脚本</a>，我们就不难发现，我们都从两种方法最初拿到第一个 <code>R=+10</code> 奖励的时候算起，看看经历过一次 <code>R=+10</code> 后，他们有没有好好利用这次的奖励。可以看出，有 Prioritized replay 的可以高效的利用这些不常拿到的奖励，并好好学习他们。所以 Prioritized replay 会更快结束每个 episode， 很快就到达了小旗子。</p>
<h1 id="22-Dueling-DQN—TensorFlow"><a href="#22-Dueling-DQN—TensorFlow" class="headerlink" title="22. Dueling DQN—TensorFlow"></a>22. Dueling DQN—TensorFlow</h1><h2 id="22-1-要点"><a href="#22-1-要点" class="headerlink" title="22.1 要点"></a>22.1 要点</h2><h2 id="22-2-Dueling算法"><a href="#22-2-Dueling算法" class="headerlink" title="22.2 Dueling算法"></a>22.2 Dueling算法</h2><h2 id="22-3-更新方法"><a href="#22-3-更新方法" class="headerlink" title="22.3 更新方法"></a>22.3 更新方法</h2><h2 id="22-4-对比结果"><a href="#22-4-对比结果" class="headerlink" title="22.4 对比结果"></a>22.4 对比结果</h2><h1 id="23-什么是Policy-Gradients"><a href="#23-什么是Policy-Gradients" class="headerlink" title="23. 什么是Policy Gradients"></a>23. 什么是Policy Gradients</h1><h2 id="23-1-和以往的强化学习方法不同"><a href="#23-1-和以往的强化学习方法不同" class="headerlink" title="23.1 和以往的强化学习方法不同"></a>23.1 和以往的强化学习方法不同</h2><h2 id="23-2-更新不同之处"><a href="#23-2-更新不同之处" class="headerlink" title="23.2 更新不同之处"></a>23.2 更新不同之处</h2><h2 id="23-3-具体更新步骤"><a href="#23-3-具体更新步骤" class="headerlink" title="23.3 具体更新步骤"></a>23.3 具体更新步骤</h2><h1 id="24-Policy-Gradients算法更新—TensorFlow"><a href="#24-Policy-Gradients算法更新—TensorFlow" class="headerlink" title="24. Policy Gradients算法更新—TensorFlow"></a>24. Policy Gradients算法更新—TensorFlow</h1><h2 id="24-1-要点"><a href="#24-1-要点" class="headerlink" title="24.1 要点"></a>24.1 要点</h2><h2 id="24-2-算法"><a href="#24-2-算法" class="headerlink" title="24.2 算法"></a>24.2 算法</h2><h2 id="24-3-算法代码形式"><a href="#24-3-算法代码形式" class="headerlink" title="24.3 算法代码形式"></a>24.3 算法代码形式</h2><h1 id="25-Policy-Gradients思维决策—TensorFlow"><a href="#25-Policy-Gradients思维决策—TensorFlow" class="headerlink" title="25. Policy Gradients思维决策—TensorFlow"></a>25. Policy Gradients思维决策—TensorFlow</h1><h2 id="25-1-主要代码结构"><a href="#25-1-主要代码结构" class="headerlink" title="25.1 主要代码结构"></a>25.1 主要代码结构</h2><h2 id="25-2-初始化"><a href="#25-2-初始化" class="headerlink" title="25.2 初始化"></a>25.2 初始化</h2><h2 id="25-3-建立Policy神经网络"><a href="#25-3-建立Policy神经网络" class="headerlink" title="25.3 建立Policy神经网络"></a>25.3 建立Policy神经网络</h2><h2 id="25-4-选行为"><a href="#25-4-选行为" class="headerlink" title="25.4 选行为"></a>25.4 选行为</h2><h2 id="25-5-存储回合"><a href="#25-5-存储回合" class="headerlink" title="25.5 存储回合"></a>25.5 存储回合</h2><h2 id="25-6-学习"><a href="#25-6-学习" class="headerlink" title="25.6 学习"></a>25.6 学习</h2><h1 id="26-什么是Actor-Critic"><a href="#26-什么是Actor-Critic" class="headerlink" title="26. 什么是Actor Critic"></a>26. 什么是Actor Critic</h1><h2 id="26-1-为什么要有Actor和Critic"><a href="#26-1-为什么要有Actor和Critic" class="headerlink" title="26.1 为什么要有Actor和Critic"></a>26.1 为什么要有Actor和Critic</h2><h2 id="26-2-Actor和Critic"><a href="#26-2-Actor和Critic" class="headerlink" title="26.2 Actor和Critic"></a>26.2 Actor和Critic</h2><h2 id="26-3-增加单步更新属性"><a href="#26-3-增加单步更新属性" class="headerlink" title="26.3 增加单步更新属性"></a>26.3 增加单步更新属性</h2><h2 id="26-4-改进版Deep-Deterministic-Policy-Gradient-DDPG"><a href="#26-4-改进版Deep-Deterministic-Policy-Gradient-DDPG" class="headerlink" title="26.4 改进版Deep Deterministic Policy Gradient(DDPG)"></a>26.4 改进版Deep Deterministic Policy Gradient(DDPG)</h2><h1 id="27-Actot-Critic—TensorFlow"><a href="#27-Actot-Critic—TensorFlow" class="headerlink" title="27. Actot Critic—TensorFlow"></a>27. Actot Critic—TensorFlow</h1><h2 id="27-1-要点"><a href="#27-1-要点" class="headerlink" title="27.1 要点"></a>27.1 要点</h2><h2 id="27-2-算法"><a href="#27-2-算法" class="headerlink" title="27.2 算法"></a>27.2 算法</h2><h2 id="27-3-代码主结构"><a href="#27-3-代码主结构" class="headerlink" title="27.3 代码主结构"></a>27.3 代码主结构</h2><h2 id="27-4-两者学习方式"><a href="#27-4-两者学习方式" class="headerlink" title="27.4 两者学习方式"></a>27.4 两者学习方式</h2><h2 id="27-5-每回合算法"><a href="#27-5-每回合算法" class="headerlink" title="27.5 每回合算法"></a>27.5 每回合算法</h2><h1 id="28-什么是Deep-Deterministic-Policy-Gradient-DDPG"><a href="#28-什么是Deep-Deterministic-Policy-Gradient-DDPG" class="headerlink" title="28. 什么是Deep Deterministic Policy Gradient(DDPG)"></a>28. 什么是Deep Deterministic Policy Gradient(DDPG)</h1><h2 id="28-1-拆分细讲"><a href="#28-1-拆分细讲" class="headerlink" title="28.1 拆分细讲"></a>28.1 拆分细讲</h2><h2 id="28-2-Deep和DQN"><a href="#28-2-Deep和DQN" class="headerlink" title="28.2 Deep和DQN"></a>28.2 Deep和DQN</h2><h2 id="28-3-Deterministic-Policy-Gradient"><a href="#28-3-Deterministic-Policy-Gradient" class="headerlink" title="28.3 Deterministic Policy Gradient"></a>28.3 Deterministic Policy Gradient</h2><h2 id="28-4-DDPG神经网络"><a href="#28-4-DDPG神经网络" class="headerlink" title="28.4 DDPG神经网络"></a>28.4 DDPG神经网络</h2><h1 id="29-Deep-Deterministic-Policy-Gradient-DDPG-—TensorFlow"><a href="#29-Deep-Deterministic-Policy-Gradient-DDPG-—TensorFlow" class="headerlink" title="29. Deep Deterministic Policy Gradient(DDPG)—TensorFlow"></a>29. Deep Deterministic Policy Gradient(DDPG)—TensorFlow</h1><h2 id="29-1-要点"><a href="#29-1-要点" class="headerlink" title="29.1 要点"></a>29.1 要点</h2><h2 id="29-2-算法"><a href="#29-2-算法" class="headerlink" title="29.2 算法"></a>29.2 算法</h2><h2 id="29-3-主结构"><a href="#29-3-主结构" class="headerlink" title="29.3 主结构"></a>29.3 主结构</h2><h2 id="29-4-主结构"><a href="#29-4-主结构" class="headerlink" title="29.4 主结构"></a>29.4 主结构</h2><h2 id="29-5-Actor-Critic"><a href="#29-5-Actor-Critic" class="headerlink" title="29.5 Actor Critic"></a>29.5 Actor Critic</h2><h2 id="29-6-记忆库Mmeory"><a href="#29-6-记忆库Mmeory" class="headerlink" title="29.6 记忆库Mmeory"></a>29.6 记忆库Mmeory</h2><h2 id="29-7-每回合算法"><a href="#29-7-每回合算法" class="headerlink" title="29.7 每回合算法"></a>29.7 每回合算法</h2><h2 id="29-8-简化版代码"><a href="#29-8-简化版代码" class="headerlink" title="29.8 简化版代码"></a>29.8 简化版代码</h2><h1 id="30-什么是Asynchronous-Advantage-Actor-Critic-A3C"><a href="#30-什么是Asynchronous-Advantage-Actor-Critic-A3C" class="headerlink" title="30. 什么是Asynchronous Advantage Actor-Critic (A3C)"></a>30. 什么是Asynchronous Advantage Actor-Critic (A3C)</h1><h2 id="30-1-平行宇宙"><a href="#30-1-平行宇宙" class="headerlink" title="30.1 平行宇宙"></a>30.1 平行宇宙</h2><h2 id="30-2-平行训练"><a href="#30-2-平行训练" class="headerlink" title="30.2 平行训练"></a>30.2 平行训练</h2><h2 id="30-3-多核训练"><a href="#30-3-多核训练" class="headerlink" title="30.3 多核训练"></a>30.3 多核训练</h2><h1 id="31-Asynchronous-Advantage-Actor-Critic-A3C-—TensorFlow"><a href="#31-Asynchronous-Advantage-Actor-Critic-A3C-—TensorFlow" class="headerlink" title="31. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow"></a>31. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow</h1><h2 id="31-1-要点"><a href="#31-1-要点" class="headerlink" title="31.1 要点"></a>31.1 要点</h2><h2 id="31-2-算法"><a href="#31-2-算法" class="headerlink" title="31.2 算法"></a>31.2 算法</h2><h2 id="31-3-主结构"><a href="#31-3-主结构" class="headerlink" title="31.3 主结构"></a>31.3 主结构</h2><h2 id="31-4-Actor-Critic网络"><a href="#31-4-Actor-Critic网络" class="headerlink" title="31.4 Actor Critic网络"></a>31.4 Actor Critic网络</h2><h2 id="31-5-Worker"><a href="#31-5-Worker" class="headerlink" title="31.5 Worker"></a>31.5 Worker</h2><h2 id="31-6-Worker并行工作"><a href="#31-6-Worker并行工作" class="headerlink" title="31.6 Worker并行工作"></a>31.6 Worker并行工作</h2><h2 id="31-7-机械手臂"><a href="#31-7-机械手臂" class="headerlink" title="31.7 机械手臂"></a>31.7 机械手臂</h2><h2 id="31-8-multiprocessing-A3C"><a href="#31-8-multiprocessing-A3C" class="headerlink" title="31.8 multiprocessing+A3C"></a>31.8 multiprocessing+A3C</h2><h1 id="32-Distributed-Proximal-Policy-Optimization-DPPO-—Tensorflow"><a href="#32-Distributed-Proximal-Policy-Optimization-DPPO-—Tensorflow" class="headerlink" title="32. Distributed Proximal Policy Optimization (DPPO) —Tensorflow"></a>32. Distributed Proximal Policy Optimization (DPPO) —Tensorflow</h1><h2 id="32-1-要点"><a href="#32-1-要点" class="headerlink" title="32.1 要点"></a>32.1 要点</h2><h2 id="32-2-OpenAI-和-DeepMind-的-Demo"><a href="#32-2-OpenAI-和-DeepMind-的-Demo" class="headerlink" title="32.2 OpenAI 和 DeepMind 的 Demo"></a>32.2 OpenAI 和 DeepMind 的 Demo</h2><h2 id="32-3-算法"><a href="#32-3-算法" class="headerlink" title="32.3 算法"></a>32.3 算法</h2><h2 id="32-4-简单的-PPO-主结构"><a href="#32-4-简单的-PPO-主结构" class="headerlink" title="32.4 简单的 PPO 主结构"></a>32.4 简单的 PPO 主结构</h2><h2 id="32-5-Distributed-PPO"><a href="#32-5-Distributed-PPO" class="headerlink" title="32.5 Distributed PPO"></a>32.5 Distributed PPO</h2>
      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 yu_mingm623@163.com </span>
    </div>
</article>


<p>
    <a  class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>莫烦-强化学习课程</p>
    <p><span class="copy-title">文章字数:</span><span class="post-count">22.1k</span></p>
    <p><span class="copy-title">本文作者:</span><a  title="郁明敏">郁明敏</a></p>
    <p><span class="copy-title">发布时间:</span>2019-12-08, 12:49:03</p>
    <p><span class="copy-title">最后更新:</span>2019-12-11, 22:45:05</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/2019/12/08/morvan-courses-rl/" title="莫烦-强化学习课程">http://www.monsteryu.top/2019/12/08/morvan-courses-rl/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>





    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js"
        value="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</input>
    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2019.11 路痴大魔王</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class="" id="rocket" ></a>

    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close"  onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div>


</body>
<script src="/js/jquery.pjax.js?v=1.0.1" ></script>

<script src="/js/script.js?v=1.0.1" ></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['@郁明敏','#AirFlow','#蓝鲸','#Logistic Regression','#Centos','#MySQL','#chrome','#Requests','#impala','#datagrip','#Flask','#hadoop','#GitHub','#hive','#Jupyter','#Python学习资源','#Linux','#LGBM','#GLIBC','#系统','#Mac','#markdown','#notepad','#pandas','#pycharm','#Python常用库','#impyla','#强化学习','#随机过程','#sublime','#windows软件','#windows','#xshell','#3-hexo','#hexo','#Python小技巧','#Python关键字','#数据挖掘','#统计基础','#Django用户登录',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: ;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 602px;
    }
    .nav.fullscreen {
        margin-left: -602px;
    }
    .nav-left {
        width: 180px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 492px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.3;
        background: url("");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
