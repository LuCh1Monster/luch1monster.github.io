<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>莫烦-强化学习课程 | 路痴大魔王</title>
  <meta name="keywords" content=" 强化学习 ">
  <meta name="description" content="莫烦-强化学习课程 | 路痴大魔王">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="页面未找到！">
<meta property="og:type" content="website">
<meta property="og:title" content="404">
<meta property="og:url" content="http:&#x2F;&#x2F;www.monsteryu.top&#x2F;404.html">
<meta property="og:site_name" content="路痴大魔王">
<meta property="og:description" content="页面未找到！">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-12-01T14:55:52.181Z">
<meta name="twitter:card" content="summary">


<link rel="icon" href="/img/avatar4.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/github.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1" ></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.0.1" ></script>

</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="false">
  <input class="theme_blog_path" value="">
</div>

<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar.jpg" />
</a>
<div class="author">
    <span>郁明敏</span>
</div>

<div class="icon">
    
        
        <a title="github" href="https://github.com/luch1monster" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-github"></use>
                </svg>
            
        </a>
        
    
        
        <a title="csdn" href="https://blog.csdn.net/LuCh1Monster" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-csdn"></use>
                </svg>
            
        </a>
        
    
        
        <a title="email" href="mailto:yu_mingm623@163.com" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-email"></use>
                </svg>
            
        </a>
        
    
        
        <a title="qq" href="http://wpa.qq.com/msgrd?v=3&uin=442523981&site=qq&menu=yes" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-qq"></use>
                </svg>
            
        </a>
        
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(36)</small></div></li>
    
        
            
            <li><div data-rel="数据库"><i class="fold iconfont icon-right"></i>数据库<small>(5)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="MySQL">MySQL<small>(2)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="蓝鲸">蓝鲸<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Hive">Hive<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Impala">Impala<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
            <li><div data-rel="数据挖掘"><i class="fold iconfont icon-right"></i>数据挖掘<small>(3)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="LR">LR<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="强化学习">强化学习<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
            <li><div data-rel="后端"><i class="fold iconfont icon-right"></i>后端<small>(11)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Python">Python<small>(10)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Hadoop">Hadoop<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
            <li><div data-rel="工具"><i class="fold iconfont icon-right"></i>工具<small>(9)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Chrome">Chrome<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Markdown">Markdown<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Notepad">Notepad<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="JetBrains">JetBrains<small>(3)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Sublime">Sublime<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="GitHub">GitHub<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="XShell">XShell<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="学习资源">学习资源<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="系统"><i class="fold iconfont icon-right"></i>系统<small>(4)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="Linux">Linux<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Mac">Mac<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="Windows">Windows<small>(2)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="文献阅读"><i class="fold iconfont icon-right"></i>文献阅读<small>(2)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="强化学习">强化学习<small>(1)</small></div>
                            
                        </li>
                            
                        <li><div data-rel="随机过程">随机过程<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
            <li><div data-rel="前端"><i class="fold iconfont icon-right"></i>前端<small>(1)</small></div>
                
                    <ul class="sub hide">
                        
                        <li><div data-rel="hexo">hexo<small>(1)</small></div>
                            
                        </li>
                            
                    </ul>
                
            </li>
            
        
    
        
            
        
    
        
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    </div>
    <div><a class="about  site_url"  href="/about">关于</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="36">
<input type="hidden" id="yelog_site_word_count" value="45.1k">
<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="#">叶落阁</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode==13){return false;}">
        <input class="search" type="text" placeholder="以 in: 开头进行全文搜索" autocomplete="off"id="local-search-input" >
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none" />
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a class="color2">Centos</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">MySQL</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Logistic Regression</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">蓝鲸</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">Requests</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">chrome</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">impala</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">datagrip</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">Flask</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">GitHub</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">hadoop</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">hive</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">Jupyter</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">Python学习资源</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">Linux</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">LGBM</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color1">GLIBC</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">系统</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">Mac</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">markdown</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">notepad</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">pycharm</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Python常用库</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">impyla</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">强化学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">sublime</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">随机过程</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">windows软件</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">windows</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">xshell</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">3-hexo</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">hexo</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Python小技巧</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Python关键字</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">数据挖掘</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">AirFlow</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        <a id="top" class="前端 hexo "
           href="/2019/11/27/3-hexo-usage/"
           data-tag="3-hexo,hexo"
           data-author="郁明敏" >
            <span class="post-title" title="3-hexo主题使用补充">3-hexo主题使用补充</span>
            <span class="post-date" title="2019-11-27 16:53:35">2019/11/27</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/09/crawl-weekday-info/"
           data-tag="Requests"
           data-author="郁明敏" >
            <span class="post-title" title="Requests获取工作日信息">Requests获取工作日信息</span>
            <span class="post-date" title="2019-12-09 16:03:30">2019/12/09</span>
        </a>
        
        <a  class="工具 Notepad "
           href="/2019/12/08/notepad-common-usages/"
           data-tag="notepad"
           data-author="郁明敏" >
            <span class="post-title" title="Notepad++用法总结">Notepad++用法总结</span>
            <span class="post-date" title="2019-12-08 20:51:47">2019/12/08</span>
        </a>
        
        <a  class="工具 JetBrains "
           href="/2019/12/08/pycharm-common-usages/"
           data-tag="pycharm"
           data-author="郁明敏" >
            <span class="post-title" title="PyCharm用法总结">PyCharm用法总结</span>
            <span class="post-date" title="2019-12-08 20:46:04">2019/12/08</span>
        </a>
        
        <a  class="工具 Sublime "
           href="/2019/12/08/sublime-common-usages/"
           data-tag="sublime"
           data-author="郁明敏" >
            <span class="post-title" title="Sublime Text总结">Sublime Text总结</span>
            <span class="post-date" title="2019-12-08 20:42:31">2019/12/08</span>
        </a>
        
        <a  class="系统 Windows "
           href="/2019/12/08/windows-common-usages/"
           data-tag="windows"
           data-author="郁明敏" >
            <span class="post-title" title="Windows总结">Windows总结</span>
            <span class="post-date" title="2019-12-08 20:31:41">2019/12/08</span>
        </a>
        
        <a  class="工具 Chrome "
           href="/2019/12/08/chrome-common-plugins/"
           data-tag="chrome"
           data-author="郁明敏" >
            <span class="post-title" title="Chrome常用插件">Chrome常用插件</span>
            <span class="post-date" title="2019-12-08 20:29:18">2019/12/08</span>
        </a>
        
        <a  class="系统 Windows "
           href="/2019/12/08/windows-common-softwares/"
           data-tag="windows软件"
           data-author="郁明敏" >
            <span class="post-title" title="Window常用软件">Window常用软件</span>
            <span class="post-date" title="2019-12-08 20:22:58">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/python-common-packages/"
           data-tag="Python常用库"
           data-author="郁明敏" >
            <span class="post-title" title="Python常用库">Python常用库</span>
            <span class="post-date" title="2019-12-08 20:04:22">2019/12/08</span>
        </a>
        
        <a  class="系统 Mac "
           href="/2019/12/08/mac-common-usages/"
           data-tag="系统,Mac"
           data-author="郁明敏" >
            <span class="post-title" title="MAC用法总结">MAC用法总结</span>
            <span class="post-date" title="2019-12-08 19:49:04">2019/12/08</span>
        </a>
        
        <a  class="数据库 MySQL "
           href="/2019/12/08/centos-migrate-mysql/"
           data-tag="Centos,MySQL"
           data-author="郁明敏" >
            <span class="post-title" title="Centos下迁移MySQL">Centos下迁移MySQL</span>
            <span class="post-date" title="2019-12-08 19:37:05">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/python-install-packages/"
           data-tag="impala,impyla"
           data-author="郁明敏" >
            <span class="post-title" title="Python安装各种库的教程">Python安装各种库的教程</span>
            <span class="post-date" title="2019-12-08 19:29:16">2019/12/08</span>
        </a>
        
        <a  class="数据库 MySQL "
           href="/2019/12/08/centos-install-mysql/"
           data-tag="Centos,MySQL"
           data-author="郁明敏" >
            <span class="post-title" title="Centos安装MySQL服务">Centos安装MySQL服务</span>
            <span class="post-date" title="2019-12-08 19:01:46">2019/12/08</span>
        </a>
        
        <a  class="系统 Linux "
           href="/2019/12/08/linux-common-usages/"
           data-tag="Linux,LGBM,GLIBC"
           data-author="郁明敏" >
            <span class="post-title" title="Linux总结">Linux总结</span>
            <span class="post-date" title="2019-12-08 18:02:08">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/jupyter-usages/"
           data-tag="Jupyter"
           data-author="郁明敏" >
            <span class="post-title" title="jupyter用法总结">jupyter用法总结</span>
            <span class="post-date" title="2019-12-08 17:39:50">2019/12/08</span>
        </a>
        
        <a  class="数据库 蓝鲸 "
           href="/2019/12/08/blue-whale-common_usages/"
           data-tag="蓝鲸"
           data-author="郁明敏" >
            <span class="post-title" title="蓝鲸使用">蓝鲸使用</span>
            <span class="post-date" title="2019-12-08 15:40:18">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/python-connect-impala-timeout/"
           data-tag="impala,impyla"
           data-author="郁明敏" >
            <span class="post-title" title="Python连接Impala超时">Python连接Impala超时</span>
            <span class="post-date" title="2019-12-08 15:34:09">2019/12/08</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/08/create-impala-table-using-python/"
           data-tag="impala"
           data-author="郁明敏" >
            <span class="post-title" title="使用Python在Impala中建表">使用Python在Impala中建表</span>
            <span class="post-date" title="2019-12-08 15:13:02">2019/12/08</span>
        </a>
        
        <a  class="工具 JetBrains "
           href="/2019/12/08/datagrip-add-customized-connections/"
           data-tag="datagrip"
           data-author="郁明敏" >
            <span class="post-title" title="DataGrip自定义连接Hive和Impala">DataGrip自定义连接Hive和Impala</span>
            <span class="post-date" title="2019-12-08 15:08:00">2019/12/08</span>
        </a>
        
        <a  class="数据库 Impala "
           href="/2019/12/08/impala-commonn-usages/"
           data-tag="impala"
           data-author="郁明敏" >
            <span class="post-title" title="Impala常规使用">Impala常规使用</span>
            <span class="post-date" title="2019-12-08 14:56:27">2019/12/08</span>
        </a>
        
        <a  class="数据库 Hive "
           href="/2019/12/08/hive-qas/"
           data-tag="hive"
           data-author="郁明敏" >
            <span class="post-title" title="Hive总结">Hive总结</span>
            <span class="post-date" title="2019-12-08 14:48:43">2019/12/08</span>
        </a>
        
        <a  class="后端 Hadoop "
           href="/2019/12/08/hadoop-qas/"
           data-tag="hadoop"
           data-author="郁明敏" >
            <span class="post-title" title="Hadoop总结">Hadoop总结</span>
            <span class="post-date" title="2019-12-08 14:27:24">2019/12/08</span>
        </a>
        
        <a  class="工具 Markdown "
           href="/2019/12/08/markdown-common-usages/"
           data-tag="markdown"
           data-author="郁明敏" >
            <span class="post-title" title="Markdown用法总结">Markdown用法总结</span>
            <span class="post-date" title="2019-12-08 13:40:22">2019/12/08</span>
        </a>
        
        <a  class="数据挖掘 强化学习 "
           href="/2019/12/08/morvan-courses-rl/"
           data-tag="强化学习"
           data-author="郁明敏" >
            <span class="post-title" title="莫烦-强化学习课程">莫烦-强化学习课程</span>
            <span class="post-date" title="2019-12-08 12:49:03">2019/12/08</span>
        </a>
        
        <a  class="工具 GitHub "
           href="/2019/12/08/github-search-project-skills/"
           data-tag="GitHub"
           data-author="郁明敏" >
            <span class="post-title" title="GitHub搜索开源项目技巧">GitHub搜索开源项目技巧</span>
            <span class="post-date" title="2019-12-08 11:10:15">2019/12/08</span>
        </a>
        
        <a  class="工具 JetBrains "
           href="/2019/12/08/pycharm-latest-activate-method/"
           data-tag="pycharm"
           data-author="郁明敏" >
            <span class="post-title" title="PyCharm2019.2最新激活方式">PyCharm2019.2最新激活方式</span>
            <span class="post-date" title="2019-12-08 10:27:59">2019/12/08</span>
        </a>
        
        <a  class="工具 XShell "
           href="/2019/12/07/xshell-connect-docker-server/"
           data-tag="xshell"
           data-author="郁明敏" >
            <span class="post-title" title="XShell连接Docker服务器下Centos终端">XShell连接Docker服务器下Centos终端</span>
            <span class="post-date" title="2019-12-07 23:40:55">2019/12/07</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/07/install-airflow/"
           data-tag="AirFlow"
           data-author="郁明敏" >
            <span class="post-title" title="AirFlow安装">AirFlow安装</span>
            <span class="post-date" title="2019-12-07 13:52:21">2019/12/07</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/07/flask-QA/"
           data-tag="Flask"
           data-author="郁明敏" >
            <span class="post-title" title="Flask总结">Flask总结</span>
            <span class="post-date" title="2019-12-07 11:58:54">2019/12/07</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/07/unfamiliar-key-words-in-python/"
           data-tag="Python关键字"
           data-author="郁明敏" >
            <span class="post-title" title="冷僻的Python内置关键字">冷僻的Python内置关键字</span>
            <span class="post-date" title="2019-12-07 02:15:12">2019/12/07</span>
        </a>
        
        <a  class="学习资源 "
           href="/2019/12/06/learning-resources-python/"
           data-tag="Python学习资源"
           data-author="郁明敏" >
            <span class="post-title" title="Python学习资源">Python学习资源</span>
            <span class="post-date" title="2019-12-06 22:52:18">2019/12/06</span>
        </a>
        
        <a  class="文献阅读 强化学习 "
           href="/2019/12/06/reinforcement-learning/"
           data-tag="强化学习"
           data-author="郁明敏" >
            <span class="post-title" title="文献-强化学习">文献-强化学习</span>
            <span class="post-date" title="2019-12-06 17:23:52">2019/12/06</span>
        </a>
        
        <a  class="后端 Python "
           href="/2019/12/04/python-common-tips/"
           data-tag="Python小技巧"
           data-author="郁明敏" >
            <span class="post-title" title="Python常用小技巧">Python常用小技巧</span>
            <span class="post-date" title="2019-12-04 17:44:20">2019/12/04</span>
        </a>
        
        <a  class="数据挖掘 "
           href="/2019/12/03/data-mining-common-steps-routines/"
           data-tag="数据挖掘"
           data-author="郁明敏" >
            <span class="post-title" title="数据挖掘下的一般步骤与套路">数据挖掘下的一般步骤与套路</span>
            <span class="post-date" title="2019-12-03 14:59:37">2019/12/03</span>
        </a>
        
        <a  class="文献阅读 随机过程 "
           href="/2019/12/01/stochastic-process/"
           data-tag="随机过程"
           data-author="郁明敏" >
            <span class="post-title" title="文献-随机过程">文献-随机过程</span>
            <span class="post-date" title="2019-12-01 23:45:46">2019/12/01</span>
        </a>
        
        <a  class="数据挖掘 LR "
           href="/2019/12/01/build-LR-model-steps/"
           data-tag="Logistic Regression"
           data-author="郁明敏" >
            <span class="post-title" title="LR建模总结">LR建模总结</span>
            <span class="post-date" title="2019-12-01 22:59:25">2019/12/01</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first">
                <i class="fa fa-arrow-circle-left" aria-hidden="true"></i>
            </div>
            <div class="brackets">
                <i class="fa fa-arrow-circle-right" aria-hidden="true"></i>
            </div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-morvan-courses-rl" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">莫烦-强化学习课程</h1>
    
    <div class="article-meta">
        
        
        <span class="author"><a>郁明敏</a></span>
        
        
        <span class="book">
            
                <a  data-rel="数据挖掘">数据挖掘</a>/
            
                <a  data-rel="强化学习">强化学习</a>
            
        </span>
        
        
        <span class="tag">
            
            <a class="color5">强化学习</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title='更新时间: 2019-12-10 23:59:19'>2019-12-08 12:49</time>
        
    </div>
    <div class="article-meta">
        
        <span>字数:13.1k</span>
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-什么是强化学习-Reinforcement-Learning"><span class="toc-text">1. 什么是强化学习(Reinforcement Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-从无到有"><span class="toc-text">1.1 从无到有</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-虚拟老师"><span class="toc-text">1.2 虚拟老师</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-对比监督学习"><span class="toc-text">1.3 对比监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-RL算法"><span class="toc-text">1.4 RL算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-强化学习汇总"><span class="toc-text">2. 强化学习汇总</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Model-free和Model-based"><span class="toc-text">2.1 Model-free和Model-based</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-基于概率和基于价值"><span class="toc-text">2.2 基于概率和基于价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-回合更新和单步更新"><span class="toc-text">2.3 回合更新和单步更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-在线学习和离线学习"><span class="toc-text">2.4 在线学习和离线学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-为什么用强化学习"><span class="toc-text">3. 为什么用强化学习?</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-强化学习介绍"><span class="toc-text">3.1 强化学习介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-模拟程序提前看"><span class="toc-text">3.2 模拟程序提前看</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-教程必备模块"><span class="toc-text">4.1 教程必备模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-快速了解强化学习"><span class="toc-text">4.2 快速了解强化学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-什么是Q-Learning"><span class="toc-text">5. 什么是Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-行为准则"><span class="toc-text">5.1 行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-Q-Learning决策"><span class="toc-text">5.2 Q-Learning决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-Q-Learning更新"><span class="toc-text">5.3 Q-Learning更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-Q-Learning整体算法"><span class="toc-text">5.4 Q-Learning整体算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-Q-Learning中的Gamma"><span class="toc-text">5.5 Q-Learning中的Gamma</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-强化学习小例子"><span class="toc-text">6. 强化学习小例子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-要点"><span class="toc-text">6.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-预设值"><span class="toc-text">6.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-Q表"><span class="toc-text">6.3 Q表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-定义动作"><span class="toc-text">6.4 定义动作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-环境反馈-S-，R"><span class="toc-text">6.5 环境反馈 S_，R</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-环境更新"><span class="toc-text">6.6 环境更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-7-强化学习主循环"><span class="toc-text">6.7 强化学习主循环</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-8-Q-Table的演变"><span class="toc-text">6.8 Q-Table的演变</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Q-Learning算法更新"><span class="toc-text">7. Q-Learning算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-要点"><span class="toc-text">7.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-算法"><span class="toc-text">7.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-算法的代码形式"><span class="toc-text">7.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-Q-Learning思维决策"><span class="toc-text">8. Q-Learning思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-代码主结构"><span class="toc-text">8.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-预设值"><span class="toc-text">8.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-决定行为"><span class="toc-text">8.3 决定行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-学习"><span class="toc-text">8.4 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-检测-state-是否存在"><span class="toc-text">8.5 检测 state 是否存在</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-什么是Sarsa"><span class="toc-text">9. 什么是Sarsa</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-Sarsa决策"><span class="toc-text">9.1 Sarsa决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-Sarsa更新行为准则"><span class="toc-text">9.2 Sarsa更新行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-对比Sarsa和Q-Learning算法"><span class="toc-text">9.3 对比Sarsa和Q-Learning算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-Sarsa算法更新"><span class="toc-text">10. Sarsa算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-要点"><span class="toc-text">10.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-算法"><span class="toc-text">10.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-算法的代码形式"><span class="toc-text">10.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-Sarsa思维决策"><span class="toc-text">11. Sarsa思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-1-代码主结构"><span class="toc-text">11.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-2-学习"><span class="toc-text">11.2 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-什么是Sarsa-lambda"><span class="toc-text">12. 什么是Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-Sarsa-n"><span class="toc-text">12.1 Sarsa(n)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-2-单步更新和回合更新"><span class="toc-text">12.2 单步更新和回合更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-有时迷茫"><span class="toc-text">12.3 有时迷茫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-4-Lambda含义"><span class="toc-text">12.4 Lambda含义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-5-Lambda取值"><span class="toc-text">12.5 Lambda取值</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-Sarsa-lambda"><span class="toc-text">13. Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#13-1-要点"><span class="toc-text">13.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-2-代码主结构"><span class="toc-text">13.2 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-3-预设值"><span class="toc-text">13.3 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-4-检测state是否存在"><span class="toc-text">13.4 检测state是否存在</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-5-学习"><span class="toc-text">13.5 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14-什么是DQN"><span class="toc-text">14. 什么是DQN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#14-1-强化学习与神经网络"><span class="toc-text">14.1 强化学习与神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-2-神经网络的作用"><span class="toc-text">14.2 神经网络的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-3-更新神经网络"><span class="toc-text">14.3 更新神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-4-DQN两大利器"><span class="toc-text">14.4 DQN两大利器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-DQN算法更新—TensorFlow"><span class="toc-text">15. DQN算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#15-1-要点"><span class="toc-text">15.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-2-算法"><span class="toc-text">15.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-3-算法的代码形式"><span class="toc-text">15.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#16-DQN算法更新—TensorFlow"><span class="toc-text">16. DQN算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#16-1-要点"><span class="toc-text">16.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-2-算法"><span class="toc-text">16.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-3-算法的代码形式"><span class="toc-text">16.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#17-DQN神经网络—TensorFlow"><span class="toc-text">17. DQN神经网络—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#17-1-要点"><span class="toc-text">17.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-2-两个神经网络"><span class="toc-text">17.2 两个神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-3-神经网络结构"><span class="toc-text">17.3 神经网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-4-常见两个网络"><span class="toc-text">17.4 常见两个网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#18-DQN思维决策—TensorFlow"><span class="toc-text">18. DQN思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#18-1-代码主结构"><span class="toc-text">18.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-2-初始值"><span class="toc-text">18.2 初始值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-3-存储记忆"><span class="toc-text">18.3 存储记忆</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-4-选行为"><span class="toc-text">18.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-5-学习"><span class="toc-text">18.5 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-6-学习效果"><span class="toc-text">18.6 学习效果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-7-修改版的-DQN"><span class="toc-text">18.7 修改版的 DQN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#19-OpenAI-gym环境库"><span class="toc-text">19. OpenAI gym环境库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#19-1-要点"><span class="toc-text">19.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-2-安装gym"><span class="toc-text">19.2 安装gym</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-3-CartPole例子"><span class="toc-text">19.3 CartPole例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-4-MountainCar例子"><span class="toc-text">19.4 MountainCar例子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#20-Double-DQN—TensorFlow"><span class="toc-text">20. Double DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#20-1-要点"><span class="toc-text">20.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-2-Double-DQN算法"><span class="toc-text">20.2 Double DQN算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-3-更新方法"><span class="toc-text">20.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-4-记录-Q-值"><span class="toc-text">20.4 记录 Q 值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-5-对比结果"><span class="toc-text">20.5 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#21-Prioritized-Experience-Replay-DQN-—TensorFlow"><span class="toc-text">21. Prioritized Experience Replay(DQN)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#21-1-要点"><span class="toc-text">21.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-2-Prioritized-Replay算法"><span class="toc-text">21.2 Prioritized Replay算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-3-SumTree有效抽样"><span class="toc-text">21.3 SumTree有效抽样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-4-Memory类"><span class="toc-text">21.4 Memory类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-5-更新方法"><span class="toc-text">21.5 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-6-对比结果"><span class="toc-text">21.6 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#22-Dueling-DQN—TensorFlow"><span class="toc-text">22. Dueling DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#22-1-要点"><span class="toc-text">22.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-2-Dueling算法"><span class="toc-text">22.2 Dueling算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-3-更新方法"><span class="toc-text">22.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-4-对比结果"><span class="toc-text">22.4 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#23-什么是Policy-Gradients"><span class="toc-text">23. 什么是Policy Gradients</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#23-1-和以往的强化学习方法不同"><span class="toc-text">23.1 和以往的强化学习方法不同</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-2-更新不同之处"><span class="toc-text">23.2 更新不同之处</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-3-具体更新步骤"><span class="toc-text">23.3 具体更新步骤</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#24-Policy-Gradients算法更新—TensorFlow"><span class="toc-text">24. Policy Gradients算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#24-1-要点"><span class="toc-text">24.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-2-算法"><span class="toc-text">24.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-3-算法代码形式"><span class="toc-text">24.3 算法代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#25-Policy-Gradients思维决策—TensorFlow"><span class="toc-text">25. Policy Gradients思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#25-1-主要代码结构"><span class="toc-text">25.1 主要代码结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-2-初始化"><span class="toc-text">25.2 初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-3-建立Policy神经网络"><span class="toc-text">25.3 建立Policy神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-4-选行为"><span class="toc-text">25.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-5-存储回合"><span class="toc-text">25.5 存储回合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-6-学习"><span class="toc-text">25.6 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#26-什么是Actor-Critic"><span class="toc-text">26. 什么是Actor Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#26-1-为什么要有Actor和Critic"><span class="toc-text">26.1 为什么要有Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-2-Actor和Critic"><span class="toc-text">26.2 Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-3-增加单步更新属性"><span class="toc-text">26.3 增加单步更新属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-4-改进版Deep-Deterministic-Policy-Gradient-DDPG"><span class="toc-text">26.4 改进版Deep Deterministic Policy Gradient(DDPG)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#27-Actot-Critic—TensorFlow"><span class="toc-text">27. Actot Critic—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#27-1-要点"><span class="toc-text">27.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-2-算法"><span class="toc-text">27.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-3-代码主结构"><span class="toc-text">27.3 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-4-两者学习方式"><span class="toc-text">27.4 两者学习方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-5-每回合算法"><span class="toc-text">27.5 每回合算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#28-什么是Deep-Deterministic-Policy-Gradient-DDPG"><span class="toc-text">28. 什么是Deep Deterministic Policy Gradient(DDPG)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#28-1-拆分细讲"><span class="toc-text">28.1 拆分细讲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-2-Deep和DQN"><span class="toc-text">28.2 Deep和DQN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-3-Deterministic-Policy-Gradient"><span class="toc-text">28.3 Deterministic Policy Gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-4-DDPG神经网络"><span class="toc-text">28.4 DDPG神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#29-Deep-Deterministic-Policy-Gradient-DDPG-—TensorFlow"><span class="toc-text">29. Deep Deterministic Policy Gradient(DDPG)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#29-1-要点"><span class="toc-text">29.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-2-算法"><span class="toc-text">29.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-3-主结构"><span class="toc-text">29.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-4-主结构"><span class="toc-text">29.4 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-5-Actor-Critic"><span class="toc-text">29.5 Actor Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-6-记忆库Mmeory"><span class="toc-text">29.6 记忆库Mmeory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-7-每回合算法"><span class="toc-text">29.7 每回合算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-8-简化版代码"><span class="toc-text">29.8 简化版代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#30-什么是Asynchronous-Advantage-Actor-Critic-A3C"><span class="toc-text">30. 什么是Asynchronous Advantage Actor-Critic (A3C)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#30-1-平行宇宙"><span class="toc-text">30.1 平行宇宙</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-2-平行训练"><span class="toc-text">30.2 平行训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-3-多核训练"><span class="toc-text">30.3 多核训练</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#31-Asynchronous-Advantage-Actor-Critic-A3C-—TensorFlow"><span class="toc-text">31. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#31-1-要点"><span class="toc-text">31.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-2-算法"><span class="toc-text">31.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-3-主结构"><span class="toc-text">31.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-4-Actor-Critic网络"><span class="toc-text">31.4 Actor Critic网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-5-Worker"><span class="toc-text">31.5 Worker</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-6-Worker并行工作"><span class="toc-text">31.6 Worker并行工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-7-机械手臂"><span class="toc-text">31.7 机械手臂</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-8-multiprocessing-A3C"><span class="toc-text">31.8 multiprocessing+A3C</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#32-Distributed-Proximal-Policy-Optimization-DPPO-—Tensorflow"><span class="toc-text">32. Distributed Proximal Policy Optimization (DPPO) —Tensorflow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#32-1-要点"><span class="toc-text">32.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-2-OpenAI-和-DeepMind-的-Demo"><span class="toc-text">32.2 OpenAI 和 DeepMind 的 Demo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-3-算法"><span class="toc-text">32.3 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-4-简单的-PPO-主结构"><span class="toc-text">32.4 简单的 PPO 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-5-Distributed-PPO"><span class="toc-text">32.5 Distributed PPO</span></a></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><div class='inner-toc'><h2>目录</h2><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-什么是强化学习-Reinforcement-Learning"><span class="toc-text">1. 什么是强化学习(Reinforcement Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-从无到有"><span class="toc-text">1.1 从无到有</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-虚拟老师"><span class="toc-text">1.2 虚拟老师</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-对比监督学习"><span class="toc-text">1.3 对比监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-RL算法"><span class="toc-text">1.4 RL算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-强化学习汇总"><span class="toc-text">2. 强化学习汇总</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Model-free和Model-based"><span class="toc-text">2.1 Model-free和Model-based</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-基于概率和基于价值"><span class="toc-text">2.2 基于概率和基于价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-回合更新和单步更新"><span class="toc-text">2.3 回合更新和单步更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-在线学习和离线学习"><span class="toc-text">2.4 在线学习和离线学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-为什么用强化学习"><span class="toc-text">3. 为什么用强化学习?</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-强化学习介绍"><span class="toc-text">3.1 强化学习介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-模拟程序提前看"><span class="toc-text">3.2 模拟程序提前看</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-教程必备模块"><span class="toc-text">4.1 教程必备模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-快速了解强化学习"><span class="toc-text">4.2 快速了解强化学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-什么是Q-Learning"><span class="toc-text">5. 什么是Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-行为准则"><span class="toc-text">5.1 行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-Q-Learning决策"><span class="toc-text">5.2 Q-Learning决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-Q-Learning更新"><span class="toc-text">5.3 Q-Learning更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-Q-Learning整体算法"><span class="toc-text">5.4 Q-Learning整体算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-Q-Learning中的Gamma"><span class="toc-text">5.5 Q-Learning中的Gamma</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-强化学习小例子"><span class="toc-text">6. 强化学习小例子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-要点"><span class="toc-text">6.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-预设值"><span class="toc-text">6.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-Q表"><span class="toc-text">6.3 Q表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-定义动作"><span class="toc-text">6.4 定义动作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-环境反馈-S-，R"><span class="toc-text">6.5 环境反馈 S_，R</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-环境更新"><span class="toc-text">6.6 环境更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-7-强化学习主循环"><span class="toc-text">6.7 强化学习主循环</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-8-Q-Table的演变"><span class="toc-text">6.8 Q-Table的演变</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Q-Learning算法更新"><span class="toc-text">7. Q-Learning算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-要点"><span class="toc-text">7.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-算法"><span class="toc-text">7.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-算法的代码形式"><span class="toc-text">7.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-Q-Learning思维决策"><span class="toc-text">8. Q-Learning思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-代码主结构"><span class="toc-text">8.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-预设值"><span class="toc-text">8.2 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-决定行为"><span class="toc-text">8.3 决定行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-学习"><span class="toc-text">8.4 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-检测-state-是否存在"><span class="toc-text">8.5 检测 state 是否存在</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-什么是Sarsa"><span class="toc-text">9. 什么是Sarsa</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-Sarsa决策"><span class="toc-text">9.1 Sarsa决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-Sarsa更新行为准则"><span class="toc-text">9.2 Sarsa更新行为准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-对比Sarsa和Q-Learning算法"><span class="toc-text">9.3 对比Sarsa和Q-Learning算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-Sarsa算法更新"><span class="toc-text">10. Sarsa算法更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-要点"><span class="toc-text">10.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-算法"><span class="toc-text">10.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-3-算法的代码形式"><span class="toc-text">10.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-Sarsa思维决策"><span class="toc-text">11. Sarsa思维决策</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-1-代码主结构"><span class="toc-text">11.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-2-学习"><span class="toc-text">11.2 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-什么是Sarsa-lambda"><span class="toc-text">12. 什么是Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-Sarsa-n"><span class="toc-text">12.1 Sarsa(n)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-2-单步更新和回合更新"><span class="toc-text">12.2 单步更新和回合更新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-有时迷茫"><span class="toc-text">12.3 有时迷茫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-4-Lambda含义"><span class="toc-text">12.4 Lambda含义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-5-Lambda取值"><span class="toc-text">12.5 Lambda取值</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-Sarsa-lambda"><span class="toc-text">13. Sarsa-lambda</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#13-1-要点"><span class="toc-text">13.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-2-代码主结构"><span class="toc-text">13.2 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-3-预设值"><span class="toc-text">13.3 预设值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-4-检测state是否存在"><span class="toc-text">13.4 检测state是否存在</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-5-学习"><span class="toc-text">13.5 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14-什么是DQN"><span class="toc-text">14. 什么是DQN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#14-1-强化学习与神经网络"><span class="toc-text">14.1 强化学习与神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-2-神经网络的作用"><span class="toc-text">14.2 神经网络的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-3-更新神经网络"><span class="toc-text">14.3 更新神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-4-DQN两大利器"><span class="toc-text">14.4 DQN两大利器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-DQN算法更新—TensorFlow"><span class="toc-text">15. DQN算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#15-1-要点"><span class="toc-text">15.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-2-算法"><span class="toc-text">15.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-3-算法的代码形式"><span class="toc-text">15.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#16-DQN算法更新—TensorFlow"><span class="toc-text">16. DQN算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#16-1-要点"><span class="toc-text">16.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-2-算法"><span class="toc-text">16.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-3-算法的代码形式"><span class="toc-text">16.3 算法的代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#17-DQN神经网络—TensorFlow"><span class="toc-text">17. DQN神经网络—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#17-1-要点"><span class="toc-text">17.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-2-两个神经网络"><span class="toc-text">17.2 两个神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-3-神经网络结构"><span class="toc-text">17.3 神经网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-4-常见两个网络"><span class="toc-text">17.4 常见两个网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#18-DQN思维决策—TensorFlow"><span class="toc-text">18. DQN思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#18-1-代码主结构"><span class="toc-text">18.1 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-2-初始值"><span class="toc-text">18.2 初始值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-3-存储记忆"><span class="toc-text">18.3 存储记忆</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-4-选行为"><span class="toc-text">18.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-5-学习"><span class="toc-text">18.5 学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-6-学习效果"><span class="toc-text">18.6 学习效果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-7-修改版的-DQN"><span class="toc-text">18.7 修改版的 DQN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#19-OpenAI-gym环境库"><span class="toc-text">19. OpenAI gym环境库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#19-1-要点"><span class="toc-text">19.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-2-安装gym"><span class="toc-text">19.2 安装gym</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-3-CartPole例子"><span class="toc-text">19.3 CartPole例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-4-MountainCar例子"><span class="toc-text">19.4 MountainCar例子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#20-Double-DQN—TensorFlow"><span class="toc-text">20. Double DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#20-1-要点"><span class="toc-text">20.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-2-Double-DQN算法"><span class="toc-text">20.2 Double DQN算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-3-更新方法"><span class="toc-text">20.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-4-记录-Q-值"><span class="toc-text">20.4 记录 Q 值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-5-对比结果"><span class="toc-text">20.5 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#21-Prioritized-Experience-Replay-DQN-—TensorFlow"><span class="toc-text">21. Prioritized Experience Replay(DQN)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#21-1-要点"><span class="toc-text">21.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-2-Prioritized-Replay算法"><span class="toc-text">21.2 Prioritized Replay算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-3-SumTree有效抽样"><span class="toc-text">21.3 SumTree有效抽样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-4-Memory类"><span class="toc-text">21.4 Memory类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-5-更新方法"><span class="toc-text">21.5 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-6-对比结果"><span class="toc-text">21.6 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#22-Dueling-DQN—TensorFlow"><span class="toc-text">22. Dueling DQN—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#22-1-要点"><span class="toc-text">22.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-2-Dueling算法"><span class="toc-text">22.2 Dueling算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-3-更新方法"><span class="toc-text">22.3 更新方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-4-对比结果"><span class="toc-text">22.4 对比结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#23-什么是Policy-Gradients"><span class="toc-text">23. 什么是Policy Gradients</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#23-1-和以往的强化学习方法不同"><span class="toc-text">23.1 和以往的强化学习方法不同</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-2-更新不同之处"><span class="toc-text">23.2 更新不同之处</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-3-具体更新步骤"><span class="toc-text">23.3 具体更新步骤</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#24-Policy-Gradients算法更新—TensorFlow"><span class="toc-text">24. Policy Gradients算法更新—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#24-1-要点"><span class="toc-text">24.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-2-算法"><span class="toc-text">24.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-3-算法代码形式"><span class="toc-text">24.3 算法代码形式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#25-Policy-Gradients思维决策—TensorFlow"><span class="toc-text">25. Policy Gradients思维决策—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#25-1-主要代码结构"><span class="toc-text">25.1 主要代码结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-2-初始化"><span class="toc-text">25.2 初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-3-建立Policy神经网络"><span class="toc-text">25.3 建立Policy神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-4-选行为"><span class="toc-text">25.4 选行为</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-5-存储回合"><span class="toc-text">25.5 存储回合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-6-学习"><span class="toc-text">25.6 学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#26-什么是Actor-Critic"><span class="toc-text">26. 什么是Actor Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#26-1-为什么要有Actor和Critic"><span class="toc-text">26.1 为什么要有Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-2-Actor和Critic"><span class="toc-text">26.2 Actor和Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-3-增加单步更新属性"><span class="toc-text">26.3 增加单步更新属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-4-改进版Deep-Deterministic-Policy-Gradient-DDPG"><span class="toc-text">26.4 改进版Deep Deterministic Policy Gradient(DDPG)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#27-Actot-Critic—TensorFlow"><span class="toc-text">27. Actot Critic—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#27-1-要点"><span class="toc-text">27.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-2-算法"><span class="toc-text">27.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-3-代码主结构"><span class="toc-text">27.3 代码主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-4-两者学习方式"><span class="toc-text">27.4 两者学习方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-5-每回合算法"><span class="toc-text">27.5 每回合算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#28-什么是Deep-Deterministic-Policy-Gradient-DDPG"><span class="toc-text">28. 什么是Deep Deterministic Policy Gradient(DDPG)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#28-1-拆分细讲"><span class="toc-text">28.1 拆分细讲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-2-Deep和DQN"><span class="toc-text">28.2 Deep和DQN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-3-Deterministic-Policy-Gradient"><span class="toc-text">28.3 Deterministic Policy Gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-4-DDPG神经网络"><span class="toc-text">28.4 DDPG神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#29-Deep-Deterministic-Policy-Gradient-DDPG-—TensorFlow"><span class="toc-text">29. Deep Deterministic Policy Gradient(DDPG)—TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#29-1-要点"><span class="toc-text">29.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-2-算法"><span class="toc-text">29.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-3-主结构"><span class="toc-text">29.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-4-主结构"><span class="toc-text">29.4 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-5-Actor-Critic"><span class="toc-text">29.5 Actor Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-6-记忆库Mmeory"><span class="toc-text">29.6 记忆库Mmeory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-7-每回合算法"><span class="toc-text">29.7 每回合算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-8-简化版代码"><span class="toc-text">29.8 简化版代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#30-什么是Asynchronous-Advantage-Actor-Critic-A3C"><span class="toc-text">30. 什么是Asynchronous Advantage Actor-Critic (A3C)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#30-1-平行宇宙"><span class="toc-text">30.1 平行宇宙</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-2-平行训练"><span class="toc-text">30.2 平行训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-3-多核训练"><span class="toc-text">30.3 多核训练</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#31-Asynchronous-Advantage-Actor-Critic-A3C-—TensorFlow"><span class="toc-text">31. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#31-1-要点"><span class="toc-text">31.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-2-算法"><span class="toc-text">31.2 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-3-主结构"><span class="toc-text">31.3 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-4-Actor-Critic网络"><span class="toc-text">31.4 Actor Critic网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-5-Worker"><span class="toc-text">31.5 Worker</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-6-Worker并行工作"><span class="toc-text">31.6 Worker并行工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-7-机械手臂"><span class="toc-text">31.7 机械手臂</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-8-multiprocessing-A3C"><span class="toc-text">31.8 multiprocessing+A3C</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#32-Distributed-Proximal-Policy-Optimization-DPPO-—Tensorflow"><span class="toc-text">32. Distributed Proximal Policy Optimization (DPPO) —Tensorflow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#32-1-要点"><span class="toc-text">32.1 要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-2-OpenAI-和-DeepMind-的-Demo"><span class="toc-text">32.2 OpenAI 和 DeepMind 的 Demo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-3-算法"><span class="toc-text">32.3 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-4-简单的-PPO-主结构"><span class="toc-text">32.4 简单的 PPO 主结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-5-Distributed-PPO"><span class="toc-text">32.5 Distributed PPO</span></a></li></ol></li></ol></div></p>
<p>本博客是学习了莫烦强化学习课程的总结，部分内容转载自莫烦的个人博客，在他的个人主页上 <a href="https://morvanzhou.github.io" target="_blank" rel="noopener">https://morvanzhou.github.io</a> 上有很多关于机器学习的相关课程，且配有视频和文字。莫烦是一位我非常敬佩的博主，能够使用最浅显易懂语言让你了解各种模型的原理，有兴趣的作者可以自己看学习一下其他的课程。</p>
<p>所有代码在莫烦的 GitHub 中可以找到，这里放一下地址: <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents" target="_blank" rel="noopener">https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents</a> </p>
<h1 id="1-什么是强化学习-Reinforcement-Learning"><a href="#1-什么是强化学习-Reinforcement-Learning" class="headerlink" title="1. 什么是强化学习(Reinforcement Learning)"></a>1. 什么是强化学习(Reinforcement Learning)</h1><p>强化学习是机器学习大家族中的一大类，使用强化学习能够让机器学着如何在环境中拿到高分，表现出优秀的成绩。而这些成绩背后却是他所付出的辛苦劳动，不断的试错，不断地尝试，累积经验，学习经验。</p>
<h2 id="1-1-从无到有"><a href="#1-1-从无到有" class="headerlink" title="1.1 从无到有"></a>1.1 从无到有</h2><p><img src="https://img-blog.csdnimg.cn/20191210204551928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>强化学习是一类算法，是让计算机实现从一开始什么都不懂，脑袋里没有一点想法，通过不断地尝试，从错误中学习，最后找到规律，学会了达到目的的方法。这就是一个完整的强化学习过程，实际中的强化学习例子有很多。比如近期最有名的 AlphaGo，机器头一次在围棋场上战胜人类高手，让计算机自己学着玩经典游戏 Atari，这些都是让计算机在不断的尝试中更新自己的行为准则，从而一步步学会如何下好围棋，如何操控游戏得到高分。既然要让计算机自己学，那计算机通过什么来学习呢？</p>
<h2 id="1-2-虚拟老师"><a href="#1-2-虚拟老师" class="headerlink" title="1.2 虚拟老师"></a>1.2 虚拟老师</h2><p><img src="https://img-blog.csdnimg.cn/20191210204737184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>原来计算机也需要一位虚拟的老师，这个老师比较吝啬，他不会告诉你如何移动，如何做决定，他为你做的事只有给你的行为打分，那我们应该以什么形式学习这些现有的资源，或者说怎么样只从分数中学习到我应该怎样做决定呢？很简单，我只需要记住那些高分，低分对应的行为，下次用同样的行为拿高分，并避免低分的行为。</p>
<p>比如老师会根据我的开心程度来打分，我开心时，可以得到高分，我不开心时得到低分。有了这些被打分的经验，我就能判断为了拿到高分，我应该选择一张开心的脸，避免选到伤心的脸，这也是强化学习的核心思想。可以看出在强化学习中，一种行为的分数是十分重要的。所以强化学习具有分数导向性。我们换一个角度来思考，这种分数导向性好比我们在监督学习中的正确标签。</p>
<h2 id="1-3-对比监督学习"><a href="#1-3-对比监督学习" class="headerlink" title="1.3 对比监督学习"></a>1.3 对比监督学习</h2><p><img src="https://img-blog.csdnimg.cn/20191210204924115.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们知道监督学习，是已经有了数据和数据对应的正确标签，比如这样。监督学习就能学习出那些脸对应哪种标签。不过强化学习还要更进一步，一开始它并没有数据和标签。</p>
<p>他要通过一次次在环境中的尝试，获取这些数据和标签，然后再学习通过哪些数据能够对应哪些标签，通过学习到的这些规律，竟可能地选择带来高分的行为 (比如这里的开心脸)。这也就证明了在强化学习中，分数标签就是他的老师，他和监督学习中的老师也差不多。</p>
<h2 id="1-4-RL算法"><a href="#1-4-RL算法" class="headerlink" title="1.4 RL算法"></a>1.4 RL算法</h2><p><img src="https://img-blog.csdnimg.cn/20191210205048606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>强化学习是一个大家族，它包含了很多种算法，我们也会——提到之中一些比较有名的算法，比如有通过行为的价值来选取特定行为的方法，包括使用表格学习的 Q-Learnin、Sarsa，使用神经网络学习的 Deep Q Network，还有直接输出行为的 Policy Gradients，又或者了解所处的环境，想象出一个虚拟的环境并从虚拟的环境中学习等等。</p>
<h1 id="2-强化学习汇总"><a href="#2-强化学习汇总" class="headerlink" title="2. 强化学习汇总"></a>2. 强化学习汇总</h1><p>了解强化学习中常用到的几种方法，以及他们的区别，对我们根据特定问题选择方法时很有帮助。强化学习是一个大家族，发展历史也不短，具有很多种不同方法。比如说比较知名的控制方法 Q-Learning、Policy Gradients，还有基于对环境的理解的 Model-based RL 等等。接下来我们通过分类的方式来了解他们的区别。</p>
<h2 id="2-1-Model-free和Model-based"><a href="#2-1-Model-free和Model-based" class="headerlink" title="2.1 Model-free和Model-based"></a>2.1 Model-free和Model-based</h2><p><img src="https://img-blog.csdnimg.cn/20191210205404948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们可以将所有强化学习的方法分为理不理解所处环境，如果我们不尝试去理解环境，环境给了我们什么就是什么。我们就把这种方法叫做 Model-free，这里的 Model 就是用模型来表示环境，那理解了环境也就是学会了用一个模型来代表环境，所以这种就是 Model-based 方法。我们想象，现在环境就是我们的世界，我们的机器人正在这个世界里玩耍，他不理解这个世界是怎样构成的，也不理解世界对于他的行为会怎么样反馈。举个例子，他决定丢颗原子弹去真实的世界，结果把自己给炸死了，所有结果都是那么现实。不过如果采取的是 Model-based RL，机器人会通过过往的经验，先理解真实世界是怎样的，并建立一个模型来模拟现实世界的反馈，最后他不仅可以在现实世界中玩耍，也能在模拟的世界中玩耍，这样就没必要去炸真实世界，连自己也炸死了，他可以像玩游戏一样炸炸游戏里的世界，也保住了自己的小命。那我们就来说说这两种方式的强化学习各用那些方法吧。</p>
<p>Model-free 的方法有很多, 像 Q-Learning, Sarsa、Policy Gradients 都是从环境中得到反馈然后从中学习。而 Model-based RL 只是多了一道程序，为真实世界建模，也可以说他们都是 Model-free 的强化学习，只是 Model-based 多出了一个虚拟环境，我们不仅可以像 Model-free 那样在现实中玩耍，还能在游戏中玩耍，而玩耍的方式也都是 Model-free 中那些玩耍方式，最终 Model-based 还有一个杀手锏是 Model-free 超级羡慕的，那就是想象力。</p>
<p>Model-free 中，机器人只能按部就班，一步一步等待真实世界的反馈，再根据反馈采取下一步行动。而 Model-based 能通过想象来预判断接下来将要发生的所有情，然后选择这些想象情况中最好的那种，并依据这种情况来采取下一步的策略，这也就是围棋场上 AlphaGo 能够超越人类的原因。接下来，我们再来用另外一种分类方法将强化学习分为基于概率和基于价值。</p>
<h2 id="2-2-基于概率和基于价值"><a href="#2-2-基于概率和基于价值" class="headerlink" title="2.2 基于概率和基于价值"></a>2.2 基于概率和基于价值</h2><p><img src="https://img-blog.csdnimg.cn/2019121020593729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>基于概率是强化学习中最直接的一种，他能通过感官分析所处的环境，直接输出下一步要采取的各种动作的概率，然后根据概率采取行动，所以每种动作都有可能被选中，只是可能性不同。而基于价值的方法输出则是所有动作的价值，我们会根据最高价值来选着动作，相比基于概率的方法，基于价值的决策部分更为铁定，毫不留情，就选价值最高的；而基于概率的，即使某个动作的概率最高，但是还是不一定会选到他。</p>
<p>我们现在说的动作都是一个一个不连续的动作，而对于选取连续的动作，基于价值的方法是无能为力的。我们却能用一个<strong>概率分布</strong>在连续动作中选取特定动作，这也是基于概率的方法的优点之一。那么这两类使用的方法又有哪些呢？</p>
<p>比如在基于概率这边，有 Policy Gradients，在基于价值这边有 Q-Learnin、Sarsa 等。而且我们还能结合这两类方法的优势之处，创造更牛逼的一种方法，叫做 Actor-Critic。<code>actor</code> 会基于概率做出动作，而 <code>critic</code> 会对做出的动作给出动作的价值，这样就在原有的 Policy Gradients 上加速了学习过程。</p>
<h2 id="2-3-回合更新和单步更新"><a href="#2-3-回合更新和单步更新" class="headerlink" title="2.3 回合更新和单步更新"></a>2.3 回合更新和单步更新</h2><p><img src="https://img-blog.csdnimg.cn/20191210210243643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>强化学习还能用另外一种方式分类，<strong>回合更新</strong>和<strong>单步更新</strong>，想象强化学习就是在玩游戏，游戏回合有开始和结束。回合更新指的是游戏开始后，我们要等待游戏结束，然后再总结这一回合中的所有转折点，再更新我们的行为准则。而单步更新则是在游戏进行中每一步都在更新，不用等待游戏的结束， 这样我们就能边玩边学习了。</p>
<p>再来说说方法，Monte-carlo Learning 和基础版的 Policy Gradients 等都是回合更新制，Q-Learning、 Sarsa、升级版的 Policy Gradients 等都是单步更新制。因为单步更新更有效率，所以现在大多方法都是基于单步更新。比如有的强化学习问题并不属于回合问题。</p>
<h2 id="2-4-在线学习和离线学习"><a href="#2-4-在线学习和离线学习" class="headerlink" title="2.4 在线学习和离线学习"></a>2.4 在线学习和离线学习</h2><p><img src="https://img-blog.csdnimg.cn/2019121021045949.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>最后一种分类方式是<strong>在线学习</strong>和<strong>离线学习</strong>，所谓在线学习，就是指我必须本人在场，并且一定是本人边玩边学习，而离线学习是你可以选择自己玩，也可以选择看着别人玩，通过看别人玩来学习别人的行为准则，离线学习 同样是从过往的经验中学习，但是这些过往的经历没必要是自己的经历，任何人的经历都能被学习。或者我也不必要边玩边学习，我可以白天先存储下来玩耍时的记忆，然后晚上通过离线学习来学习白天的记忆。那么每种学习的方法又有哪些呢？</p>
<p>最典型的在线学习就是 Sarsa 了，还有一种优化 Sarsa 的算法，叫做 Sarsa-Lambda，最典型的离线学习就是 Q-Learning，后来人也根据离线学习的属性，开发了更强大的算法，比如让计算机学会玩电动的 Deep-Q-Network。</p>
<p>这就是我们从各种不同的角度来对比了强化学习中的多种算法。</p>
<h1 id="3-为什么用强化学习"><a href="#3-为什么用强化学习" class="headerlink" title="3. 为什么用强化学习?"></a>3. 为什么用强化学习?</h1><h2 id="3-1-强化学习介绍"><a href="#3-1-强化学习介绍" class="headerlink" title="3.1 强化学习介绍"></a>3.1 强化学习介绍</h2><p><strong>强化学习</strong>(Reinforcement Learning) 是一个机器学习大家族中的分支，由于近些年来的技术突破，和<strong>深度学习</strong> (Deep Learning) 的整合使得强化学习有了进一步的运用。比如让计算机学着玩游戏，AlphaGo 挑战世界围棋高手，都是强化学习在行的事。强化学习也是让你的程序从对当前环境完全陌生，成长为一个在环境中游刃有余的高手。</p>
<p>这些教程的教学，不依赖于任何强化学习的 Python 模块。因为强化学习的复杂性、多样，到现在还没有比较好的统一化模块。不过我们还是能用最基础的方法编出优秀的强化学习程序!</p>
<h2 id="3-2-模拟程序提前看"><a href="#3-2-模拟程序提前看" class="headerlink" title="3.2 模拟程序提前看"></a>3.2 模拟程序提前看</h2><p>下面是其中莫烦强化学习教程中一些模拟视频:</p>
<ul>
<li>Maze</li>
</ul>
<iframe width="500" height="550" src="https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20sarsa_lambda.mp4" frameborder="0" allowfullscreen> </iframe>
-   Cartpole

<iframe width="500" height="550" src="https://morvanzhou.github.io/static/results/reinforcement-learning/cartpole%20dqn.mp4" frameborder="0" allowfullscreen> </iframe>
-   Mountain car

<iframe width="500" height="550" src="https://morvanzhou.github.io/static/results/reinforcement-learning/mountaincar%20dqn.mp4" frameborder="0" allowfullscreen> </iframe>
# 4. 课程要求

<h2 id="4-1-教程必备模块"><a href="#4-1-教程必备模块" class="headerlink" title="4.1 教程必备模块"></a>4.1 教程必备模块</h2><p>强化学习有一些现成的模块可以使，但是那些模块并不全面，而且强化学习很依赖与你给予的学习环境。对于不同学习环境的强化学，可能 RL 的代码就不同。所以我们要抱着以不变应万变的心态，用基础的模块，从基础学起。懂了原理，再复杂的环境也不在话下。</p>
<p>所以用到的模块和对应的教程:</p>
<ul>
<li>Numpy, Pandas (必学), 用于学习的数据处理</li>
<li>Matplotlib (可学), 偶尔会用来呈现误差曲线什么的</li>
<li>Tkinter (可学), 你可以自己用它来编写模拟环境</li>
<li>Tensorflow (可学), 后面实现神经网络与强化学习结合的时候用到</li>
<li>OpenAI gym (可学), 提供了很多现成的模拟环境</li>
</ul>
<h2 id="4-2-快速了解强化学习"><a href="#4-2-快速了解强化学习" class="headerlink" title="4.2 快速了解强化学习"></a>4.2 快速了解强化学习</h2><p>莫烦制作了每种强化学习对应的简介视频(有趣的机器学习)，大家可以只花很少的时间来观看了解这些学习方法的不同之处. 有了一定概念和基础，我们在这套教材里实现起来就容易多了。而且不懂的时候也能只花很少的时间回顾就。</p>
<h1 id="5-什么是Q-Learning"><a href="#5-什么是Q-Learning" class="headerlink" title="5. 什么是Q-Learning"></a>5. 什么是Q-Learning</h1><p>今天我们会来说说强化学习中一个很有名的算法— Q-learning。</p>
<h2 id="5-1-行为准则"><a href="#5-1-行为准则" class="headerlink" title="5.1 行为准则"></a>5.1 行为准则</h2><p><img src="https://img-blog.csdnimg.cn/20191209231339444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="写作业OR看电视"></p>
<p>我们做事情都会有一个自己的行为准则，比如小时候爸妈常说“不写完作业就不准看电视”。所以我们在 写作业的这种状态下，好的行为就是继续写作业，直到写完它，我们还可以得到奖励。不好的行为就是没写完就跑去看电视了，被爸妈发现，后果很严重。小时候这种事情做多了，也就变成我们不可磨灭的记忆。这和我们要提到的 Q-Learning 有什么关系呢？原来 Q-Learning 也是一个决策过程，和小时候的这种情况差不多。我们举例说明。</p>
<p>假设现在我们处于写作业的状态而且我们以前并没有尝试过写作业时看电视，所以现在我们有两种选择: 1) 继续写作业；2) 跑去看电视。因为以前没有被罚过，所以我选看电视，然后现在的状态变成了看电视, 我又选了继续看电视，接着我还是看电视，最后爸妈回家，发现我没写完作业就去看电视了，狠狠地惩罚了我一次。我也深刻地记下了这一次经历，并在我的脑海中将 “没写完作业就看电视” 这种行为更改为负面行为，我们在看看 Q-Learning 根据很多这样的经历是如何来决策的吧。</p>
<h2 id="5-2-Q-Learning决策"><a href="#5-2-Q-Learning决策" class="headerlink" title="5.2 Q-Learning决策"></a>5.2 Q-Learning决策</h2><p><img src="https://img-blog.csdnimg.cn/20191209235013646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>假设我们的行为准则已经学习好了，现在我们处于状态 <code>s1</code> ，我在写作业，我有两个行为 <code>a1</code> 和 <code>a2</code>，分别是看电视和写作业。根据我的经验，在这种 <code>s1</code> 状态下，<code>a2</code> 写作业带来的潜在奖励要比 <code>a1</code> 看电视高，这里的潜在奖励我们可以用一个有关于 <code>s</code> 和 <code>a</code> 的 <code>Q 表格</code>代替。在我的记忆 <code>Q表格</code> 中，<code>Q(s1, a1)=-2</code> 要小于 <code>Q(s1, a2)=1</code>，所以我们判断要选择 <code>a2</code> 作为下一个行为。现在我们的状态更新成 <code>s2</code> ，我们还是有两个同样的选择，重复上面的过程，在行为准则 <code>Q 表</code>中寻找 <code>Q(s2, a1)</code>和 <code>Q(s2, a2</code>) 的值，并比较他们的大小，选取较大的一个。接着根据 <code>a2</code> 我们到达 <code>s3</code> 并在此重复上面的决策过程。Q-Learning 的方法也就是这样决策的。看完决策，我看在来研究一下这张行为准则 Q 表是通过什么样的方式更改和提升的。</p>
<h2 id="5-3-Q-Learning更新"><a href="#5-3-Q-Learning更新" class="headerlink" title="5.3 Q-Learning更新"></a>5.3 Q-Learning更新</h2><p><img src="https://img-blog.csdnimg.cn/20191209233020278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>所以我们回到之前的流程，根据 Q 表的估计，因为在 <code>s1</code> 中，<code>a2</code> 的值比较大，通过之前的决策方法，我们在 <code>s1</code> 采取了 <code>a2</code>，并到达 <code>s2</code>，这时我们开始更新用于决策的 Q 表。接着我们并没有在实际中采取任何行为，而是再想象自己在 <code>s2</code> 上采取了每种行为，分别看看两种行为哪一个的 Q 值大，比如说 <code>Q(s2, a2)</code> 的值比 <code>Q(s2, a1)</code> 的大，所以我们把大的 <code>Q(s2, a2)</code> 乘上一个衰减值 <code>gamma</code> (比如是 0.9) 并加上到达 <code>s2</code> 时所获取的奖励 <code>R</code> (这里还没有获取到我们的棒棒糖，所以奖励为 0)，因为会获取实实在在的奖励 <code>R</code>， 我们将这个作为我现实中 <code>Q(s1, a2)</code> 的值，但是我们之前是根据 Q 表估计 <code>Q(s1, a2)</code> 的值。所以有了<strong>现实</strong>和<strong>估计值</strong>, 我们就能更新 <code>Q(s1, a2)</code> ，根据<strong>估计与现实的差距</strong>，将这个差距乘以一个学习效率 <code>alpha</code> 累加上老的 <code>Q(s1, a2)</code> 的值变成新的值。但时刻记住, 我们虽然用 <code>maxQ(s2)</code> 估算了一下 <code>s2</code> 状态，但还没有在 <code>s2</code> 做出任何的行为，<code>s2</code> 的行为决策要等到更新完了以后再重新另外做。这就是 <code>off-policy</code> 的 Q-Learning 是如何决策和学习优化决策的过程。</p>
<h2 id="5-4-Q-Learning整体算法"><a href="#5-4-Q-Learning整体算法" class="headerlink" title="5.4 Q-Learning整体算法"></a>5.4 Q-Learning整体算法</h2><p><img src="https://img-blog.csdnimg.cn/20191209233154685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这一张图概括了我们之前所有的内容。这也是 Q-Learning 的算法，每次更新我们都用到了 <code>Q 现实</code>和 <code>Q 估计</code>，而且 Q-Learning 的迷人之处就是 在 <code>Q(s1, a2) 现实</code>中, 也包含了一个 <code>Q(s2)</code> 的最大估计值，将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实，很奇妙吧。最后我们来说说这套算法中一些参数的意义。<code>Epsilon greedy</code> 是用在决策上的一种策略，比如 <code>epsilon=0.9</code> 时, 就说明有 <strong>90%</strong> 的情况我会按照 Q 表的最优值选择行为，<strong>10%</strong> 的时间使用随机选行为。<code>alpha</code> 是学习率，来决定这次的误差有多少是要被学习的，<code>alpha</code> 是一个小于1 的数。<code>gamma</code> 是对未来 reward 的衰减值。我们可以这样想象。</p>
<h2 id="5-5-Q-Learning中的Gamma"><a href="#5-5-Q-Learning中的Gamma" class="headerlink" title="5.5 Q-Learning中的Gamma"></a>5.5 Q-Learning中的Gamma</h2><p><img src="https://img-blog.csdnimg.cn/20191209233227569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们重写一下 <code>Q(s1)</code> 的公式，将 <code>Q(s2)</code> 拆开，因为 <code>Q(s2)</code> 可以像 <code>Q(s1)</code> 一样，是关于 <code>Q(s3)</code> 的，所以可以写成这样，然后以此类推，不停地这样写下去，最后就能写成这样。可以看出 <code>Q(s1)</code> 是有关于之后所有的奖励，但这些奖励正在衰减，离 <code>s1</code> 越远的状态衰减越严重。不好理解？行，我们想象 Q-Learning 的机器人天生近视眼，<code>gamma=1</code> 时，机器人有了一副合适的眼镜，在 <code>s1</code> 看到的 Q 是未来没有任何衰变的奖励，也就是机器人能清清楚楚地看到之后所有步的全部价值，但是当 <code>gamma=0</code>，近视机器人没了眼镜，只能摸到眼前的 reward, 同样也就只在乎最近的大奖励，如果 <code>gamma</code> 从 0 变到 1，眼镜的度数由浅变深，对远处的价值看得越清楚，所以机器人渐渐变得有远见，不仅仅只看眼前的利益，也为自己的未来着想。</p>
<h1 id="6-强化学习小例子"><a href="#6-强化学习小例子" class="headerlink" title="6. 强化学习小例子"></a>6. 强化学习小例子</h1><h2 id="6-1-要点"><a href="#6-1-要点" class="headerlink" title="6.1 要点"></a>6.1 要点</h2><p>这一次我们会用 tabular Q-Learning 的方法实现一个小例子，例子的环境是一个一维世界，在世界的右边有宝藏，探索者只要得到宝藏尝到了甜头，然后以后就记住了得到宝藏的方法，这就是他用强化学习所学习到的行为。</p>
<h2 id="6-2-预设值"><a href="#6-2-预设值" class="headerlink" title="6.2 预设值"></a>6.2 预设值</h2><p>这一次需要的模块和参数设置:</p>
<pre><code class="python">import numpy as np
import pandas as pd
import time

N_STATES = 6  # 1维世界的宽度
ACTIONS = [&#39;left&#39;, &#39;right&#39;]  # 探索者的可用动作
EPSILON = 0.9  # 贪婪度 greedy
ALPHA = 0.1    # 学习率
GAMMA = 0.9    # 奖励递减值
MAX_EPISODES = 13  # 最大回合数
FRESH_TIME = 0.3   # 移动间隔时间
</code></pre>
<h2 id="6-3-Q表"><a href="#6-3-Q表" class="headerlink" title="6.3 Q表"></a>6.3 Q表</h2><p>对于 tabular Q-Learning，我们必须将所有的 <code>Q-values</code> (行为值) 放在 <code>q_table</code> 中，更新 <code>q_table</code> 也是在更新他的行为准则。 <code>q_table</code> 的 <code>index</code> 是所有对应的 <code>state</code> (探索者位置)，<code>columns</code> 是对应的 <code>action</code> (探索者行为)。</p>
<pre><code class="python">def build_q_table(n_states, actions):
    table = pd.DataFrame(
        np.zeros((n_states, len(actions))),     # q_table 全 0 初始
        columns=actions,    # columns 对应的是行为名称
    )
    return table

# q_table:
&quot;&quot;&quot;
   left  right
0   0.0    0.0
1   0.0    0.0
2   0.0    0.0
3   0.0    0.0
4   0.0    0.0
5   0.0    0.0
&quot;&quot;&quot;</code></pre>
<h2 id="6-4-定义动作"><a href="#6-4-定义动作" class="headerlink" title="6.4 定义动作"></a>6.4 定义动作</h2><p>接着定义探索者是如何挑选行为的，这时我们引入 <code>epsilon greedy</code> 的概念。因为在初始阶段，随机的探索环境往往比固定的行为模式要好，所以这也是累积经验的阶段，我们希望探索者不会那么贪婪(greedy)。所以 <code>EPSILON</code> 就是用来控制贪婪程度的值。<code>EPSILON</code> 可以随着探索时间不断提升(越来越贪婪)，不过在这个例子中，我们就固定成 <code>EPSILON = 0.9</code>，90% 的时间是选择最优策略，10% 的时间来探索。</p>
<pre><code class="python"># 在某个 state 地点, 选择行为
def choose_action(state, q_table):
    state_actions = q_table.iloc[state, :]  # 选出这个 state 的所有 action 值
    if (np.random.uniform() &gt; EPSILON) or (state_actions.all() == 0):  # 非贪婪 or 或者这个 state 还没有探索过
        action_name = np.random.choice(ACTIONS)
    else:
        action_name = state_actions.argmax()    # 贪婪模式
    return action_name</code></pre>
<h2 id="6-5-环境反馈-S-，R"><a href="#6-5-环境反馈-S-，R" class="headerlink" title="6.5 环境反馈 S_，R"></a>6.5 环境反馈 S_，R</h2><p>做出行为后，环境也要给我们的行为一个反馈，反馈出下个 <code>state (S_)</code> 和 在上个 <code>state (S)</code> 做出 <code>action (A)</code> 所得到的 <code>reward (R)</code>。 这里定义的规则就是，只有当 <code>o</code> 移动到了 <code>T</code>，探索者才会得到唯一的一个奖励，奖励值 <code>R=1</code>，其他情况都没有奖励。</p>
<pre><code class="python">def get_env_feedback(S, A):
    # This is how agent will interact with the environment
    if A == &#39;right&#39;:    # move right
        if S == N_STATES - 2:   # terminate
            S_ = &#39;terminal&#39;
            R = 1
        else:
            S_ = S + 1
            R = 0
    else:   # move left
        R = 0
        if S == 0:
            S_ = S  # reach the wall
        else:
            S_ = S - 1
    return S_, R
</code></pre>
<h2 id="6-6-环境更新"><a href="#6-6-环境更新" class="headerlink" title="6.6 环境更新"></a>6.6 环境更新</h2><p>接下来就是环境的更新，不用细看。</p>
<pre><code class="python">def update_env(S, episode, step_counter):
    # This is how environment be updated
    env_list = [&#39;-&#39;]*(N_STATES-1) + [&#39;T&#39;]   # &#39;-----T&#39; our environment
    if S == &#39;terminal&#39;:
        interaction = &#39;Episode %s: total_steps = %s&#39; % (episode+1, step_counter)
        print(&#39;\r{}&#39;.format(interaction), end=&#39;&#39;)
        time.sleep(2)
        print(&#39;\r                                &#39;, end=&#39;&#39;)
    else:
        env_list[S] = &#39;o&#39;
        interaction = &#39;&#39;.join(env_list)
        print(&#39;\r{}&#39;.format(interaction), end=&#39;&#39;)
        time.sleep(FRESH_TIME)
</code></pre>
<h2 id="6-7-强化学习主循环"><a href="#6-7-强化学习主循环" class="headerlink" title="6.7 强化学习主循环"></a>6.7 强化学习主循环</h2><p>最重要的地方就在这里，你定义的 RL 方法都在这里体现。在之后的教程中，我们会更加详细得讲解 RL 中的各种方法，下面的内容大家大概看看就行，这节内容不用仔细研究。</p>
<p><img src="https://img-blog.csdnimg.cn/2019121011194063.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="RL主要算法"></p>
<pre><code class="python">def rl():
    q_table = build_q_table(N_STATES, ACTIONS)  # 初始 q table
    for episode in range(MAX_EPISODES):     # 回合
        step_counter = 0
        S = 0   # 回合初始位置
        is_terminated = False   # 是否回合结束
        update_env(S, episode, step_counter)    # 环境更新
        while not is_terminated:

            A = choose_action(S, q_table)   # 选行为
            S_, R = get_env_feedback(S, A)  # 实施行为并得到环境的反馈
            q_predict = q_table.loc[S, A]    # 估算的(状态-行为)值
            if S_ != &#39;terminal&#39;:
                # 实际的(状态-行为)值 (回合没结束)
                q_target = R + GAMMA * q_table.iloc[S_, :].max()   
            else:
                q_target = R  #  实际的(状态-行为)值 (回合结束)
                is_terminated = True  # terminate this episode

            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # q_table 更新
            S = S_  # 探索者移动到下一个 state

            update_env(S, episode, step_counter+1)  # 环境更新

            step_counter += 1
    return q_table</code></pre>
<p>写好所有的评估和更新准则后，我们就能开始训练了，把探索者丢到环境中，让它自己去玩吧。</p>
<pre><code class="python">if __name__ == &quot;__main__&quot;:
    q_table = rl()
    print(&#39;\r\nQ-table:\n&#39;)
    print(q_table)
</code></pre>
<h2 id="6-8-Q-Table的演变"><a href="#6-8-Q-Table的演变" class="headerlink" title="6.8 Q-Table的演变"></a>6.8 Q-Table的演变</h2><p>Q-Learning 学习过程其实就是不断去更新 Q 表的过程，实际训练时你会发现没带迭代到达终点的步数越来越少，其实就是下一次的迭代会基于上一次的 Q 表来做判断，这样模型就更能够判断准确。</p>
<p>这里训练 10 次，看一下到达终点的步数已经 Q 表的更新过程:</p>
<pre><code>Episode 1: total_steps = 38
   left  right
0   0.0    0.0
1   0.0    0.0
2   0.0    0.0
3   0.0    0.0
4   0.0    0.1
5   0.0    0.0
==================================================
Episode 2: total_steps = 22
   left  right
0   0.0  0.000
1   0.0  0.000
2   0.0  0.000
3   0.0  0.009
4   0.0  0.190
5   0.0  0.000
==================================================
Episode 3: total_steps = 9
   left  right
0   0.0  0.00000
1   0.0  0.00000
2   0.0  0.00081
3   0.0  0.02520
4   0.0  0.27100
5   0.0  0.00000

==================================================
Episode 4: total_steps = 5
   left  right
0   0.0  0.000000
1   0.0  0.000073
2   0.0  0.002997
3   0.0  0.047070
4   0.0  0.343900
5   0.0  0.000000

==================================================
Episode 5: total_steps = 7
      left  right
0  0.00000  0.000007
1  0.00000  0.000572
2  0.00003  0.006934
3  0.00000  0.073314
4  0.00000  0.409510
5  0.00000  0.000000

==================================================
Episode 6: total_steps = 5
      left  right
0  0.00000  0.000057
1  0.00000  0.001138
2  0.00003  0.012839
3  0.00000  0.102839
4  0.00000  0.468559
5  0.00000  0.000000

==================================================
Episode 7: total_steps = 5
      left  right
0  0.00000  0.000154
1  0.00000  0.002180
2  0.00003  0.020810
3  0.00000  0.134725
4  0.00000  0.521703
5  0.00000  0.000000

==================================================
Episode 8: total_steps = 5
      left  right
0  0.00000  0.000335
1  0.00000  0.003835
2  0.00003  0.030854
3  0.00000  0.168206
4  0.00000  0.569533
5  0.00000  0.000000

==================================================
Episode 9: total_steps = 5
      left  right
0  0.00000  0.000647
1  0.00000  0.006228
2  0.00003  0.042907
3  0.00000  0.202643
4  0.00000  0.612580
5  0.00000  0.000000

==================================================
Episode 10: total_steps = 5
      left  right
0  0.00000  0.001142
1  0.00000  0.009467
2  0.00003  0.056855
3  0.00000  0.237511
4  0.00000  0.651322
5  0.00000  0.000000

==================================================
Q-table:
      left     right
0  0.00000  0.001142
1  0.00000  0.009467
2  0.00003  0.056855
3  0.00000  0.237511
4  0.00000  0.651322
5  0.00000  0.000000
</code></pre><p>上述结果模型基本训练了5次左右，就已经学习到了相关的经验了，后面基本上每次只需要5步左右就可以到达终点了。</p>
<p>再来看看 Q 表的更新过程:</p>
<ul>
<li>第1步: 只更新了 <code>s4</code> 的 <code>right</code> 的 Q 值，其中未更新的 $Q(s4, \text{right})$ 就是 <code>q_predict</code> 为 0，这里 <code>(1 + 0.9*max(s5,:))</code> 表示 <code>q_target</code>。</li>
</ul>
<p>$$<br>Q(s4, \text{right}) \leftarrow Q(s4, \text{right}) + 0.1 \times [(1 + 0.9 \times max(s5,:)) - Q(s4, \text{right})]<br>\\ =0 + 0.1 \times [1+0.9 \times 0] = 0.1<br>$$</p>
<ul>
<li>第2步: 在第1步基础上更新了 Q表，这一步会用到上述已经更新好的 <code>Q(s4)</code> 的值，同时会更新 <code>Q(s4)</code> 和 <code>Q(s3)</code><ul>
<li>首先更新的是 <code>Q(s3)</code>，这一步是当前状态是 <code>s3</code>，且一步动作是 <code>right</code>。</li>
<li>然后更新的是 <code>Q(s4)</code>，这一步是当前状态是 <code>s4</code>，且一步动作是 <code>right</code>。</li>
</ul>
</li>
</ul>
<p>$$<br>=====更新Q(s_3)=====<br>\\ Q(s3, \text{right}) \leftarrow Q(s3, \text{right}) + 0.1 \times [(0 + 0. \times max(s4,:)) - Q(s3, \text{right})]<br>\\ =0 + 0.1 \times [0+0.9 \times 0.1 - 0] = 0.009<br>\\ =====更新Q(s_4)=====<br>\\ Q(s4, \text{right}) \leftarrow Q(s4, \text{right}) + 0.1 \times [(1 + 0.9 \times max(s5,:)) - Q(s4, \text{right})]<br>\\ =0.1 + 0.1 \times [1+0.9 \times 0 - 0.1] = 0.190<br>$$</p>
<p>这里可能有人会问，这个 <code>Q(s4)=0.190</code> 的值是基于 <code>s4</code> 状态下向=右走更新的值，为什么 <code>s4</code> 的状态不会选择采取 <code>left</code> 的动作跳回 <code>s3</code>么？如果选择往左走，<code>Q(s4)</code> 的值又会怎么变化？</p>
<p>显然在 <code>s4</code> 的状态下，是有可能选择往左走的，但是概率是 10%，90% 的情况会选择往右走，因为此时 <code>Q(s4,right)</code> 已经有相应的价值分了。下面演算一下如果 <code>s4</code> 状态下 10% 概率下选择往走时 <code>Q(s4, left)</code> 的值。<br>$$<br>Q(s4, \text{left}) \leftarrow Q(s4, \text{left}) + 0.1 \times [(0 + 0.9 \times max(s3,:)) - Q(s4, \text{left})]<br>\\ = 0 + 0.1\times[0+0.9 \times 0.1 - 0] = 0.009<br>$$<br>其他的步骤也都是基于这种演算过程，大家可以自己算一下。</p>
<h1 id="7-Q-Learning算法更新"><a href="#7-Q-Learning算法更新" class="headerlink" title="7. Q-Learning算法更新"></a>7. Q-Learning算法更新</h1><h2 id="7-1-要点"><a href="#7-1-要点" class="headerlink" title="7.1 要点"></a>7.1 要点</h2><p>上次我们知道了 RL 之中的 Q-Learning 方法是在做什么事，今天我们就来说说一个更具体的例子。让探索者学会走迷宫，黄色的是天堂 (<code>reward=1</code>)，黑色的地狱(<code>reward=-1</code>)。大多数 RL 是由 reward 导向的，所以定义 reward 是 RL 中比较重要的一点。</p>
<iframe 
    width="580"
    height="600" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20q.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<h2 id="7-2-算法"><a href="#7-2-算法" class="headerlink" title="7.2 算法"></a>7.2 算法</h2><p><img src="https://img-blog.csdnimg.cn/20191210133923830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="主要公式"></p>
<p>整个算法就是一直不断更新 Q table 里的值，然后再根据新的值来判断要在某个 state 采取怎样的 action。Q-Learning 是一个 <code>off-policy</code> 的算法，因为里面的 <code>max</code> action 让 Q-table 的更新可以不基于正在经历的经验(可以是现在学习着很久以前的经验，甚至是学习他人的经验)。不过这一次的例子, 我们没有运用到 <code>off-policy</code>，而是把 Q-Learning 用在了 <code>on-policy</code> 上，也就是现学现卖，将现在经历的直接当场学习并运用。<code>On-policy</code> 和 <code>off-policy</code> 的差别我们会在之后的 <a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-1-DQN1/" target="_blank" rel="noopener">Deep Q network (off-policy)</a> 学习中见识到。而之后的教程也会讲到一个 <code>on-policy</code> (Sarsa) 的形式，我们之后再对比。</p>
<h2 id="7-3-算法的代码形式"><a href="#7-3-算法的代码形式" class="headerlink" title="7.3 算法的代码形式"></a>7.3 算法的代码形式</h2><p>首先我们先 <code>import</code> 两个模块，<code>maze_env</code> 是我们的环境模块，已经编写好了。大家可以直接莫烦的GitHub中<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/2_Q_Learning_maze/maze_env.py" target="_blank" rel="noopener">下载</a>，<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境。 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<ul>
<li>导入自定义模块</li>
</ul>
<pre><code class="python">from maze_env import Maze
from RL_brain import QLearningTable
</code></pre>
<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Q-Learning 最重要的迭代更新部分啦。</p>
<pre><code class="python">def update():
    # 学习 100 回合
    for episode in range(100):
        # 初始化 state 的观测值
        observation = env.reset()

        while True:
            # 更新可视化环境
            env.render()

            # RL 大脑根据 state 的观测值挑选 action
            action = RL.choose_action(str(observation))

            # 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值,
            # reward 和 done (是否是掉下地狱或者升上天堂)
            observation_, reward, done = env.step(action)

            # RL 从这个序列 (state, action, reward, state_) 中学习
            RL.learn(str(observation), action, reward, str(observation_))

            # 将下一个 state 的值传到下一次循环
            observation = observation_

            # 如果掉下地狱或者升上天堂, 这回合就结束了
            if done:
                break

    # 结束游戏并关闭窗口
    print(&#39;game over&#39;)
    env.destroy()

if __name__ == &quot;__main__&quot;:
    # 定义环境 env 和 RL 方式
    env = Maze()
    RL = QLearningTable(actions=list(range(env.n_actions)))

    # 开始可视化环境 env
    env.after(100, update)
    env.mainloop()
</code></pre>
<h1 id="8-Q-Learning思维决策"><a href="#8-Q-Learning思维决策" class="headerlink" title="8. Q-Learning思维决策"></a>8. Q-Learning思维决策</h1><h2 id="8-1-代码主结构"><a href="#8-1-代码主结构" class="headerlink" title="8.1 代码主结构"></a>8.1 代码主结构</h2><p>与上回不一样的地方是，我们将要以一个 <code>class</code> 形式定义 Q-Learning，并把这种 tabular Q-Learning 方法叫做 <code>QLearningTable</code>。</p>
<pre><code class="python">class QLearningTable:
    # 初始化
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):

    # 选行为
    def choose_action(self, observation):

    # 学习更新参数
    def learn(self, s, a, r, s_):

    # 检测 state 是否存在
    def check_state_exist(self, state):
</code></pre>
<h2 id="8-2-预设值"><a href="#8-2-预设值" class="headerlink" title="8.2 预设值"></a>8.2 预设值</h2><p>初始的参数意义不会在这里提及了，请参考这个快速了解通道 <a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-3-tabular-q2/#" target="_blank" rel="noopener">机器学习系列-Q learning</a>。</p>
<pre><code class="python">import numpy as np
import pandas as pd


class QLearningTable:
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        self.actions = actions  # actions list
        self.lr = learning_rate # 学习率
        self.gamma = reward_decay   # 奖励衰减
        self.epsilon = e_greedy     # 贪婪度
        # 初始 q_table
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)   
</code></pre>
<h2 id="8-3-决定行为"><a href="#8-3-决定行为" class="headerlink" title="8.3 决定行为"></a>8.3 决定行为</h2><p>这里是定义如何根据所在的 <code>state</code>，或者是在这个 <code>state</code> 上的 观测值 (observation) 来决策。</p>
<pre><code class="python">def choose_action(self, observation):
    # 检测本 state 是否在 q_table 中存在(见后面标题内容)
    self.check_state_exist(observation) 

    # 选择 action
    if np.random.uniform() &lt; self.epsilon:  # 选择 Q value 最高的 action
        state_action = self.q_table.loc[observation, :]
        # 同一个 state, 可能会有多个相同的 Q action value, 所以我们乱序一下
        action = np.random.choice(state_action[state_action == np.max(state_action)].index)
    else:  # 随机选择 action
        action = np.random.choice(self.actions)

    return action</code></pre>
<h2 id="8-4-学习"><a href="#8-4-学习" class="headerlink" title="8.4 学习"></a>8.4 学习</h2><p>同上一个简单的 Q-Learning 例子一样，我们根据是否是 <code>terminal</code> state (回合终止符) 来判断应该如何更新 <code>q_table</code>。更新的方式是不是很熟悉呢:</p>
<pre><code class="python">update = self.lr * (q_target - q_predict)</code></pre>
<p>这可以理解成神经网络中的更新方式，<code>学习率 * (真实值 - 预测值)</code>。 将判断误差传递回去，有着和神经网络更新的异曲同工之处。</p>
<pre><code class="python">def learn(self, s, a, r, s_):
    self.check_state_exist(s_)  # 检测 q_table 中是否存在 s_ (见后面标题内容)
    q_predict = self.q_table.loc[s, a]
    if s_ != &#39;terminal&#39;:
        # 下个 state 不是 终止符
        q_target = r + self.gamma * self.q_table.loc[s_, :].max()  
    else:
        q_target = r  # 下个 state 是终止符
    # 更新对应的 state-action 值
    self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  </code></pre>
<h2 id="8-5-检测-state-是否存在"><a href="#8-5-检测-state-是否存在" class="headerlink" title="8.5 检测 state 是否存在"></a>8.5 检测 state 是否存在</h2><p>这个功能就是检测 <code>q_table</code> 中有没有当前 <code>state</code> 的步骤了，如果还没有当前 <code>state</code>，那我我们就插入一组全 0 数据，当做这个 <code>state</code> 的所有 <code>action</code> 初始 values。</p>
<pre><code class="python">def check_state_exist(self, state):
    if state not in self.q_table.index:
        # append new state to q table
        self.q_table = self.q_table.append(
            pd.Series(
                [0]*len(self.actions),
                index=self.q_table.columns,
                name=state,
            )
        )</code></pre>
<h1 id="9-什么是Sarsa"><a href="#9-什么是Sarsa" class="headerlink" title="9. 什么是Sarsa"></a>9. 什么是Sarsa</h1><p>今天我们会来说说强化学习中一个和 Q-Learning 类似的算法，叫做 Sarsa。</p>
<p><img src="https://img-blog.csdnimg.cn/20191210212458422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>在强化学习中 Sarsa 和 Q-Learning 及其类似，这节内容会基于之前我们所讲的 Q-Learning。所以还不熟悉 Q-Learning 的朋友们，请前往我制作的 Q-Learning 简介(知乎专栏)。我们会对比 Q-Learning，来看看 Sarsa 是特殊在哪些方面。和上次一样，我们还是使用写作业和看电视这个例子。没写完作业去看电视被打，写完了作业有糖吃。</p>
<h2 id="9-1-Sarsa决策"><a href="#9-1-Sarsa决策" class="headerlink" title="9.1 Sarsa决策"></a>9.1 Sarsa决策</h2><p><img src="https://img-blog.csdnimg.cn/20191210212638770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Sarsa 的决策部分和 Q-Learning 一模一样，因为我们使用的是 Q 表的形式决策，所以我们会在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩。但是不同的地方在于 Sarsa 的更新方式是不一样的。</p>
<h2 id="9-2-Sarsa更新行为准则"><a href="#9-2-Sarsa更新行为准则" class="headerlink" title="9.2 Sarsa更新行为准则"></a>9.2 Sarsa更新行为准则</h2><p><img src="https://img-blog.csdnimg.cn/20191210212726800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>同样，我们会经历正在写作业的状态 <code>s1</code>，然后再挑选一个带来最大潜在奖励的动作 <code>a2</code>，这样我们就到达了继续写作业状态 <code>s2</code>，而在这一步，如果你用的是 Q-Learning，你会观看一下在 <code>s2</code> 上选取哪一个动作会带来最大的奖励，但是在真正要做决定，却不一定会选取到那个带来最大奖励的动作，Q-Learning 在这一步只是估计了一下接下来的动作值。而 Sarsa 是实践派，他说到做到，在 <code>s2</code> 这一步估算的动作也是接下来要做的动作。所以 <code>Q(s1, a2)</code> 现实的计算值，我们也会稍稍改，去掉<code>maxQ</code> ，取而代之的是在 <code>s2</code> 上我们实实在在选取的 a2 的 Q 值。最后像 Q-Learning 一样，求出现实和估计的差距，并更新 Q 表里的 <code>Q(s1, a2)</code>。</p>
<h2 id="9-3-对比Sarsa和Q-Learning算法"><a href="#9-3-对比Sarsa和Q-Learning算法" class="headerlink" title="9.3 对比Sarsa和Q-Learning算法"></a>9.3 对比Sarsa和Q-Learning算法</h2><p><img src="https://img-blog.csdnimg.cn/2019121021312130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>从算法来看，这就是他们两最大的不同之处。因为 Sarsa 是说到做到型，所以我们也叫他 on-policy，在线学，学着自己在做的事情。而 Q-Learning 是说到但并不一定做到，所以它也叫作 Off-policy，离线学习。而因为有了 <code>maxQ</code>, Q-Learning 也是一个特别勇敢的算法。</p>
<p><img src="https://img-blog.csdnimg.cn/20191210213803956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>为什么说他勇敢呢，因为 Q-Learning 机器人永远都会选择最近的一条通往成功的道路，不管这条路会有多危险。而 Sarsa 则是相当保守，他会选择离危险远远的，拿到宝藏是次要的，保住自己的小命才是王道。这就是使用 Sarsa 方法的不同之处。</p>
<h1 id="10-Sarsa算法更新"><a href="#10-Sarsa算法更新" class="headerlink" title="10. Sarsa算法更新"></a>10. Sarsa算法更新</h1><h2 id="10-1-要点"><a href="#10-1-要点" class="headerlink" title="10.1 要点"></a>10.1 要点</h2><p>这次我们用同样的迷宫例子来实现 RL 中另一种和 Q-Learning 类似的算法，叫做 Sarsa (state-action-reward-state-action)。我们从这一个简称可以了解到，Sarsa 的整个循环都将是在一个路径上，也就是 On-Policy，下一个 <code>state_</code> 和下一个 <code>action_</code> 将会变成他真正采取的 <code>action_</code> 和 <code>state_</code>。和 <code>Q-Learning</code> 的不同之处就在这。 Q-Learning 的下个一个 <code>state_</code>、<code>action_</code> 在算法更新的时候都还是不确定的(off-policy)。而 Sarsa 的 <code>state_</code> 和 <code>action_</code> 在这次算法更新的时候已经确定好了 (on-policy)。</p>
<iframe 
    width="580"
    height="600" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20q.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<h2 id="10-2-算法"><a href="#10-2-算法" class="headerlink" title="10.2 算法"></a>10.2 算法</h2><p><img src="https://img-blog.csdnimg.cn/20191210214429797.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>整个算法还是一直不断更新 Q-table 里的值，然后再根据新的值来判断要在某个 <code>state</code> 采取怎样的 <code>action</code>。不过于 Q-Learning 不同之处:</p>
<ul>
<li>他在当前 <code>state</code> 已经想好了 <code>state</code> 对应的 <code>action</code>, 而且想好了 下一个 <code>state_</code> 和下一个 <code>action_</code> (Qlearning 还没有想好下一个 <code>action_</code>)</li>
<li>更新 <code>Q(s,a)</code> 的时候基于的是下一个 <code>Q(s_, a_)</code> (Qlearning 是基于 <code>maxQ(s_)</code>)</li>
</ul>
<p>这种不同之处使得 Sarsa 相对于 Q-Learning，更加的胆小。因为 Qlearning 永远都是想着 <code>maxQ</code> 最大化，因为这个 <code>maxQ</code> 而变得贪婪，不考虑其他非 <code>maxQ</code> 的结果。我们可以理解成 Q-Learning 是一种贪婪、大胆、勇敢的算法，对于错误、死亡并不在乎。而 Sarsa 是一种保守的算法，他在乎每一步决策，对于错误和死亡比较铭感。这一点我们会在可视化的部分看出他们的不同。两种算法都有他们的好处，比如在实际中，你比较在乎机器的损害，用一种保守的算法，在训练时就能减少损坏的次数。</p>
<h2 id="10-3-算法的代码形式"><a href="#10-3-算法的代码形式" class="headerlink" title="10.3 算法的代码形式"></a>10.3 算法的代码形式</h2><p>首先我们先 <code>import</code> 两个模块， <code>maze_env</code> 是我们的环境模块，已经编写好了，大家可以直接在这里下载，<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 tkinter 来编写虚拟环境。莫烦也有对应的教程 <code>maze_env</code> 就是用 tkinter 编写的。 而 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<pre><code class="python">from maze_env import Maze
from RL_brain import SarsaTable
</code></pre>
<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Sarsa 最重要的迭代更新部分啦。</p>
<pre><code class="python">def update():
    for episode in range(100):
        # 初始化环境
        observation = env.reset()

        # Sarsa 根据 state 观测选择行为
        action = RL.choose_action(str(observation))

        while True:
            # 刷新环境
            env.render()

            # 在环境中采取行为, 获得下一个 state_ (obervation_), reward, 和是否终止
            observation_, reward, done = env.step(action)

            # 根据下一个 state (obervation_) 选取下一个 action_
            action_ = RL.choose_action(str(observation_))

            # 从 (s, a, r, s, a) 中学习, 更新 Q_tabel 的参数 ==&gt; Sarsa
            RL.learn(str(observation), action, reward, str(observation_), action_)

            # 将下一个当成下一步的 state (observation) and action
            observation = observation_
            action = action_

            # 终止时跳出循环
            if done:
                break

    # 大循环完毕
    print(&#39;game over&#39;)
    env.destroy()

if __name__ == &quot;__main__&quot;:
    env = Maze()
    RL = SarsaTable(actions=list(range(env.n_actions)))

    env.after(100, update)
    env.mainloop()
</code></pre>
<p>下一节我们会来讲解 <code>SarsaTable</code> 这种算法具体要怎么编。</p>
<h1 id="11-Sarsa思维决策"><a href="#11-Sarsa思维决策" class="headerlink" title="11. Sarsa思维决策"></a>11. Sarsa思维决策</h1><p>接着上节内容，我们来实现 <code>RL_brain</code> 的 <code>SarsaTable</code> 部分，这也是 RL 的大脑部分，负责决策和思考。</p>
<h2 id="11-1-代码主结构"><a href="#11-1-代码主结构" class="headerlink" title="11.1 代码主结构"></a>11.1 代码主结构</h2><p>和之前定义 Q-Learning 中的 <code>QLearningTable</code> 一样，因为使用 tabular 方式的 <code>Sarsa</code> 和 <code>Q-Learning</code> 的相似度极高。</p>
<pre><code class="python">class SarsaTable:
    # 初始化 (与之前一样)
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):

    # 选行为 (与之前一样)
    def choose_action(self, observation):

    # 学习更新参数 (有改变)
    def learn(self, s, a, r, s_):

    # 检测 state 是否存在 (与之前一样)
    def check_state_exist(self, state):
</code></pre>
<p>我们甚至可以定义一个 主 <code>class RL</code> ，然后将 <code>QLearningTable</code> 和 <code>SarsaTable</code> 作为主 <code>class RL</code> 的衍生，这个主 <code>RL</code> 可以这样定义。所以我们将之前的 <code>__init__</code>、 <code>check_state_exist</code>、 <code>choose_action</code>、<code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<pre><code class="python">import numpy as np
import pandas as pd


class RL(object):
    def __init__(self, action_space, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        ... # 和 QLearningTable 中的代码一样

    def check_state_exist(self, state):
        ... # 和 QLearningTable 中的代码一样

    def choose_action(self, observation):
        ... # 和 QLearningTable 中的代码一样

    def learn(self, *args):
        pass # 每种的都有点不同, 所以用 pass
</code></pre>
<p>如果是这样定义父类的 <code>RL</code> class，通过继承关系，那之子类 <code>QLearningTable</code> class 就能简化成这样:</p>
<pre><code class="python">class QLearningTable(RL):   # 继承了父类 RL
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        super(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    # 表示继承关系

    def learn(self, s, a, r, s_):   # learn 的方法在每种类型中有不一样, 需重新定义
        self.check_state_exist(s_)
        q_predict = self.q_table.loc[s, a]
        if s_ != &#39;terminal&#39;:
            q_target = r + self.gamma * self.q_table.loc[s_, :].max()
        else:
            q_target = r
        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)
</code></pre>
<h2 id="11-2-学习"><a href="#11-2-学习" class="headerlink" title="11.2 学习"></a>11.2 学习</h2><p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。这就是我们所有的 <code>SarsaTable</code> 于父类 <code>RL</code> 不同之处的代码。是不是很简单。</p>
<pre><code class="python">class SarsaTable(RL):   # 继承 RL class

    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        super(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    # 表示继承关系

    def learn(self, s, a, r, s_, a_):
        self.check_state_exist(s_)
        q_predict = self.q_table.loc[s, a]
        if s_ != &#39;terminal&#39;:
            # q_target 基于选好的 a_ 而不是 Q(s_) 的最大值
            q_target = r + self.gamma * self.q_table.loc[s_, a_]  
        else:
            q_target = r  # 如果 s_ 是终止符
        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # 更新 q_table
</code></pre>
<h1 id="12-什么是Sarsa-lambda"><a href="#12-什么是Sarsa-lambda" class="headerlink" title="12. 什么是Sarsa-lambda"></a>12. 什么是Sarsa-lambda</h1><p>今天我们会来说说强化学习中基于 Sarsa 的一种提速方法，叫做 Sarsa-Lambda。</p>
<h2 id="12-1-Sarsa-n"><a href="#12-1-Sarsa-n" class="headerlink" title="12.1 Sarsa(n)"></a>12.1 Sarsa(n)</h2><p><img src="https://img-blog.csdnimg.cn/20191210232355243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>通过上面的介绍，我们知道这个 Sarsa 的算法是一种在线学习法，On-Policy。但是这个 Lambda 到底是什么。其实吧，Sarsa 是一种单步更新法，在环境中每走一步，更新一次自己的行为准则，我们可以在这样的 Sarsa 后面打一个括号，说他是 <code>Sarsa(0)</code>，因为他等走完这一步以后直接更新行为准则。如果延续这种想法，走完这步，再走一步，然后再更新，我们可以叫它 <code>Sarsa(1)</code>。同理，如果等待回合完毕我们一次性再更新呢，比如这回合我们走了 n 步，那我们就叫 <code>Sarsa(n)</code>。为了统一这样的流程，我们就有了一个 <code>lambda</code> 值来代替我们想要选择的步数，这也就是 <code>Sarsa(lambda)</code> 的由来。我们看看最极端的两个例子，对比单步更新和回合更新，看看回合更新的优势在哪里。</p>
<h2 id="12-2-单步更新和回合更新"><a href="#12-2-单步更新和回合更新" class="headerlink" title="12.2 单步更新和回合更新"></a>12.2 单步更新和回合更新</h2><p><img src="https://img-blog.csdnimg.cn/20191210232453544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>虽然我们每一步都在更新，但是在没有获取宝藏的时候，我们现在站着的这一步也没有得到任何更新，也就是直到获取宝藏时，我们才为获取到宝藏的上一步更新为: 这一步很好，和获取宝藏是有关联的，而之前为了获取宝藏所走的所有步都被认为和获取宝藏没关系。回合更新虽然我要等到这回合结束，才开始对本回合所经历的所有步都添加更新，但是这所有的步都是和宝藏有关系的，都是为了得到宝藏需要学习的步，所以每一个脚印在下回合被选则的几率又高了一些。在这种角度来看，回合更新似乎会有效率一些。</p>
<h2 id="12-3-有时迷茫"><a href="#12-3-有时迷茫" class="headerlink" title="12.3 有时迷茫"></a>12.3 有时迷茫</h2><p><img src="https://img-blog.csdnimg.cn/20191210232518798.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>我们看看这种情况，还是使用单步更新的方法在每一步都进行更新，但是同时记下之前的寻宝之路。你可以想像，每走一步，插上一个小旗子，这样我们就能清楚的知道除了最近的一步，找到宝物时还需要更新哪些步了。不过，有时候情况可能没有这么乐观。 开始的几次，因为完全没有头绪，我可能在原地打转了很久，然后才找到宝藏，那些重复的脚步真的对我拿到宝藏很有必要吗？答案我们都知道。所以 <code>Sarsa(lambda)</code> 就来拯救你啦。</p>
<h2 id="12-4-Lambda含义"><a href="#12-4-Lambda含义" class="headerlink" title="12.4 Lambda含义"></a>12.4 Lambda含义</h2><p><img src="https://img-blog.csdnimg.cn/20191210232548913.png" alt="在这里插入图片描述"></p>
<p>其实 lambda 就是一个衰变值，他可以让你知道离奖励越远的步可能并不是让你最快拿到奖励的步, 所以我们想象我们站在宝藏的位置，回头看看我们走过的寻宝之路，离宝藏越近的脚印越看得清，远处的脚印太渺小，我们都很难看清，那我们就索性记下离宝藏越近的脚印越重要，越需要被好好的更新。和之前我们提到过的<strong>奖励衰减值</strong> gamma 一样，lambda 是脚步衰减值，都是一个在 0 和 1 之间的数。</p>
<h2 id="12-5-Lambda取值"><a href="#12-5-Lambda取值" class="headerlink" title="12.5 Lambda取值"></a>12.5 Lambda取值</h2><p><img src="https://img-blog.csdnimg.cn/20191210232646125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>当 <code>lambda</code> 取 0，就变成了 Sarsa 的单步更新。当 <code>lambda=1</code>，就变成了回合更新，对所有步更新的力度都是一样。当 <code>lambda</code> 在 0 和 1 之间，取值越大，离宝藏越近的步更新力度越大。这样我们就不用受限于单步更新的每次只能更新最近的一步，我们可以更有效率的更新所有相关步了。</p>
<h1 id="13-Sarsa-lambda"><a href="#13-Sarsa-lambda" class="headerlink" title="13. Sarsa-lambda"></a>13. Sarsa-lambda</h1><h2 id="13-1-要点"><a href="#13-1-要点" class="headerlink" title="13.1 要点"></a>13.1 要点</h2><p>Sarsa-lambda 是基于 Sarsa 方法的升级版，它能更有效率地学习到怎么样获得好的 reward。如果说 Sarsa 和 Q-Learning 都是每次获取到 reward，只更新获取到 reward 的前一步。 那 Sarsa-lambda 就是更新获取到 reward 的前 lambda 步。<code>lambda</code> 是在 <code>[0, 1]</code> 之间取值:</p>
<ul>
<li><p>如果 <code>lambda=0</code>，Sarsa-lambda 就是 Sarsa，只更新获取到 reward 前经历的最后一步。</p>
</li>
<li><p>如果 <code>lambda=1</code>，Sarsa-lambda 更新的是获取到 reward 前所有经历的步。</p>
</li>
</ul>
<p>这样解释起来有点抽象，还是建议大家观看莫烦制作的什么是 Sarsa-lambda 短视频，用动画展示具体的区别。</p>
<iframe 
    width="580"
    height="600" 
    src="https://morvanzhou.github.io/static/results/reinforcement-learning/maze%20sarsa_lambda.mp4"
    frameborder="0" 
    allowfullscreen>
</iframe>

<h2 id="13-2-代码主结构"><a href="#13-2-代码主结构" class="headerlink" title="13.2 代码主结构"></a>13.2 代码主结构</h2><p>使用 <code>SarsaLambdaTable</code> 在算法更新迭代的部分，是和之前的 <code>SarsaTable</code> 一样的，所以这一节，我们没有算法更新部分，直接变成 思维决策部分。</p>
<pre><code class="python">class SarsaLambdaTable:
    # 初始化 (有改变)
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):

    # 选行为 (与之前一样)
    def choose_action(self, observation):

    # 学习更新参数 (有改变)
    def learn(self, s, a, r, s_):

    # 检测 state 是否存在 (有改变)
    def check_state_exist(self, state):
</code></pre>
<p>同样，我们选择继承的方式，将 <code>SarsaLambdaTable</code> 继承到 <code>RL</code>，所以我们将之前的 <code>__init__</code>、<code>check_state_exist</code>、 <code>choose_action</code>、 <code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<p>算法的相应更改请参考这个:</p>
<p><img src="https://img-blog.csdnimg.cn/20191210234954271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="13-3-预设值"><a href="#13-3-预设值" class="headerlink" title="13.3 预设值"></a>13.3 预设值</h2><p>在预设值当中，我们添加了 <code>trace_decay=0.9</code> 这个就是 <code>lambda</code> 的值了。这个值将会使得拿到 reward 前的每一步都有价值。</p>
<pre><code class="python">class SarsaLambdaTable(RL): # 继承 RL class
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):
        super(SarsaLambdaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)

        # 后向观测算法, eligibility trace.
        self.lambda_ = trace_decay
        # 空的 eligibility trace 表
        self.eligibility_trace = self.q_table.copy()
</code></pre>
<h2 id="13-4-检测state是否存在"><a href="#13-4-检测state是否存在" class="headerlink" title="13.4 检测state是否存在"></a>13.4 检测state是否存在</h2><p><code>check_state_exist</code> 和之前的是高度相似的。唯一不同的地方是我们考虑了 <code>eligibility_trace</code>。</p>
<pre><code class="python">class SarsaLambdaTable(RL): # 继承 RL class
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):
        pass
    def check_state_exist(self, state):
        if state not in self.q_table.index:
            # append new state to q table
            to_be_append = pd.Series(
                    [0] * len(self.actions),
                    index=self.q_table.columns,
                    name=state,
                )
            self.q_table = self.q_table.append(to_be_append)

            # also update eligibility trace
            self.eligibility_trace = self.eligibility_trace.append(to_be_append)
</code></pre>
<h2 id="13-5-学习"><a href="#13-5-学习" class="headerlink" title="13.5 学习"></a>13.5 学习</h2><p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaLambdaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。就是我们所有的 <code>SarsaLambdaTable</code> 于父类 <code>RL</code> 不同之处的代码。是不是很简单。</p>
<pre><code class="python">class SarsaLambdaTable(RL): # 继承 RL class
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.9):
        ...
    def check_state_exist(self, state):
        ...
    def learn(self, s, a, r, s_, a_):
        # 这部分和 Sarsa 一样
        self.check_state_exist(s_)
        q_predict = self.q_table.ix[s, a]
        if s_ != &#39;terminal&#39;:
            q_target = r + self.gamma * self.q_table.ix[s_, a_]
        else:
            q_target = r
        error = q_target - q_predict

        # 这里开始不同:
        # 对于经历过的 state-action, 我们让他+1, 证明他是得到 reward 路途中不可或缺的一环
        self.eligibility_trace.ix[s, a] += 1

        # Q table 更新
        self.q_table += self.lr * error * self.eligibility_trace

        # 随着时间衰减 eligibility trace 的值, 离获取 reward 越远的步, 他的&quot;不可或缺性&quot;越小
        self.eligibility_trace *= self.gamma*self.lambda_
</code></pre>
<p>除了图中和上面代码这种更新方式，还有一种会更加有效率。我们可以将上面的这一步替换成下面这样:</p>
<pre><code class="python"># 上面代码中的方式:
self.eligibility_trace.ix[s, a] += 1

# 更有效的方式:
self.eligibility_trace.ix[s, :] *= 0
self.eligibility_trace.ix[s, a] = 1</code></pre>
<p>他们两的不同之处可以用这张图来概括:</p>
<p><img src="https://img-blog.csdnimg.cn/20191210235506805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Q2gxTW9uc3Rlcg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这是针对于一个 state-action 值按经历次数的变化。最上面是经历 state-action 的时间点，第二张图是使用这种方式所带来的 “不可或缺性值”:</p>
<pre><code class="python">self.eligibility_trace.ix[s, a] += 1</code></pre>
<p>下面图是使用这种方法带来的 “不可或缺性值”:</p>
<pre><code class="python">self.eligibility_trace.ix[s, :] *= 0
self.eligibility_trace.ix[s, a] = 1</code></pre>
<p>实验证明选择下面这种方法会有更好的效果。大家也可以自己玩一玩, 试试两种方法的不同表现。</p>
<p>最后不要忘了，eligibility trace 只是记录每个回合的每一步，新回合开始的时候需要将 Trace 清零。</p>
<pre><code class="python">for episode in range(100):
    ...
    # 新回合, 清零
    RL.eligibility_trace *= 0

    while True: # 开始回合
        ...</code></pre>
<h1 id="14-什么是DQN"><a href="#14-什么是DQN" class="headerlink" title="14. 什么是DQN"></a>14. 什么是DQN</h1><h2 id="14-1-强化学习与神经网络"><a href="#14-1-强化学习与神经网络" class="headerlink" title="14.1 强化学习与神经网络"></a>14.1 强化学习与神经网络</h2><h2 id="14-2-神经网络的作用"><a href="#14-2-神经网络的作用" class="headerlink" title="14.2 神经网络的作用"></a>14.2 神经网络的作用</h2><h2 id="14-3-更新神经网络"><a href="#14-3-更新神经网络" class="headerlink" title="14.3 更新神经网络"></a>14.3 更新神经网络</h2><h2 id="14-4-DQN两大利器"><a href="#14-4-DQN两大利器" class="headerlink" title="14.4 DQN两大利器"></a>14.4 DQN两大利器</h2><h1 id="15-DQN算法更新—TensorFlow"><a href="#15-DQN算法更新—TensorFlow" class="headerlink" title="15. DQN算法更新—TensorFlow"></a>15. DQN算法更新—TensorFlow</h1><h2 id="15-1-要点"><a href="#15-1-要点" class="headerlink" title="15.1 要点"></a>15.1 要点</h2><h2 id="15-2-算法"><a href="#15-2-算法" class="headerlink" title="15.2 算法"></a>15.2 算法</h2><h2 id="15-3-算法的代码形式"><a href="#15-3-算法的代码形式" class="headerlink" title="15.3 算法的代码形式"></a>15.3 算法的代码形式</h2><h1 id="16-DQN算法更新—TensorFlow"><a href="#16-DQN算法更新—TensorFlow" class="headerlink" title="16. DQN算法更新—TensorFlow"></a>16. DQN算法更新—TensorFlow</h1><h2 id="16-1-要点"><a href="#16-1-要点" class="headerlink" title="16.1 要点"></a>16.1 要点</h2><h2 id="16-2-算法"><a href="#16-2-算法" class="headerlink" title="16.2 算法"></a>16.2 算法</h2><h2 id="16-3-算法的代码形式"><a href="#16-3-算法的代码形式" class="headerlink" title="16.3 算法的代码形式"></a>16.3 算法的代码形式</h2><h1 id="17-DQN神经网络—TensorFlow"><a href="#17-DQN神经网络—TensorFlow" class="headerlink" title="17. DQN神经网络—TensorFlow"></a>17. DQN神经网络—TensorFlow</h1><h2 id="17-1-要点"><a href="#17-1-要点" class="headerlink" title="17.1 要点"></a>17.1 要点</h2><h2 id="17-2-两个神经网络"><a href="#17-2-两个神经网络" class="headerlink" title="17.2 两个神经网络"></a>17.2 两个神经网络</h2><h2 id="17-3-神经网络结构"><a href="#17-3-神经网络结构" class="headerlink" title="17.3 神经网络结构"></a>17.3 神经网络结构</h2><h2 id="17-4-常见两个网络"><a href="#17-4-常见两个网络" class="headerlink" title="17.4 常见两个网络"></a>17.4 常见两个网络</h2><h1 id="18-DQN思维决策—TensorFlow"><a href="#18-DQN思维决策—TensorFlow" class="headerlink" title="18. DQN思维决策—TensorFlow"></a>18. DQN思维决策—TensorFlow</h1><h2 id="18-1-代码主结构"><a href="#18-1-代码主结构" class="headerlink" title="18.1 代码主结构"></a>18.1 代码主结构</h2><h2 id="18-2-初始值"><a href="#18-2-初始值" class="headerlink" title="18.2 初始值"></a>18.2 初始值</h2><h2 id="18-3-存储记忆"><a href="#18-3-存储记忆" class="headerlink" title="18.3 存储记忆"></a>18.3 存储记忆</h2><h2 id="18-4-选行为"><a href="#18-4-选行为" class="headerlink" title="18.4 选行为"></a>18.4 选行为</h2><h2 id="18-5-学习"><a href="#18-5-学习" class="headerlink" title="18.5 学习"></a>18.5 学习</h2><h2 id="18-6-学习效果"><a href="#18-6-学习效果" class="headerlink" title="18.6 学习效果"></a>18.6 学习效果</h2><h2 id="18-7-修改版的-DQN"><a href="#18-7-修改版的-DQN" class="headerlink" title="18.7 修改版的 DQN"></a>18.7 修改版的 DQN</h2><h1 id="19-OpenAI-gym环境库"><a href="#19-OpenAI-gym环境库" class="headerlink" title="19. OpenAI gym环境库"></a>19. OpenAI gym环境库</h1><h2 id="19-1-要点"><a href="#19-1-要点" class="headerlink" title="19.1 要点"></a>19.1 要点</h2><h2 id="19-2-安装gym"><a href="#19-2-安装gym" class="headerlink" title="19.2 安装gym"></a>19.2 安装gym</h2><h2 id="19-3-CartPole例子"><a href="#19-3-CartPole例子" class="headerlink" title="19.3 CartPole例子"></a>19.3 CartPole例子</h2><h2 id="19-4-MountainCar例子"><a href="#19-4-MountainCar例子" class="headerlink" title="19.4 MountainCar例子"></a>19.4 MountainCar例子</h2><h1 id="20-Double-DQN—TensorFlow"><a href="#20-Double-DQN—TensorFlow" class="headerlink" title="20. Double DQN—TensorFlow"></a>20. Double DQN—TensorFlow</h1><h2 id="20-1-要点"><a href="#20-1-要点" class="headerlink" title="20.1 要点"></a>20.1 要点</h2><h2 id="20-2-Double-DQN算法"><a href="#20-2-Double-DQN算法" class="headerlink" title="20.2 Double DQN算法"></a>20.2 Double DQN算法</h2><h2 id="20-3-更新方法"><a href="#20-3-更新方法" class="headerlink" title="20.3 更新方法"></a>20.3 更新方法</h2><h2 id="20-4-记录-Q-值"><a href="#20-4-记录-Q-值" class="headerlink" title="20.4 记录 Q 值"></a>20.4 记录 Q 值</h2><h2 id="20-5-对比结果"><a href="#20-5-对比结果" class="headerlink" title="20.5 对比结果"></a>20.5 对比结果</h2><h1 id="21-Prioritized-Experience-Replay-DQN-—TensorFlow"><a href="#21-Prioritized-Experience-Replay-DQN-—TensorFlow" class="headerlink" title="21. Prioritized Experience Replay(DQN)—TensorFlow"></a>21. Prioritized Experience Replay(DQN)—TensorFlow</h1><h2 id="21-1-要点"><a href="#21-1-要点" class="headerlink" title="21.1 要点"></a>21.1 要点</h2><h2 id="21-2-Prioritized-Replay算法"><a href="#21-2-Prioritized-Replay算法" class="headerlink" title="21.2 Prioritized Replay算法"></a>21.2 Prioritized Replay算法</h2><h2 id="21-3-SumTree有效抽样"><a href="#21-3-SumTree有效抽样" class="headerlink" title="21.3 SumTree有效抽样"></a>21.3 SumTree有效抽样</h2><h2 id="21-4-Memory类"><a href="#21-4-Memory类" class="headerlink" title="21.4 Memory类"></a>21.4 Memory类</h2><h2 id="21-5-更新方法"><a href="#21-5-更新方法" class="headerlink" title="21.5 更新方法"></a>21.5 更新方法</h2><h2 id="21-6-对比结果"><a href="#21-6-对比结果" class="headerlink" title="21.6 对比结果"></a>21.6 对比结果</h2><h1 id="22-Dueling-DQN—TensorFlow"><a href="#22-Dueling-DQN—TensorFlow" class="headerlink" title="22. Dueling DQN—TensorFlow"></a>22. Dueling DQN—TensorFlow</h1><h2 id="22-1-要点"><a href="#22-1-要点" class="headerlink" title="22.1 要点"></a>22.1 要点</h2><h2 id="22-2-Dueling算法"><a href="#22-2-Dueling算法" class="headerlink" title="22.2 Dueling算法"></a>22.2 Dueling算法</h2><h2 id="22-3-更新方法"><a href="#22-3-更新方法" class="headerlink" title="22.3 更新方法"></a>22.3 更新方法</h2><h2 id="22-4-对比结果"><a href="#22-4-对比结果" class="headerlink" title="22.4 对比结果"></a>22.4 对比结果</h2><h1 id="23-什么是Policy-Gradients"><a href="#23-什么是Policy-Gradients" class="headerlink" title="23. 什么是Policy Gradients"></a>23. 什么是Policy Gradients</h1><h2 id="23-1-和以往的强化学习方法不同"><a href="#23-1-和以往的强化学习方法不同" class="headerlink" title="23.1 和以往的强化学习方法不同"></a>23.1 和以往的强化学习方法不同</h2><h2 id="23-2-更新不同之处"><a href="#23-2-更新不同之处" class="headerlink" title="23.2 更新不同之处"></a>23.2 更新不同之处</h2><h2 id="23-3-具体更新步骤"><a href="#23-3-具体更新步骤" class="headerlink" title="23.3 具体更新步骤"></a>23.3 具体更新步骤</h2><h1 id="24-Policy-Gradients算法更新—TensorFlow"><a href="#24-Policy-Gradients算法更新—TensorFlow" class="headerlink" title="24. Policy Gradients算法更新—TensorFlow"></a>24. Policy Gradients算法更新—TensorFlow</h1><h2 id="24-1-要点"><a href="#24-1-要点" class="headerlink" title="24.1 要点"></a>24.1 要点</h2><h2 id="24-2-算法"><a href="#24-2-算法" class="headerlink" title="24.2 算法"></a>24.2 算法</h2><h2 id="24-3-算法代码形式"><a href="#24-3-算法代码形式" class="headerlink" title="24.3 算法代码形式"></a>24.3 算法代码形式</h2><h1 id="25-Policy-Gradients思维决策—TensorFlow"><a href="#25-Policy-Gradients思维决策—TensorFlow" class="headerlink" title="25. Policy Gradients思维决策—TensorFlow"></a>25. Policy Gradients思维决策—TensorFlow</h1><h2 id="25-1-主要代码结构"><a href="#25-1-主要代码结构" class="headerlink" title="25.1 主要代码结构"></a>25.1 主要代码结构</h2><h2 id="25-2-初始化"><a href="#25-2-初始化" class="headerlink" title="25.2 初始化"></a>25.2 初始化</h2><h2 id="25-3-建立Policy神经网络"><a href="#25-3-建立Policy神经网络" class="headerlink" title="25.3 建立Policy神经网络"></a>25.3 建立Policy神经网络</h2><h2 id="25-4-选行为"><a href="#25-4-选行为" class="headerlink" title="25.4 选行为"></a>25.4 选行为</h2><h2 id="25-5-存储回合"><a href="#25-5-存储回合" class="headerlink" title="25.5 存储回合"></a>25.5 存储回合</h2><h2 id="25-6-学习"><a href="#25-6-学习" class="headerlink" title="25.6 学习"></a>25.6 学习</h2><h1 id="26-什么是Actor-Critic"><a href="#26-什么是Actor-Critic" class="headerlink" title="26. 什么是Actor Critic"></a>26. 什么是Actor Critic</h1><h2 id="26-1-为什么要有Actor和Critic"><a href="#26-1-为什么要有Actor和Critic" class="headerlink" title="26.1 为什么要有Actor和Critic"></a>26.1 为什么要有Actor和Critic</h2><h2 id="26-2-Actor和Critic"><a href="#26-2-Actor和Critic" class="headerlink" title="26.2 Actor和Critic"></a>26.2 Actor和Critic</h2><h2 id="26-3-增加单步更新属性"><a href="#26-3-增加单步更新属性" class="headerlink" title="26.3 增加单步更新属性"></a>26.3 增加单步更新属性</h2><h2 id="26-4-改进版Deep-Deterministic-Policy-Gradient-DDPG"><a href="#26-4-改进版Deep-Deterministic-Policy-Gradient-DDPG" class="headerlink" title="26.4 改进版Deep Deterministic Policy Gradient(DDPG)"></a>26.4 改进版Deep Deterministic Policy Gradient(DDPG)</h2><h1 id="27-Actot-Critic—TensorFlow"><a href="#27-Actot-Critic—TensorFlow" class="headerlink" title="27. Actot Critic—TensorFlow"></a>27. Actot Critic—TensorFlow</h1><h2 id="27-1-要点"><a href="#27-1-要点" class="headerlink" title="27.1 要点"></a>27.1 要点</h2><h2 id="27-2-算法"><a href="#27-2-算法" class="headerlink" title="27.2 算法"></a>27.2 算法</h2><h2 id="27-3-代码主结构"><a href="#27-3-代码主结构" class="headerlink" title="27.3 代码主结构"></a>27.3 代码主结构</h2><h2 id="27-4-两者学习方式"><a href="#27-4-两者学习方式" class="headerlink" title="27.4 两者学习方式"></a>27.4 两者学习方式</h2><h2 id="27-5-每回合算法"><a href="#27-5-每回合算法" class="headerlink" title="27.5 每回合算法"></a>27.5 每回合算法</h2><h1 id="28-什么是Deep-Deterministic-Policy-Gradient-DDPG"><a href="#28-什么是Deep-Deterministic-Policy-Gradient-DDPG" class="headerlink" title="28. 什么是Deep Deterministic Policy Gradient(DDPG)"></a>28. 什么是Deep Deterministic Policy Gradient(DDPG)</h1><h2 id="28-1-拆分细讲"><a href="#28-1-拆分细讲" class="headerlink" title="28.1 拆分细讲"></a>28.1 拆分细讲</h2><h2 id="28-2-Deep和DQN"><a href="#28-2-Deep和DQN" class="headerlink" title="28.2 Deep和DQN"></a>28.2 Deep和DQN</h2><h2 id="28-3-Deterministic-Policy-Gradient"><a href="#28-3-Deterministic-Policy-Gradient" class="headerlink" title="28.3 Deterministic Policy Gradient"></a>28.3 Deterministic Policy Gradient</h2><h2 id="28-4-DDPG神经网络"><a href="#28-4-DDPG神经网络" class="headerlink" title="28.4 DDPG神经网络"></a>28.4 DDPG神经网络</h2><h1 id="29-Deep-Deterministic-Policy-Gradient-DDPG-—TensorFlow"><a href="#29-Deep-Deterministic-Policy-Gradient-DDPG-—TensorFlow" class="headerlink" title="29. Deep Deterministic Policy Gradient(DDPG)—TensorFlow"></a>29. Deep Deterministic Policy Gradient(DDPG)—TensorFlow</h1><h2 id="29-1-要点"><a href="#29-1-要点" class="headerlink" title="29.1 要点"></a>29.1 要点</h2><h2 id="29-2-算法"><a href="#29-2-算法" class="headerlink" title="29.2 算法"></a>29.2 算法</h2><h2 id="29-3-主结构"><a href="#29-3-主结构" class="headerlink" title="29.3 主结构"></a>29.3 主结构</h2><h2 id="29-4-主结构"><a href="#29-4-主结构" class="headerlink" title="29.4 主结构"></a>29.4 主结构</h2><h2 id="29-5-Actor-Critic"><a href="#29-5-Actor-Critic" class="headerlink" title="29.5 Actor Critic"></a>29.5 Actor Critic</h2><h2 id="29-6-记忆库Mmeory"><a href="#29-6-记忆库Mmeory" class="headerlink" title="29.6 记忆库Mmeory"></a>29.6 记忆库Mmeory</h2><h2 id="29-7-每回合算法"><a href="#29-7-每回合算法" class="headerlink" title="29.7 每回合算法"></a>29.7 每回合算法</h2><h2 id="29-8-简化版代码"><a href="#29-8-简化版代码" class="headerlink" title="29.8 简化版代码"></a>29.8 简化版代码</h2><h1 id="30-什么是Asynchronous-Advantage-Actor-Critic-A3C"><a href="#30-什么是Asynchronous-Advantage-Actor-Critic-A3C" class="headerlink" title="30. 什么是Asynchronous Advantage Actor-Critic (A3C)"></a>30. 什么是Asynchronous Advantage Actor-Critic (A3C)</h1><h2 id="30-1-平行宇宙"><a href="#30-1-平行宇宙" class="headerlink" title="30.1 平行宇宙"></a>30.1 平行宇宙</h2><h2 id="30-2-平行训练"><a href="#30-2-平行训练" class="headerlink" title="30.2 平行训练"></a>30.2 平行训练</h2><h2 id="30-3-多核训练"><a href="#30-3-多核训练" class="headerlink" title="30.3 多核训练"></a>30.3 多核训练</h2><h1 id="31-Asynchronous-Advantage-Actor-Critic-A3C-—TensorFlow"><a href="#31-Asynchronous-Advantage-Actor-Critic-A3C-—TensorFlow" class="headerlink" title="31. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow"></a>31. Asynchronous Advantage Actor-Critic (A3C) —TensorFlow</h1><h2 id="31-1-要点"><a href="#31-1-要点" class="headerlink" title="31.1 要点"></a>31.1 要点</h2><h2 id="31-2-算法"><a href="#31-2-算法" class="headerlink" title="31.2 算法"></a>31.2 算法</h2><h2 id="31-3-主结构"><a href="#31-3-主结构" class="headerlink" title="31.3 主结构"></a>31.3 主结构</h2><h2 id="31-4-Actor-Critic网络"><a href="#31-4-Actor-Critic网络" class="headerlink" title="31.4 Actor Critic网络"></a>31.4 Actor Critic网络</h2><h2 id="31-5-Worker"><a href="#31-5-Worker" class="headerlink" title="31.5 Worker"></a>31.5 Worker</h2><h2 id="31-6-Worker并行工作"><a href="#31-6-Worker并行工作" class="headerlink" title="31.6 Worker并行工作"></a>31.6 Worker并行工作</h2><h2 id="31-7-机械手臂"><a href="#31-7-机械手臂" class="headerlink" title="31.7 机械手臂"></a>31.7 机械手臂</h2><h2 id="31-8-multiprocessing-A3C"><a href="#31-8-multiprocessing-A3C" class="headerlink" title="31.8 multiprocessing+A3C"></a>31.8 multiprocessing+A3C</h2><h1 id="32-Distributed-Proximal-Policy-Optimization-DPPO-—Tensorflow"><a href="#32-Distributed-Proximal-Policy-Optimization-DPPO-—Tensorflow" class="headerlink" title="32. Distributed Proximal Policy Optimization (DPPO) —Tensorflow"></a>32. Distributed Proximal Policy Optimization (DPPO) —Tensorflow</h1><h2 id="32-1-要点"><a href="#32-1-要点" class="headerlink" title="32.1 要点"></a>32.1 要点</h2><h2 id="32-2-OpenAI-和-DeepMind-的-Demo"><a href="#32-2-OpenAI-和-DeepMind-的-Demo" class="headerlink" title="32.2 OpenAI 和 DeepMind 的 Demo"></a>32.2 OpenAI 和 DeepMind 的 Demo</h2><h2 id="32-3-算法"><a href="#32-3-算法" class="headerlink" title="32.3 算法"></a>32.3 算法</h2><h2 id="32-4-简单的-PPO-主结构"><a href="#32-4-简单的-PPO-主结构" class="headerlink" title="32.4 简单的 PPO 主结构"></a>32.4 简单的 PPO 主结构</h2><h2 id="32-5-Distributed-PPO"><a href="#32-5-Distributed-PPO" class="headerlink" title="32.5 Distributed PPO"></a>32.5 Distributed PPO</h2>
      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 yu_mingm623@163.com </span>
    </div>
</article>


<p>
    <a  class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>莫烦-强化学习课程</p>
    <p><span class="copy-title">文章字数:</span><span class="post-count">14.1k</span></p>
    <p><span class="copy-title">本文作者:</span><a  title="郁明敏">郁明敏</a></p>
    <p><span class="copy-title">发布时间:</span>2019-12-08, 12:49:03</p>
    <p><span class="copy-title">最后更新:</span>2019-12-10, 23:59:19</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/2019/12/08/morvan-courses-rl/" title="莫烦-强化学习课程">http://www.monsteryu.top/2019/12/08/morvan-courses-rl/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>





    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js"
        value="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</input>
    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2019.11 路痴大魔王</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class="" id="rocket" ></a>

    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close"  onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div>


</body>
<script src="/js/jquery.pjax.js?v=1.0.1" ></script>

<script src="/js/script.js?v=1.0.1" ></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['@郁明敏','#Centos','#MySQL','#Logistic Regression','#蓝鲸','#Requests','#chrome','#impala','#datagrip','#Flask','#GitHub','#hadoop','#hive','#Jupyter','#Python学习资源','#Linux','#LGBM','#GLIBC','#系统','#Mac','#markdown','#notepad','#pycharm','#Python常用库','#impyla','#强化学习','#sublime','#随机过程','#windows软件','#windows','#xshell','#3-hexo','#hexo','#Python小技巧','#Python关键字','#数据挖掘','#AirFlow',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: ;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 492px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.3;
        background: url("");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
